{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cac24be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourav/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from embedding_generator import EmbeddingGenerator\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ab688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = EmbeddingGenerator()\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open('chunks_with_embeddings.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract chunks and their embeddings\n",
    "chunks = [item['chunk_text'] for item in data]\n",
    "chunk_embeddings = torch.tensor([item['embedding'] for item in data], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901dbc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What are the theoretical limitations of embedding-based retrieval?\",\n",
    "    \"How does the geometry of the embedding space affect retrieval performance?\",\n",
    "    \"What is the 'norm concentration' phenomenon in high-dimensional spaces?\",\n",
    "    \"Can embedding models truly capture semantic similarity for all tasks?\",\n",
    "    \"What are the failure modes for embedding-based search?\",\n",
    "    \"How can neural retrieval be optimized for hardware accelerators?\",\n",
    "    \"What are the bottlenecks in modern neural retrieval pipelines?\",\n",
    "    \"What is the trade-off between retrieval effectiveness and efficiency on accelerators?\",\n",
    "    \"Explain the concept of query-side latency in neural retrieval.\",\n",
    "    \"What are some state-of-the-art techniques for efficient neural retrieval on GPUs or TPUs?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d6f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_mixture_of_logits(query: str, top_k: int = 5, \n",
    "                                  temperature: float = 2.0) -> List[Tuple[float, str]]:\n",
    "    \"\"\"\n",
    "    Retrieves the top_k most relevant chunks using a mixture of logits approach.\n",
    "    \n",
    "    Args:\n",
    "        query: The query string\n",
    "        top_k: Number of top results to return\n",
    "        temperature: Temperature parameter for softmax scaling\n",
    "    \n",
    "    Returns:\n",
    "        List of (score, chunk) tuples\n",
    "    \"\"\"\n",
    "    # Get query embedding using our generator\n",
    "    query_embedding = torch.tensor(generator.get_embedding(query))\n",
    "    # Compute raw logits (dot product)\n",
    "    logits = torch.matmul(chunk_embeddings, query_embedding)\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    scaled_logits = logits / temperature\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_logits, dim=0)\n",
    "    \n",
    "    # Get top-k indices and scores\n",
    "    top_k_values, top_k_indices = torch.topk(attention_weights, k=top_k)\n",
    "    \n",
    "    # Create result list\n",
    "    results = [(score.item(), chunks[idx.item()]) \n",
    "              for score, idx in zip(top_k_values, top_k_indices)]\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6524ab61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query: What are the theoretical limitations of embedding-based retrieval? ===\n",
      "\n",
      "Score: 0.0119\n",
      "Chunk: On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Iftekhar Naim1 and Jinhyuk Lee1\n",
      "1Google DeepMind,2Johns Hopkins University\n",
      "Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a\n",
      "nascentriseinusingthemforreasoning,instruction-following,coding, andmore. Thesenewbenchmarks\n",
      "push embeddings to work forany queryand any notion of relevancethat could be given. While prior\n",
      "works have pointed out theoretical limitations of vector embeddings, there is a common assumption\n",
      "that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome\n",
      "with better training data and larger models. In this work, we demonstrate that we may encounter these\n",
      "theoretical limitations in realistic settings with extremely simple queries. We connect known results\n",
      "in learning theory, showing that the number of top-ùëò subsets of documents capable of being returned\n",
      "as the result of some query is limited by the dimension of the embedding. We empirically show that\n",
      "this holds true even if we restrict toùëò = 2, and directly optimize on the test set with free parameterized\n",
      "embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these\n",
      "theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple\n",
      "nature of the task. Our work shows the limits of embedding models under the existing single vector\n",
      "paradigm and calls for future research to develop methods that can resolve this fundamental limitation.\n",
      "1. Introduction\n",
      "Over the last two decades, information retrieval (IR) has moved from models dominated by sparse\n",
      "techniques (such as BM25 [Robertson et al., 1995]) to those that use neural language models (LM)\n",
      "as their backbones [Lee et al., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].\n",
      "\n",
      "Score: 0.0117\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments provide theoretical insight for the most common type of embedding model\n",
      "(single vector) they do not hold necessarily for other architectures, such as multi-vector models.\n",
      "Although we showed initial empirical results with non-single vector models, we leave it to future work\n",
      "to extend our theoretical connections to these settings.\n",
      "We also did not show theoretical results for the setting where the user allows some mistakes, e.g.\n",
      "capturing only the majority of the combinations. We leave putting a bound on this scenario to future\n",
      "work and would invite the reader to examine works like Ben-David et al. [2002].\n",
      "We have showed the theoretical connection that proves that some combinations cannot be repre-\n",
      "sented by embedding models, however, we cannot prove apriori whichtypes of combinations they\n",
      "will fail on. Thus, it is possible that there are some instruction-following or reasoning tasks they can\n",
      "solve perfectly, however,we do knowthat there exists some tasks that they will never be able to solve.\n",
      "Acknowledgments\n",
      "We thank Tanmaya Dabral, Zhongli Ding, Anthony Chen, Ming-Wei Chang, Kenton Lee, and Kristina\n",
      "Toutanova for their helpful feedback.\n",
      "References\n",
      "N. Alon, S. Moran, and A. Yehudayoff. Sign rank, vc dimension and spectral gaps. InElectronic\n",
      "Colloquium on Computational Complexity (ECCC), volume 21, page 10, 2014.\n",
      "P. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy. Llm2vec: Large\n",
      "language models are secretly powerful text encoders.arXiv preprint arXiv:2404.05961, 2024.\n",
      "S. Ben-David, N. Eiron, and H. U. Simon. Limitations of learning via embeddings in euclidean half\n",
      "spaces. Journal of Machine Learning Research, 3(Nov):441‚Äì461, 2002.\n",
      "C. Bohler, P. Cheilaris, R. Klein, C.-H. Liu, E. Papadopoulou, and M. Zavershynskyi. On the\n",
      "complexity of higher order abstract voronoi diagrams. Computational Geometry, 48(8):539‚Äì\n",
      "\n",
      "Score: 0.0117\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Our previous results show both theoretically and empirically that embedding models cannot represent\n",
      "all combinations of documents in their top-ùëòsets, making them unable to represent and solve some\n",
      "retrieval tasks. As current embedding models have grown larger (e.g. up to 4096), this has helped\n",
      "reduce negative effects for smaller dataset sizes. However, with enough combinations of top-ùëò sets\n",
      "the dimensionality would have to increase to an infeasible size for non-toy datasets.\n",
      "Thus, our results show an interesting tradeoff: embeddings can represent a large amount of\n",
      "combinations but notall combinations. Although they are useful for first stage results to a degree,\n",
      "more expressive retriever architectures will be needed. We briefly discuss some of these below.\n",
      "Cross-Encoders Although not suitable for first stage retrieval at scale, they are already typically\n",
      "used to improve first stage results. However, is LIMIT challenging for rerankers also?\n",
      "We evaluate a long context reranker, Gemini-2.5-Pro [Comanici et al., 2025] on the small setting\n",
      "as a comparison. We give Gemini all 46 documents and all 1000 queries at once, asking it to output\n",
      "the relevant documents for each query with one generation. We find that it can successfully solve\n",
      "(100%) all 1000 queries in one forward pass. This is in contrast to even the best embedding models\n",
      "with a recall@2 of less than 60% (Figure 4). Thus we can see that LIMIT is simple for state-of-the-art\n",
      "reranker models as they do not have the same limitations based on embedding dimension. However,\n",
      "they still have the limitation of being more computationally expensive than embedding models and\n",
      "thus cannot be used for first stage retrieval when there are large numbers of documents.\n",
      "Multi-vector models Multi-vector models are more expressive through the use of multiple vectors\n",
      "\n",
      "Score: 0.0117\n",
      "Chunk: setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "empirically show how the embedding dimension enables the solving of retrieval tasks. We find there\n",
      "exists a crucial point for each embedding dimension (ùëë) where the number of documents is too large\n",
      "for the embedding dimension to encode all combinations. We then gather these crucial points for a\n",
      "variety ofùëë and show that this relationship can be modeled empirically with a polynomial function.\n",
      "We also go one step further and construct a realistic but simple dataset based on these theoret-\n",
      "ical limitations (called LIMIT). Despite the simplicity of the task (e.g.,who likes Apples? and\n",
      "Jon likes Apples, ...), we find it is very difficult for even state-of-the-art embedding mod-\n",
      "els [Lee et al., 2025, Zhang et al., 2025] on MTEB [Enevoldsen et al., 2025] due to the theoretical\n",
      "underpinnings, and impossible1 for models with small embedding dimensions.\n",
      "Overall, our work contributes: (1) a theoretical basis for the fundamental limitations of embedding\n",
      "models, (2) a best-case empirical analysis showing that this proof holds for any dataset instantiation\n",
      "(by free embedding optimization), and (3) a simple real-world natural language instantiation called\n",
      "LIMIT that even state-of-the-art embedding models cannot solve.\n",
      "These results imply interesting findings for the community: on one hand we see neural embedding\n",
      "models becoming immensely successful. However, academic benchmarks test only a small amount of\n",
      "the queries that could be issued (and these queries are often overfitted to), hiding these limitations.\n",
      "Our work shows that as the tasks given to embedding models require returning ever-increasing\n",
      "combinations of top-ùëòrelevant documents (e.g., through instructions connecting previously unrelated\n",
      "1At least with current optimization techniques for retrieval.\n",
      "2\n",
      "\n",
      "Score: 0.0117\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "documents with logical operators), we will reach a limit of combinations they cannot represent.\n",
      "Thus, the community should be aware of these limitations, both when designing evaluations (as\n",
      "LIMITshows)andbychoosingalternativeretrievalapproaches‚Äìsuchascross-encoders ormulti-vector\n",
      "models ‚Äì when attempting to create models that can handle the full range of instruction-based queries,\n",
      "i.e. any query and relevance definition.\n",
      "2. Related Work\n",
      "2.1. Neural Embedding Models\n",
      "There has been immense progress on embedding models in recent years [Lee et al., 2019, Craswell\n",
      "et al., 2020, BehnamGhader et al., 2024], moving from simple web search (text-only) to advanced\n",
      "instruction-following and multi-modal representations. These models generally followed advances in\n",
      "language models, such as pre-trained LMs [Hoffmann et al., 2022], multi-modal LMs [Li et al., 2024,\n",
      "Team, 2024], and advances in instruction-following [Zhou et al., 2023, Ouyang et al., 2022]. Some\n",
      "of the prominent examples in retrieval include CoPali [Faysse et al., 2024] and DSE [Ma et al., 2024]\n",
      "which focus on multimodal embeddings, Instructor [Su et al., 2022] and FollowIR [Weller et al.,\n",
      "2024a] for instruction following, and GritLM [Muennighoff et al., 2024] and Gemini Embeddings\n",
      "[Lee et al., 2025] for pre-trained LMs turned embedders.\n",
      "Ourwork, thoughfocusedsolelyontextualrepresentationsfor simplicity, appliestoallmodalities\n",
      "of single vector embeddings for any domain of dataset. As the space of things to represent grows\n",
      "(through instructions or multi-modality) they will increasingly run into these theoretical limitations.\n",
      "2.2. Empirical tasks pushing the limits of dense retrieval\n",
      "Retrieval models have been pushed beyond their initial use cases to handle a broad variety of areas.\n",
      "Notable works include efforts to represent a wide group of domains [Thakur et al., 2021, Lee et al.,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Query: How does the geometry of the embedding space affect retrieval performance? ===\n",
      "\n",
      "Score: 0.0116\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "ùëë Critical-ùëõ\n",
      "4 10\n",
      "5 14\n",
      "6 19\n",
      "7 24\n",
      "8 28\n",
      "9 32\n",
      "10 36\n",
      "11 42\n",
      "12 47\n",
      "13 54\n",
      "14 62\n",
      "15 70\n",
      "16 79\n",
      "17 89\n",
      "18 99\n",
      "19 109\n",
      "20 120\n",
      "21 132\n",
      "22 144\n",
      "23 157\n",
      "24 170\n",
      "25 184\n",
      "26 198\n",
      "27 213\n",
      "28 229\n",
      "29 245\n",
      "30 261\n",
      "31 278\n",
      "32 296\n",
      "33 314\n",
      "34 333\n",
      "35 352\n",
      "36 372\n",
      "37 392\n",
      "38 413\n",
      "39 434\n",
      "40 460\n",
      "41 484\n",
      "42 505\n",
      "43 545\n",
      "44 605\n",
      "45 626\n",
      "Table 6|Critical Values of n for different d values in the Free Embedding optimization experiments.\n",
      "See Figure 2 for the corresponding figure.\n",
      "Model BEIR LIMIT R@100\n",
      "Snowflake Arctic 55.22 3.3\n",
      "Promptriever 56.40 18.9\n",
      "E5-Mistral 57.07 8.3\n",
      "GritLM 57.40 12.9\n",
      "Gemini Embed 62.65 10.0\n",
      "Qwen3 Embed 62.76 4.8\n",
      "Table 7|BEIR vs LIMIT results. See Figure 7 for the comparable plot.\n",
      "24\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Model Dim Random Dense Cycle Disjoint\n",
      "BM25 default 96.1 93.0 96.0 96.6\n",
      "E5-Mistral 7B 32 1.7 0.6 1.7 2.2\n",
      "E5-Mistral 7B 64 4.3 0.5 3.3 4.8\n",
      "E5-Mistral 7B 128 10.3 0.9 9.1 10.5\n",
      "E5-Mistral 7B 256 16.9 1.2 14.0 15.5\n",
      "E5-Mistral 7B 512 26.4 2.5 24.0 26.6\n",
      "E5-Mistral 7B 768 31.5 3.1 27.7 30.0\n",
      "E5-Mistral 7B 1024 34.0 3.8 29.5 32.8\n",
      "E5-Mistral 7B 2048 36.8 4.3 33.6 36.7\n",
      "E5-Mistral 7B 3072 38.9 4.7 35.8 37.6\n",
      "E5-Mistral 7B 4096 40.4 4.8 36.6 38.8\n",
      "GTE-ModernColBERT default 71.1 61.8 65.3 70.1\n",
      "GritLM 7B 32 1.5 0.6 1.9 1.5\n",
      "GritLM 7B 64 3.6 0.6 2.9 3.9\n",
      "GritLM 7B 128 8.0 1.6 6.3 8.4\n",
      "GritLM 7B 256 15.8 2.0 14.4 16.0\n",
      "GritLM 7B 512 33.7 4.5 29.5 33.8\n",
      "GritLM 7B 768 39.0 5.6 34.4 40.1\n",
      "GritLM 7B 1024 43.3 6.6 37.4 44.1\n",
      "GritLM 7B 2048 55.3 9.0 49.0 55.8\n",
      "GritLM 7B 3072 61.5 10.9 54.3 61.6\n",
      "GritLM 7B 4096 61.8 10.4 56.6 63.2\n",
      "Promptriever Llama3 8B 32 0.7 0.6 1.2 1.1\n",
      "Promptriever Llama3 8B 64 2.6 1.1 2.8 2.3\n",
      "Promptriever Llama3 8B 128 5.7 1.3 5.7 7.1\n",
      "Promptriever Llama3 8B 256 16.2 1.7 12.6 16.3\n",
      "Promptriever Llama3 8B 512 31.9 4.7 26.0 29.0\n",
      "Promptriever Llama3 8B 768 37.5 8.5 33.2 37.5\n",
      "Promptriever Llama3 8B 1024 42.3 11.8 37.5 40.5\n",
      "Promptriever Llama3 8B 2048 52.7 14.1 49.1 53.7\n",
      "Promptriever Llama3 8B 3072 56.6 15.8 52.9 57.4\n",
      "Promptriever Llama3 8B 4096 62.0 19.4 58.6 63.6\n",
      "Qwen3 Embed 32 3.2 0.7 2.7 2.6\n",
      "Qwen3 Embed 64 5.4 1.1 5.0 5.7\n",
      "Qwen3 Embed 128 9.9 1.9 7.9 9.4\n",
      "Qwen3 Embed 256 14.2 2.4 11.6 12.5\n",
      "Qwen3 Embed 512 18.0 3.3 14.7 15.9\n",
      "Qwen3 Embed 768 19.5 3.5 15.5 18.0\n",
      "Qwen3 Embed 1024 20.4 3.6 16.1 18.7\n",
      "Qwen3 Embed 2048 22.3 4.1 17.2 21.4\n",
      "Qwen3 Embed 3072 21.9 4.3 17.9 21.1\n",
      "Qwen3 Embed 4096 22.7 4.5 17.8 20.9\n",
      "Gemini Embed 2 0.0 0.1 0.1 0.0\n",
      "Gemini Embed 4 0.0 0.0 0.0 0.1\n",
      "Gemini Embed 8 0.2 0.0 0.0 0.2\n",
      "Gemini Embed 16 0.2 0.0 0.2 0.1\n",
      "Gemini Embed 32 0.4 0.0 0.2 0.1\n",
      "Gemini Embed 64 0.6 0.2 0.3 0.5\n",
      "Gemini Embed 128 1.4 0.3 0.8 1.4\n",
      "Gemini Embed 256 7.1 1.2 5.8 7.4\n",
      "Gemini Embed 512 18.9 3.6 17.6 19.7\n",
      "Gemini Embed 768 33.5 7.6 31.0 34.5\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Split Dim Recall@2 Recall@10 Recall@100\n",
      "Test 32 85.5 98.4 100.0\n",
      "Test 64 90.4 98.7 100.0\n",
      "Test 128 93.1 99.5 99.9\n",
      "Test 256 94.2 99.7 100.0\n",
      "Test 384 95.6 99.6 100.0\n",
      "Test 512 94.0 99.5 99.9\n",
      "Test 768 96.1 99.8 100.0\n",
      "Test 1024 96.5 99.8 100.0\n",
      "Train 32 0.0 0.0 0.0\n",
      "Train 64 0.1 0.3 2.2\n",
      "Train 128 0.2 0.7 3.1\n",
      "Train 256 0.0 0.0 0.4\n",
      "Train 384 1.1 2.7 8.3\n",
      "Train 512 0.7 2.3 9.8\n",
      "Train 768 0.7 2.4 9.9\n",
      "Train 1024 1.0 2.8 11.2\n",
      "Table 2|Fine-tuning results in table form. See Figure 5 for the comparable plot.\n",
      "20\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Iftekhar Naim1 and Jinhyuk Lee1\n",
      "1Google DeepMind,2Johns Hopkins University\n",
      "Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a\n",
      "nascentriseinusingthemforreasoning,instruction-following,coding, andmore. Thesenewbenchmarks\n",
      "push embeddings to work forany queryand any notion of relevancethat could be given. While prior\n",
      "works have pointed out theoretical limitations of vector embeddings, there is a common assumption\n",
      "that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome\n",
      "with better training data and larger models. In this work, we demonstrate that we may encounter these\n",
      "theoretical limitations in realistic settings with extremely simple queries. We connect known results\n",
      "in learning theory, showing that the number of top-ùëò subsets of documents capable of being returned\n",
      "as the result of some query is limited by the dimension of the embedding. We empirically show that\n",
      "this holds true even if we restrict toùëò = 2, and directly optimize on the test set with free parameterized\n",
      "embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these\n",
      "theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple\n",
      "nature of the task. Our work shows the limits of embedding models under the existing single vector\n",
      "paradigm and calls for future research to develop methods that can resolve this fundamental limitation.\n",
      "1. Introduction\n",
      "Over the last two decades, information retrieval (IR) has moved from models dominated by sparse\n",
      "techniques (such as BM25 [Robertson et al., 1995]) to those that use neural language models (LM)\n",
      "as their backbones [Lee et al., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "empirically show how the embedding dimension enables the solving of retrieval tasks. We find there\n",
      "exists a crucial point for each embedding dimension (ùëë) where the number of documents is too large\n",
      "for the embedding dimension to encode all combinations. We then gather these crucial points for a\n",
      "variety ofùëë and show that this relationship can be modeled empirically with a polynomial function.\n",
      "We also go one step further and construct a realistic but simple dataset based on these theoret-\n",
      "ical limitations (called LIMIT). Despite the simplicity of the task (e.g.,who likes Apples? and\n",
      "Jon likes Apples, ...), we find it is very difficult for even state-of-the-art embedding mod-\n",
      "els [Lee et al., 2025, Zhang et al., 2025] on MTEB [Enevoldsen et al., 2025] due to the theoretical\n",
      "underpinnings, and impossible1 for models with small embedding dimensions.\n",
      "Overall, our work contributes: (1) a theoretical basis for the fundamental limitations of embedding\n",
      "models, (2) a best-case empirical analysis showing that this proof holds for any dataset instantiation\n",
      "(by free embedding optimization), and (3) a simple real-world natural language instantiation called\n",
      "LIMIT that even state-of-the-art embedding models cannot solve.\n",
      "These results imply interesting findings for the community: on one hand we see neural embedding\n",
      "models becoming immensely successful. However, academic benchmarks test only a small amount of\n",
      "the queries that could be issued (and these queries are often overfitted to), hiding these limitations.\n",
      "Our work shows that as the tasks given to embedding models require returning ever-increasing\n",
      "combinations of top-ùëòrelevant documents (e.g., through instructions connecting previously unrelated\n",
      "1At least with current optimization techniques for retrieval.\n",
      "2\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Query: What is the 'norm concentration' phenomenon in high-dimensional spaces? ===\n",
      "\n",
      "Score: 0.0116\n",
      "Chunk: majority of the volume of a unit hypersphere lies near its ‚Äòequa-\n",
      "tor‚Äô which is in turn a (ùëë‚àí1)dimensional hypersphere. Further,\n",
      "the majority of a unit hypersphere‚Äôs volume lies near its surface.\n",
      "This property encourages component-level hypersphere embed-\n",
      "dings, even if undertrained, to be near-orthogonal to each other:\n",
      "limùëë‚Üíinf ‚ü®ùëìùëòùë¢ (ùë¢),ùëîùëòùë• (ùë•)‚ü©/||ùëìùëòùë¢ (ùë¢)||2/||ùëîùëòùë• (ùë•)||2 = 0. In contrast,\n",
      "without normalization, assume the dimensions of each embedding\n",
      "are i.i.d. in [-1, 1], the limit of the above equation goes to infin-\n",
      "ity. Formally, the modified MoL setup can be found in Equation 9,\n",
      "where the term ùúè (ùúè > 1) rescales post softmax output range back\n",
      "to (0,1), similar to temperature in L2-normalized softmax.\n",
      "ùúôùëÄùëúùêøùëì ùëñùëõùëéùëô(ùë•,ùë¢)=\n",
      "‚àëÔ∏Å\n",
      "ùëòùë¢,ùëòùë•\n",
      "ùúãùëòùë¢,ùëòùë• ,ùúÉ(ùë•,ùë¢)\n",
      "\u001c ùëìùëòùë¢,ùúÉ(ùë¢)\n",
      "||ùëìùëòùë¢,ùúÉ(ùë¢)||2\n",
      ", ùëîùëòùë• ,ùúÉ(ùë•)\n",
      "||ùëîùëòùë• ,ùúÉ(ùë•)||2\n",
      "\u001d\n",
      "/ùúè\n",
      "(9)\n",
      "Besides solving the problem of undertrained component-level\n",
      "embeddings, the other nice property of Equation 9 is that we force\n",
      "MoL logit components to measure angular distances instead of\n",
      "both vector magnitudes and angular distances, which helps avoid-\n",
      "ing degrading the model to only learn about popular items given\n",
      "impressions on items generally follow a power-law distribution.\n",
      "We present ablation studies in Section 5.1.3.\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: ùê∑ = 1024 (ùê∑ùëà = 768, ùê∑ùëã = 128, ùê∑ùëãùëà = 128), ùêæ = 256, ùêø= 128.\n",
      "GFLOPs HBM (fp32)\n",
      "w/o gating decomposition 2473.9 44GB\n",
      "w/ gating decomposition 1101.0 (-55.5%) 16GB (-63.6%)\n",
      "ùêø)+ùêµùëãùêæ(ùê∑ùëãùëà +ùêø))with gating function decomposition. With de-\n",
      "composition of user, item, user-item tensors, we are able to reduce\n",
      "the computational cost of the most expensive part (withùê∑ùëãùëà ‚â™ùê∑)\n",
      "while maintaining model quality. We show the benefit of this ap-\n",
      "proach for practical settings in Table 2.\n",
      "Finally, we regularize ùúãùúÉ(ùë•|ùë¢)by forcing it to take the form of\n",
      "a probability distribution per (user, item) pair, i.e.,√ç\n",
      "ùëò ùúãùëò,ùúÉ(ùë•|ùë¢)=\n",
      "1. We found that applying dropout on top of this distribution to\n",
      "be helpful, which encourages more balanced mixture component\n",
      "utilization similar to Mixture-of-Experts [14, 16].\n",
      "3.3 Component-level Hypersphere Embeddings\n",
      "The large number of component-level embeddings used in Equa-\n",
      "tion 4 and 6 could introduce two problems, preventing generaliza-\n",
      "tion of the model to the torso and long tail in retrieval setting:\n",
      "‚Ä¢The gating function ùúãùúÉ(ùë•,ùë¢)can make component-level\n",
      "embeddings undertrained (esp. in online training settings),\n",
      "which in turn introduces training instability over time.\n",
      "‚Ä¢They may exacerbate overfitting or require significantly\n",
      "more negatives (more compute) to properly fit the model.\n",
      "We propose L2 normalizing the component embeddings as a\n",
      "simple and effective regularization to fix the problem. The intu-\n",
      "ition is as follows. L2 normalization forces all MoL component\n",
      "embeddings in Equation 6 to lay on the surface of a ùëë-dimensional\n",
      "hypersphere. Denote the volume of a ùëë-dimensional hypersphere\n",
      "by ùëâ(ùëë). We haveùëâ(ùëë)‚â• 1‚àö\n",
      "ùëë‚àí1ùëâ(ùëë‚àí1); as ùëëgrows to infinity, the\n",
      "majority of the volume of a unit hypersphere lies near its ‚Äòequa-\n",
      "tor‚Äô which is in turn a (ùëë‚àí1)dimensional hypersphere. Further,\n",
      "the majority of a unit hypersphere‚Äôs volume lies near its surface.\n",
      "This property encourages component-level hypersphere embed-\n",
      "dings, even if undertrained, to be near-orthogonal to each other:\n",
      "\n",
      "Score: 0.0114\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "ùëë Critical-ùëõ\n",
      "4 10\n",
      "5 14\n",
      "6 19\n",
      "7 24\n",
      "8 28\n",
      "9 32\n",
      "10 36\n",
      "11 42\n",
      "12 47\n",
      "13 54\n",
      "14 62\n",
      "15 70\n",
      "16 79\n",
      "17 89\n",
      "18 99\n",
      "19 109\n",
      "20 120\n",
      "21 132\n",
      "22 144\n",
      "23 157\n",
      "24 170\n",
      "25 184\n",
      "26 198\n",
      "27 213\n",
      "28 229\n",
      "29 245\n",
      "30 261\n",
      "31 278\n",
      "32 296\n",
      "33 314\n",
      "34 333\n",
      "35 352\n",
      "36 372\n",
      "37 392\n",
      "38 413\n",
      "39 434\n",
      "40 460\n",
      "41 484\n",
      "42 505\n",
      "43 545\n",
      "44 605\n",
      "45 626\n",
      "Table 6|Critical Values of n for different d values in the Free Embedding optimization experiments.\n",
      "See Figure 2 for the corresponding figure.\n",
      "Model BEIR LIMIT R@100\n",
      "Snowflake Arctic 55.22 3.3\n",
      "Promptriever 56.40 18.9\n",
      "E5-Mistral 57.07 8.3\n",
      "GritLM 57.40 12.9\n",
      "Gemini Embed 62.65 10.0\n",
      "Qwen3 Embed 62.76 4.8\n",
      "Table 7|BEIR vs LIMIT results. See Figure 7 for the comparable plot.\n",
      "24\n",
      "\n",
      "Score: 0.0113\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Model Dim Random Dense Cycle Disjoint\n",
      "BM25 default 96.1 93.0 96.0 96.6\n",
      "E5-Mistral 7B 32 1.7 0.6 1.7 2.2\n",
      "E5-Mistral 7B 64 4.3 0.5 3.3 4.8\n",
      "E5-Mistral 7B 128 10.3 0.9 9.1 10.5\n",
      "E5-Mistral 7B 256 16.9 1.2 14.0 15.5\n",
      "E5-Mistral 7B 512 26.4 2.5 24.0 26.6\n",
      "E5-Mistral 7B 768 31.5 3.1 27.7 30.0\n",
      "E5-Mistral 7B 1024 34.0 3.8 29.5 32.8\n",
      "E5-Mistral 7B 2048 36.8 4.3 33.6 36.7\n",
      "E5-Mistral 7B 3072 38.9 4.7 35.8 37.6\n",
      "E5-Mistral 7B 4096 40.4 4.8 36.6 38.8\n",
      "GTE-ModernColBERT default 71.1 61.8 65.3 70.1\n",
      "GritLM 7B 32 1.5 0.6 1.9 1.5\n",
      "GritLM 7B 64 3.6 0.6 2.9 3.9\n",
      "GritLM 7B 128 8.0 1.6 6.3 8.4\n",
      "GritLM 7B 256 15.8 2.0 14.4 16.0\n",
      "GritLM 7B 512 33.7 4.5 29.5 33.8\n",
      "GritLM 7B 768 39.0 5.6 34.4 40.1\n",
      "GritLM 7B 1024 43.3 6.6 37.4 44.1\n",
      "GritLM 7B 2048 55.3 9.0 49.0 55.8\n",
      "GritLM 7B 3072 61.5 10.9 54.3 61.6\n",
      "GritLM 7B 4096 61.8 10.4 56.6 63.2\n",
      "Promptriever Llama3 8B 32 0.7 0.6 1.2 1.1\n",
      "Promptriever Llama3 8B 64 2.6 1.1 2.8 2.3\n",
      "Promptriever Llama3 8B 128 5.7 1.3 5.7 7.1\n",
      "Promptriever Llama3 8B 256 16.2 1.7 12.6 16.3\n",
      "Promptriever Llama3 8B 512 31.9 4.7 26.0 29.0\n",
      "Promptriever Llama3 8B 768 37.5 8.5 33.2 37.5\n",
      "Promptriever Llama3 8B 1024 42.3 11.8 37.5 40.5\n",
      "Promptriever Llama3 8B 2048 52.7 14.1 49.1 53.7\n",
      "Promptriever Llama3 8B 3072 56.6 15.8 52.9 57.4\n",
      "Promptriever Llama3 8B 4096 62.0 19.4 58.6 63.6\n",
      "Qwen3 Embed 32 3.2 0.7 2.7 2.6\n",
      "Qwen3 Embed 64 5.4 1.1 5.0 5.7\n",
      "Qwen3 Embed 128 9.9 1.9 7.9 9.4\n",
      "Qwen3 Embed 256 14.2 2.4 11.6 12.5\n",
      "Qwen3 Embed 512 18.0 3.3 14.7 15.9\n",
      "Qwen3 Embed 768 19.5 3.5 15.5 18.0\n",
      "Qwen3 Embed 1024 20.4 3.6 16.1 18.7\n",
      "Qwen3 Embed 2048 22.3 4.1 17.2 21.4\n",
      "Qwen3 Embed 3072 21.9 4.3 17.9 21.1\n",
      "Qwen3 Embed 4096 22.7 4.5 17.8 20.9\n",
      "Gemini Embed 2 0.0 0.1 0.1 0.0\n",
      "Gemini Embed 4 0.0 0.0 0.0 0.1\n",
      "Gemini Embed 8 0.2 0.0 0.0 0.2\n",
      "Gemini Embed 16 0.2 0.0 0.2 0.1\n",
      "Gemini Embed 32 0.4 0.0 0.2 0.1\n",
      "Gemini Embed 64 0.6 0.2 0.3 0.5\n",
      "Gemini Embed 128 1.4 0.3 0.8 1.4\n",
      "Gemini Embed 256 7.1 1.2 5.8 7.4\n",
      "Gemini Embed 512 18.9 3.6 17.6 19.7\n",
      "Gemini Embed 768 33.5 7.6 31.0 34.5\n",
      "\n",
      "Score: 0.0113\n",
      "Chunk: KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing Liu\n",
      "Table 9: Hyperparameters used for public datasets\n",
      "ML-1M ML-20M Beauty Games Books\n",
      "SASRec\n",
      "embedding dim ùëë 50 256 50 50 64\n",
      "encoder blocks ùëè 2 4 2 2 4\n",
      "attention heads ‚Ñé 1 8 1 1 4\n",
      "dropout rate 0.2 0.2 0.5 0.5 0.5\n",
      "Dot Product temperature 20 20 20 20 20\n",
      "MLP\n",
      "hidden layer size 512 512 128 128 256\n",
      "dropout rate 0.1 0.1 0.2 0.2 0.2\n",
      "Inference FLOPs per (ùë•, ùë¢) 50.5K 256.5K 12.6K 12.6K 32.3K\n",
      "NeuMF\n",
      "GMF dim 32 32 32 32 32\n",
      "MLP hidden dim 256 256 128 128 256\n",
      "MLP output dim 64 64 64 64 64\n",
      "dropout rate 0.1 0.1 0.2 0.2 0.1\n",
      "final MLP hidden dim 256 256 128 128 256\n",
      "Inference FLOPs per (ùë•, ùë¢) 49.3K 152.3K 24.6K 24.6K 56.3K\n",
      "FM\n",
      "hidden dim for output layer 256 256 128 128 256\n",
      "prediction layer dropout rate 0.2 0.2 0.3 0.3 0.2\n",
      "Inference FLOPs per (ùë•, ùë¢) 197.6K 405K 10.7K 10.7K 203.6K\n",
      "MoL\n",
      "(ùëòùë¢ x ùëòùë• x embedding dim) (8 x 8 x 32) (8 x 8 x 32) (4 x 4 x 32) (4 x 4 x 32) (8 x 8 x 32)\n",
      "hidden dim for gating MLPs 128 128 128 128 128\n",
      "hidden dim for embedding proj. MLPs 512 512 N/A N/A 512\n",
      "item-side embedding proj. MLP dropout rate 0.1 0.1 0.3 0.3 0.1\n",
      "gating softmax dropout rate 0.2 0.2 - - 0.2\n",
      "gating input dropout rate - - 0.2 0.2 -\n",
      "component-level hypersphere embeddings On On Off Off On\n",
      "ùúè 20 20 20 20 20\n",
      "Inference FLOPs per (ùë•, ùë¢) 211.7K 445.3K 12.9K 12.9K 227.6K\n",
      "Non-cachable inference FLOPs per (ùë•, ùë¢) 32.9K 33.1K 4.4K 4.4K 21.5K\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Query: Can embedding models truly capture semantic similarity for all tasks? ===\n",
      "\n",
      "Score: 0.0116\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "32 512 1024 2048 3072 4096\n",
      "Embed Dim\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8Score\n",
      "Recall@2\n",
      "32 512 1024 2048 3072 4096\n",
      "Embed Dim\n",
      "Recall@10\n",
      "32 512 1024 2048 3072 4096\n",
      "Embed Dim\n",
      "Recall@100\n",
      "E5-Mistral 7B\n",
      "Snowflake Arctic L\n",
      "GritLM 7B\n",
      "Promptriever Llama3 8B\n",
      "Qwen3 Embed\n",
      "Gemini Embed\n",
      "BM25\n",
      "GTE-ModernColBERT\n",
      "Figure 3|Scores on the LIMIT task. Despite the simplicity of the task we see that SOTA models\n",
      "struggle. We also see that the dimensionality of the model is a limiting factor and that as the\n",
      "dimension increases, so does performance. Even multi-vector models struggle. Lexical models like\n",
      "BM25 do very well due to their higher dimensionality. Stars indicate models trained with MRL.\n",
      "Models We evaluate the state-of-the-art embedding models including GritLM [Muennighoff et al.,\n",
      "2024], Qwen 3 Embeddings [Zhang et al., 2025], Promptriever [Weller et al., 2024b], Gemini\n",
      "Embeddings [Lee et al., 2025], Snowflake‚Äôs Arctic Embed Large v2.0 [Yu et al., 2024], and E5-Mistral\n",
      "Instruct [Wang et al., 2022, 2023]. These models range in embedding dimension (1024 to 4096)\n",
      "as well as in training style (instruction-based, hard negative optimized, etc.). We also evaluate\n",
      "three non-single vector models to show the distinction: BM25 [Robertson et al., 1995, L√π, 2024],\n",
      "gte-ModernColBERT [Chaffin, 2025, Chaffin and Sourty, 2024], and a token-wise TF-IDF.9\n",
      "We show results at the full embedding dimension and also with truncated embedding dimension\n",
      "(typically used with matryoshka learning, aka MRL [Kusupati et al., 2022]). For models not trained\n",
      "with MRL this will result in sub-par scores, thus, models trained with MRL are indicating with stars in\n",
      "the plots. However, as there are no LLMs with an embedding dimension smaller than 384, we include\n",
      "MRL for all models to small dimensions (32) to show the impact of embedding dimensionality.\n",
      "Results Figure 3 shows the results on the full LIMIT while Figure 4 shows the results on the small\n",
      "\n",
      "Score: 0.0116\n",
      "Chunk: 1. Introduction\n",
      "Over the last two decades, information retrieval (IR) has moved from models dominated by sparse\n",
      "techniques (such as BM25 [Robertson et al., 1995]) to those that use neural language models (LM)\n",
      "as their backbones [Lee et al., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].\n",
      "These neural models are predominantly used in a single vector capacity, where they output a single\n",
      "embedding representing the entire input (also known asdense retrieval). These embedding models\n",
      "are capable of generalizing to new retrieval datasets and have been tasked with solving increasingly\n",
      "complicated retrieval problems [Thakur et al., 2021, Enevoldsen et al., 2025, Lee et al., 2025].\n",
      "In recent years this has been pushed even further with the rise of instruction-following retrieval\n",
      "benchmarks, where models are asked to representany relevance definitionfor any query[Weller\n",
      "et al., 2025a,b, Song et al., 2025, Xiao et al., 2024, Su et al., 2024]. For example, the QUEST dataset\n",
      "[Malaviya et al., 2023] uses logical operators to combine different concepts, studying the difficulty\n",
      "of retrieval for complex queries (e.g., ‚ÄúMoths or Insects or Arthropods of Guadeloupe‚Äù). On the\n",
      "other hand, datasets like BRIGHT [Su et al., 2024] explore the challenges stemming from different\n",
      "definitions of relevance by defining relevance in ways that require reasoning. One subtask includes\n",
      "reasoning over a given Leetcode problem (the query) to find other Leetcode problems that share a\n",
      "sub-task (e.g. others problems using dynamic programming). Although models cannot solve these\n",
      "benchmarks yet, the community has proposed these problems in order to push the boundaries of\n",
      "what dense retrievers are capable of‚Äîwhich is now implicitlyevery taskthat could be defined.\n",
      "Rather than proposing empirical benchmarks to gauge what embedding models can achieve, we\n",
      "seek to understand at a more fundamental level what the limitations are. Since embedding models use\n",
      "‚àóWork done during internship at GDM.\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "T. Song, G. Gan, M. Shang, and Y. Zhao. Ifir: A comprehensive benchmark for evaluating instruction-\n",
      "following in expert-domain information retrieval.arXiv preprint arXiv:2503.04644, 2025.\n",
      "H. Su, W. Shi, J. Kasai, Y. Wang, Y. Hu, M. Ostendorf, W.-t. Yih, N. A. Smith, L. Zettlemoyer, and T. Yu.\n",
      "One embedder, any task: Instruction-finetuned text embeddings.arXiv preprint arXiv:2212.09741,\n",
      "2022.\n",
      "H. Su, H. Yen, M. Xia, W. Shi, N. Muennighoff, H.-y. Wang, H. Liu, Q. Shi, Z. S. Siegel, M. Tang, et al.\n",
      "Bright: A realistic and challenging benchmark for reasoning-intensive retrieval.arXiv preprint\n",
      "arXiv:2407.12883, 2024.\n",
      "C. Team. Chameleon: Mixed-modal early-fusion foundation models.arXiv preprint arXiv:2405.09818,\n",
      "2024.\n",
      "N. Thakur, N. Reimers, A. R√ºckl√©, A. Srivastava, and I. Gurevych. Beir: A heterogenous benchmark\n",
      "for zero-shot evaluation of information retrieval models.arXiv preprint arXiv:2104.08663, 2021.\n",
      "N. Thakur, J. Lin, S. Havens, M. Carbin, O. Khattab, and A. Drozdov. Freshstack: Building realistic\n",
      "benchmarks for evaluating retrieval on technical documents.arXiv preprint arXiv:2504.13128,\n",
      "2025.\n",
      "G. Voronoi. Nouvelles applications des param√®tres continus √† la th√©orie des formes quadratiques.\n",
      "deuxi√®mem√©moire.recherches surlesparall√©llo√®dresprimitifs. Journal f√ºrdiereine undangewandte\n",
      "Mathematik (Crelles Journal), 1908(134):198‚Äì287, 1908.\n",
      "D. Wadden, S. Lin, K. Lo, L. L. Wang, M. van Zuylen, A. Cohan, and H. Hajishirzi. Fact or fiction:\n",
      "Verifying scientific claims.arXiv preprint arXiv:2004.14974, 2020.\n",
      "L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text embeddings by\n",
      "weakly-supervised contrastive pre-training.arXiv preprint arXiv:2212.03533, 2022.\n",
      "L. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Improving text embeddings with\n",
      "large language models.arXiv preprint arXiv:2401.00368, 2023.\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text embeddings by\n",
      "weakly-supervised contrastive pre-training.arXiv preprint arXiv:2212.03533, 2022.\n",
      "L. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Improving text embeddings with\n",
      "large language models.arXiv preprint arXiv:2401.00368, 2023.\n",
      "B. Warner, A. Chaffin, B. Clavi√©, O. Weller, O. Hallstr√∂m, S. Taghadouini, A. Gallagher, R. Biswas,\n",
      "F. Ladhak, T. Aarsen, et al. Smarter, better, faster, longer: A modern bidirectional encoder for fast,\n",
      "memory efficient, and long context finetuning and inference.arXiv preprint arXiv:2412.13663,\n",
      "2024.\n",
      "J. Wei, Z. Sun, S. Papay, S. McKinney, J. Han, I. Fulford, H. W. Chung, A. T. Passos, W. Fedus, and\n",
      "A. Glaese. Browsecomp: A simple yet challenging benchmark for browsing agents.arXiv preprint\n",
      "arXiv:2504.12516, 2025.\n",
      "O. Weller, B. Chang, S. MacAvaney, K. Lo, A. Cohan, B. Van Durme, D. Lawrie, and L. Soldaini.\n",
      "Followir: Evaluating and teaching information retrieval models to follow instructions.arXiv preprint\n",
      "arXiv:2403.15246, 2024a.\n",
      "O. Weller, B. Van Durme, D. Lawrie, A. Paranjape, Y. Zhang, and J. Hessel. Promptriever: Instruction-\n",
      "trained retrievers can be prompted like language models.arXiv preprint arXiv:2409.11136, 2024b.\n",
      "O. Weller, B. Chang, E. Yang, M. Yarmohammadi, S. Barham, S. MacAvaney, A. Cohan, L. Soldaini,\n",
      "B. Van Durme, and D. Lawrie. mfollowir: a multilingual benchmark for instruction following in\n",
      "retrieval. arXiv preprint arXiv:2501.19264, 2025a.\n",
      "16\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: 2.2. Empirical tasks pushing the limits of dense retrieval\n",
      "Retrieval models have been pushed beyond their initial use cases to handle a broad variety of areas.\n",
      "Notable works include efforts to represent a wide group of domains [Thakur et al., 2021, Lee et al.,\n",
      "2024], a diverse set of instructions [Weller et al., 2024a, Zhou et al., 2024, Oh et al., 2024], and to\n",
      "handle reasoning over the queries [Xiao et al., 2024, Su et al., 2024]. This has pushed the focus of\n",
      "embedding models from basic keyword matching to embeddings that can represent the full semantic\n",
      "meaning of language. As such, it is more common than ever to connect what were previously unrelated\n",
      "documents into the top-ùëò relevant set,2 increasing the number of combinations that models must be\n",
      "able to represent. This has motivated our interest in understanding the limits of what embeddings\n",
      "can represent, as current work expects it to handleevery task.\n",
      "Previousworkhasexploredempiricallythelimitsofmodels: ReimersandGurevych[2020]showed\n",
      "that smaller dimension embedding models have more false positives, especially with larger-scale\n",
      "corpora. Ormazabal et al. [2019] showed the empirical limitations of models in the cross-lingual\n",
      "setting and Yin and Shen [2018] showed how embedding dimensions relate to the bias-variance\n",
      "tradeoff. In contrast, our work provides a theoretical connection between the embedding dimension\n",
      "and the sign-rank of the query relevance (qrel) matrix, while also showing empirical limitations.\n",
      "2.3. Theoretical Limits of Vectors in Geometric Space\n",
      "Understanding and finding nearest neighbors in semantic space has a long history in mathematics\n",
      "research, withearlyworksuchastheVoronoidiagrambeingstudiedasfarbackas1644andformalized\n",
      "in 1908 [Voronoi, 1908]. The order-k version of the Voronoi diagram (i.e. the Voronoi diagram\n",
      "2You can imagine an easy way to connect any two documents merely by using logical operators, i.e. X and Y.\n",
      "3\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Query: What are the failure modes for embedding-based search? ===\n",
      "\n",
      "Score: 0.0116\n",
      "Chunk: On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Iftekhar Naim1 and Jinhyuk Lee1\n",
      "1Google DeepMind,2Johns Hopkins University\n",
      "Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a\n",
      "nascentriseinusingthemforreasoning,instruction-following,coding, andmore. Thesenewbenchmarks\n",
      "push embeddings to work forany queryand any notion of relevancethat could be given. While prior\n",
      "works have pointed out theoretical limitations of vector embeddings, there is a common assumption\n",
      "that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome\n",
      "with better training data and larger models. In this work, we demonstrate that we may encounter these\n",
      "theoretical limitations in realistic settings with extremely simple queries. We connect known results\n",
      "in learning theory, showing that the number of top-ùëò subsets of documents capable of being returned\n",
      "as the result of some query is limited by the dimension of the embedding. We empirically show that\n",
      "this holds true even if we restrict toùëò = 2, and directly optimize on the test set with free parameterized\n",
      "embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these\n",
      "theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple\n",
      "nature of the task. Our work shows the limits of embedding models under the existing single vector\n",
      "paradigm and calls for future research to develop methods that can resolve this fundamental limitation.\n",
      "1. Introduction\n",
      "Over the last two decades, information retrieval (IR) has moved from models dominated by sparse\n",
      "techniques (such as BM25 [Robertson et al., 1995]) to those that use neural language models (LM)\n",
      "as their backbones [Lee et al., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments provide theoretical insight for the most common type of embedding model\n",
      "(single vector) they do not hold necessarily for other architectures, such as multi-vector models.\n",
      "Although we showed initial empirical results with non-single vector models, we leave it to future work\n",
      "to extend our theoretical connections to these settings.\n",
      "We also did not show theoretical results for the setting where the user allows some mistakes, e.g.\n",
      "capturing only the majority of the combinations. We leave putting a bound on this scenario to future\n",
      "work and would invite the reader to examine works like Ben-David et al. [2002].\n",
      "We have showed the theoretical connection that proves that some combinations cannot be repre-\n",
      "sented by embedding models, however, we cannot prove apriori whichtypes of combinations they\n",
      "will fail on. Thus, it is possible that there are some instruction-following or reasoning tasks they can\n",
      "solve perfectly, however,we do knowthat there exists some tasks that they will never be able to solve.\n",
      "Acknowledgments\n",
      "We thank Tanmaya Dabral, Zhongli Ding, Anthony Chen, Ming-Wei Chang, Kenton Lee, and Kristina\n",
      "Toutanova for their helpful feedback.\n",
      "References\n",
      "N. Alon, S. Moran, and A. Yehudayoff. Sign rank, vc dimension and spectral gaps. InElectronic\n",
      "Colloquium on Computational Complexity (ECCC), volume 21, page 10, 2014.\n",
      "P. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy. Llm2vec: Large\n",
      "language models are secretly powerful text encoders.arXiv preprint arXiv:2404.05961, 2024.\n",
      "S. Ben-David, N. Eiron, and H. U. Simon. Limitations of learning via embeddings in euclidean half\n",
      "spaces. Journal of Machine Learning Research, 3(Nov):441‚Äì461, 2002.\n",
      "C. Bohler, P. Cheilaris, R. Klein, C.-H. Liu, E. Papadopoulou, and M. Zavershynskyi. On the\n",
      "complexity of higher order abstract voronoi diagrams. Computational Geometry, 48(8):539‚Äì\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "empirically show how the embedding dimension enables the solving of retrieval tasks. We find there\n",
      "exists a crucial point for each embedding dimension (ùëë) where the number of documents is too large\n",
      "for the embedding dimension to encode all combinations. We then gather these crucial points for a\n",
      "variety ofùëë and show that this relationship can be modeled empirically with a polynomial function.\n",
      "We also go one step further and construct a realistic but simple dataset based on these theoret-\n",
      "ical limitations (called LIMIT). Despite the simplicity of the task (e.g.,who likes Apples? and\n",
      "Jon likes Apples, ...), we find it is very difficult for even state-of-the-art embedding mod-\n",
      "els [Lee et al., 2025, Zhang et al., 2025] on MTEB [Enevoldsen et al., 2025] due to the theoretical\n",
      "underpinnings, and impossible1 for models with small embedding dimensions.\n",
      "Overall, our work contributes: (1) a theoretical basis for the fundamental limitations of embedding\n",
      "models, (2) a best-case empirical analysis showing that this proof holds for any dataset instantiation\n",
      "(by free embedding optimization), and (3) a simple real-world natural language instantiation called\n",
      "LIMIT that even state-of-the-art embedding models cannot solve.\n",
      "These results imply interesting findings for the community: on one hand we see neural embedding\n",
      "models becoming immensely successful. However, academic benchmarks test only a small amount of\n",
      "the queries that could be issued (and these queries are often overfitted to), hiding these limitations.\n",
      "Our work shows that as the tasks given to embedding models require returning ever-increasing\n",
      "combinations of top-ùëòrelevant documents (e.g., through instructions connecting previously unrelated\n",
      "1At least with current optimization techniques for retrieval.\n",
      "2\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "32 512 1024 2048 3072 4096\n",
      "Embed Dim\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0Score\n",
      "Recall@2\n",
      "32 512 1024 2048 3072 4096\n",
      "Embed Dim\n",
      "Recall@10\n",
      "32 512 1024 2048 3072 4096\n",
      "Embed Dim\n",
      "Recall@20\n",
      "E5-Mistral 7B\n",
      "Snowflake Arctic L\n",
      "GritLM 7B\n",
      "Promptriever Llama3 8B\n",
      "Qwen3 Embed\n",
      "Gemini Embed\n",
      "BM25\n",
      "GTE-ModernColBERT\n",
      "Figure 4|Scores on the LIMIT small task (N=46) over embedding dimensions. Despite having just\n",
      "46 documents, model struggle even with recall@10 and cannot solve the task even with recall@20.\n",
      "models (although still far from solving the task) while BM25 comes close to perfect scores. Both of\n",
      "these alterative architectures (sparse and multi-vector) offer various trade-offs, see ¬ß5.6 for analysis.\n",
      "5.3. Is this Domain Shift?\n",
      "32128 256 384 512 768 1024\n",
      "Embed Dim\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0Recall@2\n",
      "Trained on:\n",
      "Test\n",
      "Train\n",
      "Figure 5|Training on LIMIT train does\n",
      "not significantly help, indicating the\n",
      "issue is not domain shift. But models\n",
      "can solve it if they overfit to the test set.\n",
      "Although our queries look similar to standard web search\n",
      "queries, we wondered whether there could be some do-\n",
      "main shift causing the low performance. If so, we would\n",
      "expect that training on a training set of similar examples\n",
      "would significantly improve performance. On the other\n",
      "hand, if the task was intrinsically hard, training on the\n",
      "training set would provide little help whereas training\n",
      "on the test set would allow the model to overfit to those\n",
      "tokens (similar to the free parameterized experiments).\n",
      "To test this we take an off the shelf embedding model\n",
      "and train it on either the training set (created synthetically\n",
      "usingnon-testsetattributes)ortheofficialtestsetofLIMIT.\n",
      "We use lightonai/modernbert-embed-large and\n",
      "fine-tune it on these splits, using the full dataset for in\n",
      "batchnegatives(excludingpositives)usingSentenceTrans-\n",
      "formers [Reimers and Gurevych, 2019]. We show a range of dimensions by projecting the hidden\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Our previous results show both theoretically and empirically that embedding models cannot represent\n",
      "all combinations of documents in their top-ùëòsets, making them unable to represent and solve some\n",
      "retrieval tasks. As current embedding models have grown larger (e.g. up to 4096), this has helped\n",
      "reduce negative effects for smaller dataset sizes. However, with enough combinations of top-ùëò sets\n",
      "the dimensionality would have to increase to an infeasible size for non-toy datasets.\n",
      "Thus, our results show an interesting tradeoff: embeddings can represent a large amount of\n",
      "combinations but notall combinations. Although they are useful for first stage results to a degree,\n",
      "more expressive retriever architectures will be needed. We briefly discuss some of these below.\n",
      "Cross-Encoders Although not suitable for first stage retrieval at scale, they are already typically\n",
      "used to improve first stage results. However, is LIMIT challenging for rerankers also?\n",
      "We evaluate a long context reranker, Gemini-2.5-Pro [Comanici et al., 2025] on the small setting\n",
      "as a comparison. We give Gemini all 46 documents and all 1000 queries at once, asking it to output\n",
      "the relevant documents for each query with one generation. We find that it can successfully solve\n",
      "(100%) all 1000 queries in one forward pass. This is in contrast to even the best embedding models\n",
      "with a recall@2 of less than 60% (Figure 4). Thus we can see that LIMIT is simple for state-of-the-art\n",
      "reranker models as they do not have the same limitations based on embedding dimension. However,\n",
      "they still have the limitation of being more computationally expensive than embedding models and\n",
      "thus cannot be used for first stage retrieval when there are large numbers of documents.\n",
      "Multi-vector models Multi-vector models are more expressive through the use of multiple vectors\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Query: How can neural retrieval be optimized for hardware accelerators? ===\n",
      "\n",
      "Score: 0.0118\n",
      "Chunk: Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\n",
      "ùúôùúÉ(ùë¢,ùë•)‚Äôs for ùë•‚Äôs that are positively associated with the user ahead\n",
      "of other ùúôùúÉ(ùë¢,ùë•‚Ä≤)‚Äôs for ùë•‚Ä≤‚ààX.\n",
      "It‚Äôs useful to formulate the similarity between user ùë¢and item ùë•\n",
      "as a probability distributionùëù(ùë•|ùë¢)[3, 48]. We do so by considering\n",
      "ùúôùúÉ(ùë•,ùë¢)as unnormalized logits and pass them through softmax:\n",
      "ùëù(ùë•|ùë¢)= ùëíùúôùúÉ (ùë•,ùë¢)\n",
      "√ç\n",
      "ùë•‚Ä≤‚ààX ùëíùúôùúÉ (ùë•‚Ä≤,ùë¢) (1)\n",
      "Specific to information retrieval and recommendation setting,\n",
      "the size of X can be very large, potentially in the range of mil-\n",
      "lions to billions for practical problem settings. Dot products (two\n",
      "tower, dual encoder setups, etc.) are hence commonly used. In this\n",
      "setup, we learn user and item representations as twoùëë-dimensional\n",
      "embeddings, ùëìùúÉ(ùë¢) ‚ààRùëë and ùëîùúÉ(ùë•) ‚ààRùëë. We recommend item\n",
      "ùë• to user ùë¢ with probability proportional to their inner products,\n",
      "ùúôùúÉ(ùë•,ùë¢)‚àº‚ü® ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©; recall that we obtain normalized proba-\n",
      "bility distribution with softmax, hence\n",
      "ln ùëù(ùë•|ùë¢)= ‚ü®ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©‚àíùëçùë¢ (2)\n",
      "where ùëçùë¢ is the partition function.\n",
      "2.2 Architecture\n",
      "Figure 1 shows our main architecture used in the rest of the paper.\n",
      "We highlight the main components of this architecture below.\n",
      "Overall flow . Our design decomposes retrieval into multiple\n",
      "stages. These stages run in a cascading fashion to produce the final\n",
      "top ùëò candidates. One example is illustrated in Figure 1(a), where\n",
      "an accelerator friendly algorithm, h-indexer, is used to find a large\n",
      "number of candidates (ùëò‚Ä≤= 105) out of a 100M corpus, and then a\n",
      "complex similarity function (e.g., mixture-of-logits) is used to find\n",
      "the final top ùëò (e.g., 100) candidates. We show in Section 5.2.1 that\n",
      "this design significantly improves throughput without degrading\n",
      "recall for suitably chosen values of ùëò‚Ä≤.\n",
      "Main similarity function . The hierarchical retrieval design\n",
      "enables complex neural networks to be used when modeling (user,\n",
      "item) similarities. In Section 3, we discuss one such instance of\n",
      "\n",
      "Score: 0.0117\n",
      "Chunk: Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\n",
      "3.4 Final MoL Algorithm\n",
      "Pseudocode for the final mixture-of-logits model using dot products\n",
      "as mixture components can be found in Algorithm 1, which is also\n",
      "illustrated in Figure 1(b). In practice, we use simple 2-layer MLPs\n",
      "for ùëûùë¢ùëíùëüùë¶ùëäùëíùëñùëî‚Ñéùë°ùêπùëõ , ùëñùë°ùëíùëöùëäùëíùëñùëî‚Ñéùë°ùêπùëõ , and ùëêùëüùëúùë†ùë†ùëäùëíùëñùëî‚Ñéùë°ùêπùëõ and set\n",
      "their output size to be ùëòùë¢ ¬∑ùëòùë•. We use a simple ùëêùëúùëöùëèùëñùëõùëéùë°ùëñùëúùëõùêπùëõ ,\n",
      "ùëì(ùë¢ùë§,ùë•ùë§,ùëêùë§ )= ùë¢ùë§ ¬∑ùë•ùë§ +ùëêùë§ followed by SiLU nonlinearity [7].\n",
      "Algorithm 1 Mixture of Logits (MoL) Algorithm.\n",
      "1: procedure MoL(ùëëùëéùë°ùëé,ùëûùë¢ùëíùëüùë¶,ùëò,ùëò ‚Ä≤,ùëòùë¢,ùëòùë•,ùëë,ùúè )\n",
      "2: ùë¢ùë†ùëíùëüùê∏ùëöùëèùë† ‚Üêl2Norm(ùë¢ùë†ùëíùëüùê∏ùëöùëèùëÉùëüùëúùëó (ùëûùë¢ùëíùëüùë¶))\n",
      "3: ùëñùë°ùëíùëöùê∏ùëöùëèùë† ‚Üê[] ‚ä≤ Of shape (ùëòùë•,ùëë) √óùëò‚Ä≤\n",
      "4: for ùë• ‚ààùëëùëéùë°ùëé do ‚ä≤ Cachable\n",
      "5: ùëñùë°ùëíùëöùê∏ùëöùëèùë†.ùëéùëùùëùùëíùëõùëë (l2Norm(ùëñùë°ùëíùëöùê∏ùëöùëèùëÉùëüùëúùëó (ùë•)))\n",
      "6: end for\n",
      "7: ùëêùëô ‚Üêùëöùëö(ùë¢ùë†ùëíùëüùê∏ùëöùëèùë†,ùëñùë°ùëíùëöùê∏ùëöùëèùë†.ùëüùëíùë†‚Ñéùëéùëùùëí (‚àí1,ùëë).ùë°())/ùúè\n",
      "8: ùëêùëô ‚Üêùëêùëô.ùë£ùëñùëíùë§ (ùëòùë¢,ùëò‚Ä≤,ùëòùë•).ùëùùëíùëüùëöùë¢ùë°ùëí(1,0,2).ùëüùëíùë†‚Ñéùëéùëùùëí(ùëò‚Ä≤,‚àí1)\n",
      "9: ùëîùëéùë°ùëñùëõùëîùëäùëíùëñùëî‚Ñéùë°ùë† ‚ÜêdecomposedGating(ùëëùëéùë°ùëé,ùëûùë¢ùëíùëüùë¶,ùëêùëô )\n",
      "10: ùë†ùëñùëöùëñùëôùëéùëüùëñùë°ùëñùëíùë† ‚Üê(ùëîùëéùë°ùëñùëõùëîùëäùëíùëñùëî‚Ñéùë°ùë† ¬∑ùëêùëô).ùë†ùë¢ùëö(‚àí1)\n",
      "11: return ùëéùëüùëîùë†ùëúùëüùë°(ùë†ùëñùëöùëñùëôùëéùëüùëñùë°ùëñùëíùë† )[: ùëò]\n",
      "12: end procedure\n",
      "13: procedure decomposedGating(ùëëùëéùë°ùëé,ùë¢,ùëéùëôùëôùêøùëúùëîùëñùë°ùë† )\n",
      "14: ùë¢ùëäùëíùëñùëî‚Ñéùë°ùë† ‚Üêùë¢ùë†ùëíùëüùëäùëíùëñùëî‚Ñéùë°ùêπùëõ (ùë¢)\n",
      "15: ùë•ùëäùëíùëñùëî‚Ñéùë°ùë† ‚Üêùëñùë°ùëíùëöùëäùëíùëñùëî‚Ñéùë°ùêπùëõ (ùëëùëéùë°ùëé) ‚ä≤ Cachable\n",
      "16: ùëêùëüùëúùë†ùë†ùëäùëíùëñùëî‚Ñéùë°ùë† ‚Üêùëêùëüùëúùë†ùë†ùëäùëíùëñùëî‚Ñéùë°ùêπùëõ (ùëéùëôùëôùêøùëúùëîùëñùë°ùë†)\n",
      "17: return softmax (ùëêùëúùëöùëèùëñùëõùëíùêπùëõ(\n",
      "ùë¢ùëäùëíùëñùëî‚Ñéùë°ùë†,ùë•ùëäùëíùëñùëî‚Ñéùë°ùë†,ùëêùëüùëúùë†ùë†ùëäùëíùëñùëî‚Ñéùë°ùë† )) ‚ä≤ Of shape (ùëò‚Ä≤,ùëòùë¢ ¬∑ùëòùë•)\n",
      "18: end procedure\n",
      "The time complexity for caching item-side computations in MoL\n",
      "is |X|¬∑(ùëÇ(ùëñùë°ùëíùëöùê∏ùëöùëèùëÉùëüùëúùëó )+ùëÇ(ùëñùë°ùëíùëöùëäùëíùëñùëî‚Ñéùë°ùêπùëõ )), which is negligible\n",
      "on accelerators. Assuming all 2-layer MLPs have a hidden dimen-\n",
      "sion of ùëë‚Ñé, user embedding input dimension is ùëëùë¢, and all cachable\n",
      "computation are cached, the MoL stage has a computational cost of\n",
      "ùëÇ(ùë¢ùë†ùëíùëüùê∏ùëöùëèùëÉùëüùëúùëó )+ùëÇ(ùëòùë¢ ¬∑ùëë+ùëò‚Ä≤¬∑ùëòùë¢ ¬∑ùëòùë• ¬∑ùëë)+ùëÇ(ùë¢ùë†ùëíùëüùëäùëíùëñùëî‚Ñéùë°ùêπùëõ )+\n",
      "ùëò‚Ä≤¬∑ùëÇ(ùëêùëüùëúùë†ùë†ùëäùëíùëñùëî‚Ñéùë°ùêπùëõ )+ùëò‚Ä≤¬∑ùëÇ(ùëêùëúùëöùëèùëñùëõùëíùêπùëõ)+ùëÇ(ùëò‚Ä≤¬∑ùëòùë¢ ¬∑ùëòùë•). This\n",
      "cost is dominated by the user-item cross parts: ùëÇ(ùëò‚Ä≤¬∑ùëòùë¢ ¬∑ùëòùë• ¬∑ùëë+\n",
      "ùëò‚Ä≤¬∑ùëÇ(ùëêùëüùëúùë†ùë†ùëäùëíùëñùëî‚Ñéùë°ùêπùëõ )+ùëò‚Ä≤¬∑ùëÇ(ùëêùëúùëöùëèùëñùëõùëíùêπùëõ))= ùëÇ(ùëò‚Ä≤ùëòùë¢ùëòùë•(ùëë+ùëë‚Ñé)).\n",
      "4 HIERARCHICAL RETRIEVAL AND\n",
      "INFRASTRUCTURE OPTIMIZATIONS\n",
      "We made the following optimizations to enable large-scale MoL\n",
      "\n",
      "Score: 0.0117\n",
      "Chunk: Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\n",
      "when those exist, and grid-searched parameters otherwise. Detailed\n",
      "configurations are in Appendix A. All models are implemented in\n",
      "PyTorch and are run on a single A100 80GB GPU.\n",
      "Experiment results can be found in Table 4 and Table 6. Dot\n",
      "product + SS significantly outperforms baseline (BCE) in all settings,\n",
      "confirming the importance of sampled softmax loss in retrieval\n",
      "setting. Non-dot product based methods (e.g., NeuMF and MoL)\n",
      "tend to significantly outperform dot products on datasets that are\n",
      "not extremely sparse (ML-1M, ML-20M, and Amazon Books). Dot\n",
      "products remain competitive for highly sparse datasets, e.g., Beauty\n",
      "and Games (avg < 10 interactions per item).\n",
      "We also observe that we can indeed obtain high rank ùúô(ùë¢,ùë•)\n",
      "distributions with non-dot-product methods, as shown in Table 5.\n",
      "5.1.3 Ablation Studies. We run the following ablation studies to\n",
      "understand how different components in MoL contribute to overall\n",
      "performance. We fix the baseline as the default MoL configuration,\n",
      "which differs from Table 4 and 6 in that we enabled component-level\n",
      "hypersphere embeddings for Amazon Beauty and Games datasets.\n",
      "‚Ä¢no-l2-norm: Ablate component-level hypersphere embeddings.\n",
      "‚Ä¢no-gating-dropout: Ablate dropout in gating.\n",
      "‚Ä¢50%-ùëòùë¢ √óùëòùë•: Use 50% fewer mixture components for ùëòùë¢ and ùëòùë•.\n",
      "‚Ä¢25%-negatives: Use 75% fewer negative examples.\n",
      "Results can be found in Table 7. Most components show consistent\n",
      "gains except on the sparse Amazon datasets. Those that consistently\n",
      "contribute to gains (> 2% are underlined) include component-level\n",
      "hypersphere embeddings, gating dropout, and number of negatives.\n",
      "5.2 Industrial Datasets\n",
      "We next consider a non-sequential model setup, and evaluate model\n",
      "performance on a proprietary industrial dataset against strong\n",
      "baselines. All models share the same features, labels, and losses (incl.\n",
      "number of negatives), and all non-MoL setups use similar compute\n",
      "\n",
      "Score: 0.0116\n",
      "Chunk: branching is involved, can be hard to scale on accelerators.\n",
      "Accelerating Inference for Retrieval. In the traditional dot product\n",
      "retrieval setting, or MIPS (Maximum Inner Product Search), product\n",
      "quantization and hashing techniques [10, 33] have been well studied\n",
      "and are widely used in the non-accelerator retrieval settings.\n",
      "Partitioning the item space is a complementary approach to\n",
      "speed up inference by reducing search space [8, 22, 24, 30, 47, 50, 51].\n",
      "Search space can be partitioned with clustering [ 24, 47], spatial-\n",
      "partitioning trees [22, 30], or with approaches where the partition\n",
      "strategy is learned [8, 50, 51]. This line of work can be viewed as\n",
      "alternatives to h-indexer for hierarchical retrieval settings.\n",
      "A recent line of work investigates efficient MIPS-based KNN\n",
      "retrieval on accelerators [2, 15]. There have not been significant\n",
      "work on non-MIPS setups on accelerators to date.\n",
      "\n",
      "Score: 0.0116\n",
      "Chunk: Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\n",
      "Table 6: Amazon Reviews (Beauty, Games, and Books) results on top of sequential methods.\n",
      "method hr@10 hr@50 hr@200 hr@500 mrr\n",
      "Beauty\n",
      "baseline (BCE) 0.0256 0.0743 0.1604 0.2370 0.0126\n",
      "Dot product + SS 0.0655 (+155.4%) 0.1379 (+85.5%) 0.2271 (+41.6%) 0.2914 (+23.0%) 0.0333 (+164.4%)\n",
      "MLP + SS 0.0393 (+53.4%) 0.1005 (+35.2%) 0.1864 (+16.2%) 0.2465 (+4.0%) 0.0189 (+49.8%)\n",
      "NeuMF + SS 0.0535 (+108.9%) 0.1247 (+67.8%) 0.2152 (+34.2%) 0.2818 (+18.9%) 0.0264 (+109.6%)\n",
      "DeepFM (4√ó4) + SS 0.0494 (+92.9%) 0.1271 (+71.0%) 0.2238 (+39.5%) 0.2953 (+24.6%) 0.0241 (+91.3%)\n",
      "MoL (4√ó4) + SS 0.0541 (+111.0%) 0.1288 (+73.3%) 0.2246 (+40.0%) 0.2961 (+25.0%) 0.0262 (+107.5%)\n",
      "Games\n",
      "baseline (BCE) 0.0794 0.2209 0.4217 0.5642 0.0383\n",
      "Dot product + SS 0.1246 (+57.0%) 0.2771 (+25.4%) 0.4505 (+6.8%) 0.5736 (+1.7%) 0.0600 (+56.7%)\n",
      "MLP + SS 0.1066 (+34.3%) 0.2537 (+14.9%) 0.4293 (+1.8%) 0.5582 (-1.1%) 0.0498 (+30.1%)\n",
      "NeuMF + SS 0.1175 (+48.0%) 0.2722 (+23.2%) 0.4479 (+6.2%) 0.5755 (+2.0%) 0.0559 (+46.0%)\n",
      "DeepFM (4√ó4) + SS 0.1216 (+53.2%) 0.2817 (+27.5%) 0.4690 (+11.2%) 0.5965 (+5.7%) 0.0576 (+50.5%)\n",
      "MoL (4√ó4) + SS 0.1221 (+53.9%) 0.2834 (+28.3%) 0.4713 (+11.8%) 0.5964 (+5.7%) 0.0586 (+53.2%)\n",
      "Books\n",
      "baseline (BCE) 0.0247 0.0660 0.1370 0.2079 0.0123\n",
      "Dot product + SS 0.0317 (+28.3%) 0.0814 (+23.3%) 0.1588 (+15.9%) 0.2295 (+10.4%) 0.0156 (+26.8%)\n",
      "MLP + SS 0.0297 (+20.2%) 0.0759 (+15.0%) 0.1500 (+9.5%) 0.2190 (+5.3%) 0.0144 (+17.1%)\n",
      "NeuMF + SS 0.0358 (+45.0%) 0.0871 (+32.0%) 0.1644 (+20.0%) 0.2338 (+12.5%) 0.0178 (+44.7%)\n",
      "DeepFM (8√ó8) + SS 0.0361 (+46.2%) 0.0905 (+37.1%) 0.1706 (+24.5%) 0.2414 (+16.1%) 0.0179 (+45.5%)\n",
      "MoL (8√ó8) + SS 0.0388 (+57.1%) 0.0934 (+41.5%) 0.1751 (+27.8%) 0.2479 (+19.2%) 0.0194 (+57.7%)\n",
      "Table 7: Ablation studies on top of MoL and sequential methods.\n",
      "ML-1M (HR@10) ML-20M (HR@10) Beauty (HR@200) Games (HR@200) Books (HR@200)\n",
      "MoL 0.3079 0.3114 0.2088 0.4618 0.1751\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Query: What are the bottlenecks in modern neural retrieval pipelines? ===\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\n",
      "ùúôùúÉ(ùë¢,ùë•)‚Äôs for ùë•‚Äôs that are positively associated with the user ahead\n",
      "of other ùúôùúÉ(ùë¢,ùë•‚Ä≤)‚Äôs for ùë•‚Ä≤‚ààX.\n",
      "It‚Äôs useful to formulate the similarity between user ùë¢and item ùë•\n",
      "as a probability distributionùëù(ùë•|ùë¢)[3, 48]. We do so by considering\n",
      "ùúôùúÉ(ùë•,ùë¢)as unnormalized logits and pass them through softmax:\n",
      "ùëù(ùë•|ùë¢)= ùëíùúôùúÉ (ùë•,ùë¢)\n",
      "√ç\n",
      "ùë•‚Ä≤‚ààX ùëíùúôùúÉ (ùë•‚Ä≤,ùë¢) (1)\n",
      "Specific to information retrieval and recommendation setting,\n",
      "the size of X can be very large, potentially in the range of mil-\n",
      "lions to billions for practical problem settings. Dot products (two\n",
      "tower, dual encoder setups, etc.) are hence commonly used. In this\n",
      "setup, we learn user and item representations as twoùëë-dimensional\n",
      "embeddings, ùëìùúÉ(ùë¢) ‚ààRùëë and ùëîùúÉ(ùë•) ‚ààRùëë. We recommend item\n",
      "ùë• to user ùë¢ with probability proportional to their inner products,\n",
      "ùúôùúÉ(ùë•,ùë¢)‚àº‚ü® ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©; recall that we obtain normalized proba-\n",
      "bility distribution with softmax, hence\n",
      "ln ùëù(ùë•|ùë¢)= ‚ü®ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©‚àíùëçùë¢ (2)\n",
      "where ùëçùë¢ is the partition function.\n",
      "2.2 Architecture\n",
      "Figure 1 shows our main architecture used in the rest of the paper.\n",
      "We highlight the main components of this architecture below.\n",
      "Overall flow . Our design decomposes retrieval into multiple\n",
      "stages. These stages run in a cascading fashion to produce the final\n",
      "top ùëò candidates. One example is illustrated in Figure 1(a), where\n",
      "an accelerator friendly algorithm, h-indexer, is used to find a large\n",
      "number of candidates (ùëò‚Ä≤= 105) out of a 100M corpus, and then a\n",
      "complex similarity function (e.g., mixture-of-logits) is used to find\n",
      "the final top ùëò (e.g., 100) candidates. We show in Section 5.2.1 that\n",
      "this design significantly improves throughput without degrading\n",
      "recall for suitably chosen values of ùëò‚Ä≤.\n",
      "Main similarity function . The hierarchical retrieval design\n",
      "enables complex neural networks to be used when modeling (user,\n",
      "item) similarities. In Section 3, we discuss one such instance of\n",
      "\n",
      "Score: 0.0114\n",
      "Chunk: in various works, including Factorization Machines (FMs) [31] and\n",
      "their deep neural network variants [9, 43], product-based NNs [29],\n",
      "DCNs [41], and so on. The applications of these approaches have\n",
      "been limited to CTR predictions, or ranking. We believe that this is\n",
      "largely due to the difficulty of generalizing MLPs to long tail as dis-\n",
      "cussed by Rendle et al. in [32]. Compared with Attention FMs [43]\n",
      "and PNNs [29], MoL leverages adaptive embedding compression\n",
      "and gating decomposition to significantly reduce compute and\n",
      "memory usage, and l2 norm‚Äôed features to improve generalization.\n",
      "Multi-Interest Recommendations and Short-/Long-Term Interests.\n",
      "A line of work related to interaction modeling is multi-interest\n",
      "embeddings [25, 26, 36] which aims to capture diverse preferences\n",
      "of a user over different topics and time periods. For cases where the\n",
      "embedding combiner is not user- and item- dependent, like SINE\n",
      "and SDM [26, 36], the resulting model could be more expressive\n",
      "but still necessarily low rank as we‚Äôve shown in Equation 5. Item-\n",
      "dependent approaches like MIND [25] can be viewed as a special\n",
      "form of MoL where there is exactly one item-side embedding and\n",
      "gating function takes a specific form (e.g., dot-product attention).\n",
      "Learned Discrete Structures for Retrieval. Yet another approach\n",
      "is to learn explicit structures so that retrieval can be viewed as a\n",
      "root-to-leaf traversal, beam search, or operating on top of binary\n",
      "codes directly [8, 19, 50‚Äì52]. In particular, TDM [51] first enabled\n",
      "expressive neural networks to be applied at retrieval time. These\n",
      "approaches are more sensitive to hyperparameters, and if heavy\n",
      "branching is involved, can be hard to scale on accelerators.\n",
      "Accelerating Inference for Retrieval. In the traditional dot product\n",
      "retrieval setting, or MIPS (Maximum Inner Product Search), product\n",
      "quantization and hashing techniques [10, 33] have been well studied\n",
      "and are widely used in the non-accelerator retrieval settings.\n",
      "\n",
      "Score: 0.0114\n",
      "Chunk: Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\n",
      "when those exist, and grid-searched parameters otherwise. Detailed\n",
      "configurations are in Appendix A. All models are implemented in\n",
      "PyTorch and are run on a single A100 80GB GPU.\n",
      "Experiment results can be found in Table 4 and Table 6. Dot\n",
      "product + SS significantly outperforms baseline (BCE) in all settings,\n",
      "confirming the importance of sampled softmax loss in retrieval\n",
      "setting. Non-dot product based methods (e.g., NeuMF and MoL)\n",
      "tend to significantly outperform dot products on datasets that are\n",
      "not extremely sparse (ML-1M, ML-20M, and Amazon Books). Dot\n",
      "products remain competitive for highly sparse datasets, e.g., Beauty\n",
      "and Games (avg < 10 interactions per item).\n",
      "We also observe that we can indeed obtain high rank ùúô(ùë¢,ùë•)\n",
      "distributions with non-dot-product methods, as shown in Table 5.\n",
      "5.1.3 Ablation Studies. We run the following ablation studies to\n",
      "understand how different components in MoL contribute to overall\n",
      "performance. We fix the baseline as the default MoL configuration,\n",
      "which differs from Table 4 and 6 in that we enabled component-level\n",
      "hypersphere embeddings for Amazon Beauty and Games datasets.\n",
      "‚Ä¢no-l2-norm: Ablate component-level hypersphere embeddings.\n",
      "‚Ä¢no-gating-dropout: Ablate dropout in gating.\n",
      "‚Ä¢50%-ùëòùë¢ √óùëòùë•: Use 50% fewer mixture components for ùëòùë¢ and ùëòùë•.\n",
      "‚Ä¢25%-negatives: Use 75% fewer negative examples.\n",
      "Results can be found in Table 7. Most components show consistent\n",
      "gains except on the sparse Amazon datasets. Those that consistently\n",
      "contribute to gains (> 2% are underlined) include component-level\n",
      "hypersphere embeddings, gating dropout, and number of negatives.\n",
      "5.2 Industrial Datasets\n",
      "We next consider a non-sequential model setup, and evaluate model\n",
      "performance on a proprietary industrial dataset against strong\n",
      "baselines. All models share the same features, labels, and losses (incl.\n",
      "number of negatives), and all non-MoL setups use similar compute\n",
      "\n",
      "Score: 0.0114\n",
      "Chunk: this design significantly improves throughput without degrading\n",
      "recall for suitably chosen values of ùëò‚Ä≤.\n",
      "Main similarity function . The hierarchical retrieval design\n",
      "enables complex neural networks to be used when modeling (user,\n",
      "item) similarities. In Section 3, we discuss one such instance of\n",
      "similarity functions, mixture of logits , that significantly outperforms\n",
      "dot products. When designing this architecture, we aggressively\n",
      "make intermedidate tensors available for caching (green boxes in\n",
      "Fig. 1). For instance, we can cache item-side gating weights and\n",
      "combine them cheaply with non-cachable weights at inference time.\n",
      "Figure 2: Infra efficiency in production: GPU utilization and\n",
      "peak memory scaling with serving FLOPs.\n",
      "h-indexer. We found that a simple, but highly optimized dot\n",
      "product combined with specialized top-ùëò algorithm works well for\n",
      "up to 100M corpus. This stage is co-trained with the main similarity\n",
      "function. We discuss this design in details in Section 4.1.\n",
      "3 MIXTURE OF LOGITS: AN\n",
      "ACCELERATOR-AWARE MODEL DESIGN\n",
      "In this section, we propose a new high rank similarity function,\n",
      "mixture of logits (MoL). MoL is designed for neural retrieval settings\n",
      "on accelerators. The basic idea in MoL is to parameterize ùúôùúÉ(ùë¢,ùë•)\n",
      "as an adaptive mixture of more elementary logits,\n",
      "ùúôùëÄùëúùêø(ùë•,ùë¢)=\n",
      "‚àëÔ∏Å\n",
      "ùëò\n",
      "ùúãùëò,ùúÉ(ùë•,ùë¢)ùõøùëò,ùúÉ(ùë•,ùë¢) (3)\n",
      "where ùúãùëò,ùúÉ(ùë•,ùë¢)are adaptive gating weights and ùõøùëò,ùúÉ(ùë•,ùë¢)are\n",
      "elementary logits.\n",
      "MoL achieves high rank in two ways. First, since ùõøùëò,ùúÉ(ùë•,ùë¢)in\n",
      "Equation 3 can be parameterized by any form of neural networks,\n",
      "ùúôùúÉ(ùë•,ùë¢)has arbitrarily high rank. Second, as ùúãùëò,ùúÉ(ùë•,ùë¢)takes ùë• and\n",
      "ùë¢as input, we can create a high rank matrixln ùëù(ùë•|ùë¢)by combining\n",
      "low rank similarity functions, e.g., dot products, in a cost-efficient\n",
      "fashion. For instance, using ùëògroups of dot products (of potentially\n",
      "different embedding dimensions), we can derive\n",
      "ùúôùëÄùëúùêøùëëùëúùë° ùëùùëüùëúùëëùë¢ùëêùë°ùë† (ùë•,ùë¢)=\n",
      "‚àëÔ∏Å\n",
      "ùëò\n",
      "ùúãùëò,ùúÉ(ùë•,ùë¢)‚ü®ùëìùëò,ùúÉ(ùë¢),ùëîùëò,ùúÉ(ùë•)‚ü© (4)\n",
      "Although the elementary logits ‚ü®ùëìùëò,ùúÉ(ùë¢),ùëîùëò,ùúÉ(ùë•)‚ü©themselves\n",
      "\n",
      "Score: 0.0114\n",
      "Chunk: theoretical results hold empirically as well, through best case optimization of the vectors themselves.\n",
      "We then make a practical connection to existing state-of-the-art models by creating a simple natural\n",
      "language instantiation of the theory, called LIMIT, that these models cannot solve. Our results imply\n",
      "that the community should consider how instruction-based retrieval will impact retrievers, as there\n",
      "will be combinations of top-ùëò documents cannot represent.\n",
      "12\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Query: What is the trade-off between retrieval effectiveness and efficiency on accelerators? ===\n",
      "\n",
      "Score: 0.0118\n",
      "Chunk: branching is involved, can be hard to scale on accelerators.\n",
      "Accelerating Inference for Retrieval. In the traditional dot product\n",
      "retrieval setting, or MIPS (Maximum Inner Product Search), product\n",
      "quantization and hashing techniques [10, 33] have been well studied\n",
      "and are widely used in the non-accelerator retrieval settings.\n",
      "Partitioning the item space is a complementary approach to\n",
      "speed up inference by reducing search space [8, 22, 24, 30, 47, 50, 51].\n",
      "Search space can be partitioned with clustering [ 24, 47], spatial-\n",
      "partitioning trees [22, 30], or with approaches where the partition\n",
      "strategy is learned [8, 50, 51]. This line of work can be viewed as\n",
      "alternatives to h-indexer for hierarchical retrieval settings.\n",
      "A recent line of work investigates efficient MIPS-based KNN\n",
      "retrieval on accelerators [2, 15]. There have not been significant\n",
      "work on non-MIPS setups on accelerators to date.\n",
      "\n",
      "Score: 0.0116\n",
      "Chunk: Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\n",
      "ùúôùúÉ(ùë¢,ùë•)‚Äôs for ùë•‚Äôs that are positively associated with the user ahead\n",
      "of other ùúôùúÉ(ùë¢,ùë•‚Ä≤)‚Äôs for ùë•‚Ä≤‚ààX.\n",
      "It‚Äôs useful to formulate the similarity between user ùë¢and item ùë•\n",
      "as a probability distributionùëù(ùë•|ùë¢)[3, 48]. We do so by considering\n",
      "ùúôùúÉ(ùë•,ùë¢)as unnormalized logits and pass them through softmax:\n",
      "ùëù(ùë•|ùë¢)= ùëíùúôùúÉ (ùë•,ùë¢)\n",
      "√ç\n",
      "ùë•‚Ä≤‚ààX ùëíùúôùúÉ (ùë•‚Ä≤,ùë¢) (1)\n",
      "Specific to information retrieval and recommendation setting,\n",
      "the size of X can be very large, potentially in the range of mil-\n",
      "lions to billions for practical problem settings. Dot products (two\n",
      "tower, dual encoder setups, etc.) are hence commonly used. In this\n",
      "setup, we learn user and item representations as twoùëë-dimensional\n",
      "embeddings, ùëìùúÉ(ùë¢) ‚ààRùëë and ùëîùúÉ(ùë•) ‚ààRùëë. We recommend item\n",
      "ùë• to user ùë¢ with probability proportional to their inner products,\n",
      "ùúôùúÉ(ùë•,ùë¢)‚àº‚ü® ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©; recall that we obtain normalized proba-\n",
      "bility distribution with softmax, hence\n",
      "ln ùëù(ùë•|ùë¢)= ‚ü®ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©‚àíùëçùë¢ (2)\n",
      "where ùëçùë¢ is the partition function.\n",
      "2.2 Architecture\n",
      "Figure 1 shows our main architecture used in the rest of the paper.\n",
      "We highlight the main components of this architecture below.\n",
      "Overall flow . Our design decomposes retrieval into multiple\n",
      "stages. These stages run in a cascading fashion to produce the final\n",
      "top ùëò candidates. One example is illustrated in Figure 1(a), where\n",
      "an accelerator friendly algorithm, h-indexer, is used to find a large\n",
      "number of candidates (ùëò‚Ä≤= 105) out of a 100M corpus, and then a\n",
      "complex similarity function (e.g., mixture-of-logits) is used to find\n",
      "the final top ùëò (e.g., 100) candidates. We show in Section 5.2.1 that\n",
      "this design significantly improves throughput without degrading\n",
      "recall for suitably chosen values of ùëò‚Ä≤.\n",
      "Main similarity function . The hierarchical retrieval design\n",
      "enables complex neural networks to be used when modeling (user,\n",
      "item) similarities. In Section 3, we discuss one such instance of\n",
      "\n",
      "Score: 0.0116\n",
      "Chunk: Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\n",
      "when those exist, and grid-searched parameters otherwise. Detailed\n",
      "configurations are in Appendix A. All models are implemented in\n",
      "PyTorch and are run on a single A100 80GB GPU.\n",
      "Experiment results can be found in Table 4 and Table 6. Dot\n",
      "product + SS significantly outperforms baseline (BCE) in all settings,\n",
      "confirming the importance of sampled softmax loss in retrieval\n",
      "setting. Non-dot product based methods (e.g., NeuMF and MoL)\n",
      "tend to significantly outperform dot products on datasets that are\n",
      "not extremely sparse (ML-1M, ML-20M, and Amazon Books). Dot\n",
      "products remain competitive for highly sparse datasets, e.g., Beauty\n",
      "and Games (avg < 10 interactions per item).\n",
      "We also observe that we can indeed obtain high rank ùúô(ùë¢,ùë•)\n",
      "distributions with non-dot-product methods, as shown in Table 5.\n",
      "5.1.3 Ablation Studies. We run the following ablation studies to\n",
      "understand how different components in MoL contribute to overall\n",
      "performance. We fix the baseline as the default MoL configuration,\n",
      "which differs from Table 4 and 6 in that we enabled component-level\n",
      "hypersphere embeddings for Amazon Beauty and Games datasets.\n",
      "‚Ä¢no-l2-norm: Ablate component-level hypersphere embeddings.\n",
      "‚Ä¢no-gating-dropout: Ablate dropout in gating.\n",
      "‚Ä¢50%-ùëòùë¢ √óùëòùë•: Use 50% fewer mixture components for ùëòùë¢ and ùëòùë•.\n",
      "‚Ä¢25%-negatives: Use 75% fewer negative examples.\n",
      "Results can be found in Table 7. Most components show consistent\n",
      "gains except on the sparse Amazon datasets. Those that consistently\n",
      "contribute to gains (> 2% are underlined) include component-level\n",
      "hypersphere embeddings, gating dropout, and number of negatives.\n",
      "5.2 Industrial Datasets\n",
      "We next consider a non-sequential model setup, and evaluate model\n",
      "performance on a proprietary industrial dataset against strong\n",
      "baselines. All models share the same features, labels, and losses (incl.\n",
      "number of negatives), and all non-MoL setups use similar compute\n",
      "\n",
      "Score: 0.0116\n",
      "Chunk: is designed with efficient training and inference on accelerators\n",
      "in mind. It not only significantly outperforms dot-product based\n",
      "baselines in hit rate, but also better generalizes to torso and long\n",
      "tail where there are limited training examples by utilizing gating\n",
      "designs and component-level hypersphere embeddings.\n",
      "Our second contribution is to enable efficient training and in-\n",
      "ference with expressive similarity functions (e.g., MoL), on modern\n",
      "accelerators (Section 4). We propose a hierarchical retrieval strat-\n",
      "egy, and in particular, an accelerator-friendly algorithm for very\n",
      "large corpus called h-indexer. Combined with other optimizations\n",
      "like quantization and kernel fusions, h-indexer enables us to scale\n",
      "up retrieval to hundreds of millions of items with latency compa-\n",
      "rable to MIPS setups. It‚Äôs worth remarking that, as highlighted in\n",
      "Figure 2, the serving cost, e.g., latency and throughput ‚Äî on GPUs\n",
      "in this work ‚Äî does not necessarily increase with compute thanks\n",
      "to the significantly higher GPU utilization of our model. Our hier-\n",
      "archical design, unlike dot-product, can also harness more compute\n",
      "with constant GPU memory budget, which is critical given limited\n",
      "memory capacity in production.\n",
      "Our final contribution is to conduct extensive experiments on\n",
      "public and industrial datasets (Section 5) with MoL and h-indexer.\n",
      "Our proposed method achieves up to 77.3% gain in HR on standard\n",
      "benchmark datasets like MovieLens and Amazon Reviews, and up\n",
      "to +27.6% gain at HR@50 over strong baselines on one of the largest\n",
      "recommendation system deployments at Meta. The strong perfor-\n",
      "mance of MoL validates our hypothesis that expressive similarity\n",
      "functions are needed to capture dynamics of real-world datasets at\n",
      "retrieval stage. The proposed method also reduces Matthew effect,\n",
      "showing that MoL indeed generalizes well over the entire corpus.\n",
      "We finally compare the proposed approach with related works\n",
      "in Section 6 and conclude in Section 7.\n",
      "2 OVERVIEW\n",
      "\n",
      "Score: 0.0116\n",
      "Chunk: theoretical results hold empirically as well, through best case optimization of the vectors themselves.\n",
      "We then make a practical connection to existing state-of-the-art models by creating a simple natural\n",
      "language instantiation of the theory, called LIMIT, that these models cannot solve. Our results imply\n",
      "that the community should consider how instruction-based retrieval will impact retrievers, as there\n",
      "will be combinations of top-ùëò documents cannot represent.\n",
      "12\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Query: Explain the concept of query-side latency in neural retrieval. ===\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: 1. Introduction\n",
      "Over the last two decades, information retrieval (IR) has moved from models dominated by sparse\n",
      "techniques (such as BM25 [Robertson et al., 1995]) to those that use neural language models (LM)\n",
      "as their backbones [Lee et al., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].\n",
      "These neural models are predominantly used in a single vector capacity, where they output a single\n",
      "embedding representing the entire input (also known asdense retrieval). These embedding models\n",
      "are capable of generalizing to new retrieval datasets and have been tasked with solving increasingly\n",
      "complicated retrieval problems [Thakur et al., 2021, Enevoldsen et al., 2025, Lee et al., 2025].\n",
      "In recent years this has been pushed even further with the rise of instruction-following retrieval\n",
      "benchmarks, where models are asked to representany relevance definitionfor any query[Weller\n",
      "et al., 2025a,b, Song et al., 2025, Xiao et al., 2024, Su et al., 2024]. For example, the QUEST dataset\n",
      "[Malaviya et al., 2023] uses logical operators to combine different concepts, studying the difficulty\n",
      "of retrieval for complex queries (e.g., ‚ÄúMoths or Insects or Arthropods of Guadeloupe‚Äù). On the\n",
      "other hand, datasets like BRIGHT [Su et al., 2024] explore the challenges stemming from different\n",
      "definitions of relevance by defining relevance in ways that require reasoning. One subtask includes\n",
      "reasoning over a given Leetcode problem (the query) to find other Leetcode problems that share a\n",
      "sub-task (e.g. others problems using dynamic programming). Although models cannot solve these\n",
      "benchmarks yet, the community has proposed these problems in order to push the boundaries of\n",
      "what dense retrievers are capable of‚Äîwhich is now implicitlyevery taskthat could be defined.\n",
      "Rather than proposing empirical benchmarks to gauge what embedding models can achieve, we\n",
      "seek to understand at a more fundamental level what the limitations are. Since embedding models use\n",
      "‚àóWork done during internship at GDM.\n",
      "\n",
      "Score: 0.0114\n",
      "Chunk: theoretical results hold empirically as well, through best case optimization of the vectors themselves.\n",
      "We then make a practical connection to existing state-of-the-art models by creating a simple natural\n",
      "language instantiation of the theory, called LIMIT, that these models cannot solve. Our results imply\n",
      "that the community should consider how instruction-based retrieval will impact retrievers, as there\n",
      "will be combinations of top-ùëò documents cannot represent.\n",
      "12\n",
      "\n",
      "Score: 0.0114\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "O. Weller, K. Ricci, E. Yang, A. Yates, D. Lawrie, and B. Van Durme. Rank1: Test-time compute for\n",
      "reranking in information retrieval.arXiv preprint arXiv:2502.18418, 2025b.\n",
      "C. Xiao, G. T. Hudson, and N. A. Moubayed. Rar-b: Reasoning as retrieval benchmark.arXiv preprint\n",
      "arXiv:2404.06347, 2024.\n",
      "Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa: A\n",
      "dataset for diverse, explainable multi-hop question answering.arXiv preprint arXiv:1809.09600,\n",
      "2018.\n",
      "Z. Yin and Y. Shen. On the dimensionality of word embedding.Advances in neural information\n",
      "processing systems, 31, 2018.\n",
      "P. Yu, L. Merrick, G. Nuti, and D. Campos. Arctic-embed 2.0: Multilingual retrieval without compro-\n",
      "mise. arXiv preprint arXiv:2412.04506, 2024.\n",
      "Y. Zhang, M. Li, D. Long, X. Zhang, H. Lin, B. Yang, P. Xie, A. Yang, D. Liu, J. Lin, F. Huang, and\n",
      "J. Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models.\n",
      "arXiv preprint arXiv:2506.05176, 2025.\n",
      "J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following\n",
      "evaluation for large language models.arXiv preprint arXiv:2311.07911, 2023.\n",
      "J. Zhou, Y. Zheng, W. Chen, Q. Zheng, Z. Shang, W. Zhang, R. Meng, and X. Shen. Beyond content\n",
      "relevance: Evaluating instruction following in retrieval models.ArXiv, abs/2410.23841, 2024. URL\n",
      "https://api.semanticscholar.org/CorpusID:273707185.\n",
      "7. Using the Triangle Inequality to Provide Theoretical Limits\n",
      "Itistemptingtousethetriangleinequalitytoshowthatembeddingmodelshavetheoreticallimitations.\n",
      "This is true for metric spaces, however, vector search often uses cosine similarity which operates in\n",
      "non-metric space. Thus, for realistic scenarios, we cannot use the triangle inequality to bound what\n",
      "embedding models can represent.\n",
      "8. Relationship to Order-K Voronoi Regions\n",
      "We also provide an explanation for how our results compare to Clarkson [1988] which put bounds\n",
      "\n",
      "Score: 0.0114\n",
      "Chunk: in various works, including Factorization Machines (FMs) [31] and\n",
      "their deep neural network variants [9, 43], product-based NNs [29],\n",
      "DCNs [41], and so on. The applications of these approaches have\n",
      "been limited to CTR predictions, or ranking. We believe that this is\n",
      "largely due to the difficulty of generalizing MLPs to long tail as dis-\n",
      "cussed by Rendle et al. in [32]. Compared with Attention FMs [43]\n",
      "and PNNs [29], MoL leverages adaptive embedding compression\n",
      "and gating decomposition to significantly reduce compute and\n",
      "memory usage, and l2 norm‚Äôed features to improve generalization.\n",
      "Multi-Interest Recommendations and Short-/Long-Term Interests.\n",
      "A line of work related to interaction modeling is multi-interest\n",
      "embeddings [25, 26, 36] which aims to capture diverse preferences\n",
      "of a user over different topics and time periods. For cases where the\n",
      "embedding combiner is not user- and item- dependent, like SINE\n",
      "and SDM [26, 36], the resulting model could be more expressive\n",
      "but still necessarily low rank as we‚Äôve shown in Equation 5. Item-\n",
      "dependent approaches like MIND [25] can be viewed as a special\n",
      "form of MoL where there is exactly one item-side embedding and\n",
      "gating function takes a specific form (e.g., dot-product attention).\n",
      "Learned Discrete Structures for Retrieval. Yet another approach\n",
      "is to learn explicit structures so that retrieval can be viewed as a\n",
      "root-to-leaf traversal, beam search, or operating on top of binary\n",
      "codes directly [8, 19, 50‚Äì52]. In particular, TDM [51] first enabled\n",
      "expressive neural networks to be applied at retrieval time. These\n",
      "approaches are more sensitive to hyperparameters, and if heavy\n",
      "branching is involved, can be hard to scale on accelerators.\n",
      "Accelerating Inference for Retrieval. In the traditional dot product\n",
      "retrieval setting, or MIPS (Maximum Inner Product Search), product\n",
      "quantization and hashing techniques [10, 33] have been well studied\n",
      "and are widely used in the non-accelerator retrieval settings.\n",
      "\n",
      "Score: 0.0114\n",
      "Chunk: Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\n",
      "ùúôùúÉ(ùë¢,ùë•)‚Äôs for ùë•‚Äôs that are positively associated with the user ahead\n",
      "of other ùúôùúÉ(ùë¢,ùë•‚Ä≤)‚Äôs for ùë•‚Ä≤‚ààX.\n",
      "It‚Äôs useful to formulate the similarity between user ùë¢and item ùë•\n",
      "as a probability distributionùëù(ùë•|ùë¢)[3, 48]. We do so by considering\n",
      "ùúôùúÉ(ùë•,ùë¢)as unnormalized logits and pass them through softmax:\n",
      "ùëù(ùë•|ùë¢)= ùëíùúôùúÉ (ùë•,ùë¢)\n",
      "√ç\n",
      "ùë•‚Ä≤‚ààX ùëíùúôùúÉ (ùë•‚Ä≤,ùë¢) (1)\n",
      "Specific to information retrieval and recommendation setting,\n",
      "the size of X can be very large, potentially in the range of mil-\n",
      "lions to billions for practical problem settings. Dot products (two\n",
      "tower, dual encoder setups, etc.) are hence commonly used. In this\n",
      "setup, we learn user and item representations as twoùëë-dimensional\n",
      "embeddings, ùëìùúÉ(ùë¢) ‚ààRùëë and ùëîùúÉ(ùë•) ‚ààRùëë. We recommend item\n",
      "ùë• to user ùë¢ with probability proportional to their inner products,\n",
      "ùúôùúÉ(ùë•,ùë¢)‚àº‚ü® ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©; recall that we obtain normalized proba-\n",
      "bility distribution with softmax, hence\n",
      "ln ùëù(ùë•|ùë¢)= ‚ü®ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©‚àíùëçùë¢ (2)\n",
      "where ùëçùë¢ is the partition function.\n",
      "2.2 Architecture\n",
      "Figure 1 shows our main architecture used in the rest of the paper.\n",
      "We highlight the main components of this architecture below.\n",
      "Overall flow . Our design decomposes retrieval into multiple\n",
      "stages. These stages run in a cascading fashion to produce the final\n",
      "top ùëò candidates. One example is illustrated in Figure 1(a), where\n",
      "an accelerator friendly algorithm, h-indexer, is used to find a large\n",
      "number of candidates (ùëò‚Ä≤= 105) out of a 100M corpus, and then a\n",
      "complex similarity function (e.g., mixture-of-logits) is used to find\n",
      "the final top ùëò (e.g., 100) candidates. We show in Section 5.2.1 that\n",
      "this design significantly improves throughput without degrading\n",
      "recall for suitably chosen values of ùëò‚Ä≤.\n",
      "Main similarity function . The hierarchical retrieval design\n",
      "enables complex neural networks to be used when modeling (user,\n",
      "item) similarities. In Section 3, we discuss one such instance of\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Query: What are some state-of-the-art techniques for efficient neural retrieval on GPUs or TPUs? ===\n",
      "\n",
      "Score: 0.0116\n",
      "Chunk: Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\n",
      "when those exist, and grid-searched parameters otherwise. Detailed\n",
      "configurations are in Appendix A. All models are implemented in\n",
      "PyTorch and are run on a single A100 80GB GPU.\n",
      "Experiment results can be found in Table 4 and Table 6. Dot\n",
      "product + SS significantly outperforms baseline (BCE) in all settings,\n",
      "confirming the importance of sampled softmax loss in retrieval\n",
      "setting. Non-dot product based methods (e.g., NeuMF and MoL)\n",
      "tend to significantly outperform dot products on datasets that are\n",
      "not extremely sparse (ML-1M, ML-20M, and Amazon Books). Dot\n",
      "products remain competitive for highly sparse datasets, e.g., Beauty\n",
      "and Games (avg < 10 interactions per item).\n",
      "We also observe that we can indeed obtain high rank ùúô(ùë¢,ùë•)\n",
      "distributions with non-dot-product methods, as shown in Table 5.\n",
      "5.1.3 Ablation Studies. We run the following ablation studies to\n",
      "understand how different components in MoL contribute to overall\n",
      "performance. We fix the baseline as the default MoL configuration,\n",
      "which differs from Table 4 and 6 in that we enabled component-level\n",
      "hypersphere embeddings for Amazon Beauty and Games datasets.\n",
      "‚Ä¢no-l2-norm: Ablate component-level hypersphere embeddings.\n",
      "‚Ä¢no-gating-dropout: Ablate dropout in gating.\n",
      "‚Ä¢50%-ùëòùë¢ √óùëòùë•: Use 50% fewer mixture components for ùëòùë¢ and ùëòùë•.\n",
      "‚Ä¢25%-negatives: Use 75% fewer negative examples.\n",
      "Results can be found in Table 7. Most components show consistent\n",
      "gains except on the sparse Amazon datasets. Those that consistently\n",
      "contribute to gains (> 2% are underlined) include component-level\n",
      "hypersphere embeddings, gating dropout, and number of negatives.\n",
      "5.2 Industrial Datasets\n",
      "We next consider a non-sequential model setup, and evaluate model\n",
      "performance on a proprietary industrial dataset against strong\n",
      "baselines. All models share the same features, labels, and losses (incl.\n",
      "number of negatives), and all non-MoL setups use similar compute\n",
      "\n",
      "Score: 0.0116\n",
      "Chunk: Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\n",
      "ùúôùúÉ(ùë¢,ùë•)‚Äôs for ùë•‚Äôs that are positively associated with the user ahead\n",
      "of other ùúôùúÉ(ùë¢,ùë•‚Ä≤)‚Äôs for ùë•‚Ä≤‚ààX.\n",
      "It‚Äôs useful to formulate the similarity between user ùë¢and item ùë•\n",
      "as a probability distributionùëù(ùë•|ùë¢)[3, 48]. We do so by considering\n",
      "ùúôùúÉ(ùë•,ùë¢)as unnormalized logits and pass them through softmax:\n",
      "ùëù(ùë•|ùë¢)= ùëíùúôùúÉ (ùë•,ùë¢)\n",
      "√ç\n",
      "ùë•‚Ä≤‚ààX ùëíùúôùúÉ (ùë•‚Ä≤,ùë¢) (1)\n",
      "Specific to information retrieval and recommendation setting,\n",
      "the size of X can be very large, potentially in the range of mil-\n",
      "lions to billions for practical problem settings. Dot products (two\n",
      "tower, dual encoder setups, etc.) are hence commonly used. In this\n",
      "setup, we learn user and item representations as twoùëë-dimensional\n",
      "embeddings, ùëìùúÉ(ùë¢) ‚ààRùëë and ùëîùúÉ(ùë•) ‚ààRùëë. We recommend item\n",
      "ùë• to user ùë¢ with probability proportional to their inner products,\n",
      "ùúôùúÉ(ùë•,ùë¢)‚àº‚ü® ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©; recall that we obtain normalized proba-\n",
      "bility distribution with softmax, hence\n",
      "ln ùëù(ùë•|ùë¢)= ‚ü®ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©‚àíùëçùë¢ (2)\n",
      "where ùëçùë¢ is the partition function.\n",
      "2.2 Architecture\n",
      "Figure 1 shows our main architecture used in the rest of the paper.\n",
      "We highlight the main components of this architecture below.\n",
      "Overall flow . Our design decomposes retrieval into multiple\n",
      "stages. These stages run in a cascading fashion to produce the final\n",
      "top ùëò candidates. One example is illustrated in Figure 1(a), where\n",
      "an accelerator friendly algorithm, h-indexer, is used to find a large\n",
      "number of candidates (ùëò‚Ä≤= 105) out of a 100M corpus, and then a\n",
      "complex similarity function (e.g., mixture-of-logits) is used to find\n",
      "the final top ùëò (e.g., 100) candidates. We show in Section 5.2.1 that\n",
      "this design significantly improves throughput without degrading\n",
      "recall for suitably chosen values of ùëò‚Ä≤.\n",
      "Main similarity function . The hierarchical retrieval design\n",
      "enables complex neural networks to be used when modeling (user,\n",
      "item) similarities. In Section 3, we discuss one such instance of\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: this design significantly improves throughput without degrading\n",
      "recall for suitably chosen values of ùëò‚Ä≤.\n",
      "Main similarity function . The hierarchical retrieval design\n",
      "enables complex neural networks to be used when modeling (user,\n",
      "item) similarities. In Section 3, we discuss one such instance of\n",
      "similarity functions, mixture of logits , that significantly outperforms\n",
      "dot products. When designing this architecture, we aggressively\n",
      "make intermedidate tensors available for caching (green boxes in\n",
      "Fig. 1). For instance, we can cache item-side gating weights and\n",
      "combine them cheaply with non-cachable weights at inference time.\n",
      "Figure 2: Infra efficiency in production: GPU utilization and\n",
      "peak memory scaling with serving FLOPs.\n",
      "h-indexer. We found that a simple, but highly optimized dot\n",
      "product combined with specialized top-ùëò algorithm works well for\n",
      "up to 100M corpus. This stage is co-trained with the main similarity\n",
      "function. We discuss this design in details in Section 4.1.\n",
      "3 MIXTURE OF LOGITS: AN\n",
      "ACCELERATOR-AWARE MODEL DESIGN\n",
      "In this section, we propose a new high rank similarity function,\n",
      "mixture of logits (MoL). MoL is designed for neural retrieval settings\n",
      "on accelerators. The basic idea in MoL is to parameterize ùúôùúÉ(ùë¢,ùë•)\n",
      "as an adaptive mixture of more elementary logits,\n",
      "ùúôùëÄùëúùêø(ùë•,ùë¢)=\n",
      "‚àëÔ∏Å\n",
      "ùëò\n",
      "ùúãùëò,ùúÉ(ùë•,ùë¢)ùõøùëò,ùúÉ(ùë•,ùë¢) (3)\n",
      "where ùúãùëò,ùúÉ(ùë•,ùë¢)are adaptive gating weights and ùõøùëò,ùúÉ(ùë•,ùë¢)are\n",
      "elementary logits.\n",
      "MoL achieves high rank in two ways. First, since ùõøùëò,ùúÉ(ùë•,ùë¢)in\n",
      "Equation 3 can be parameterized by any form of neural networks,\n",
      "ùúôùúÉ(ùë•,ùë¢)has arbitrarily high rank. Second, as ùúãùëò,ùúÉ(ùë•,ùë¢)takes ùë• and\n",
      "ùë¢as input, we can create a high rank matrixln ùëù(ùë•|ùë¢)by combining\n",
      "low rank similarity functions, e.g., dot products, in a cost-efficient\n",
      "fashion. For instance, using ùëògroups of dot products (of potentially\n",
      "different embedding dimensions), we can derive\n",
      "ùúôùëÄùëúùêøùëëùëúùë° ùëùùëüùëúùëëùë¢ùëêùë°ùë† (ùë•,ùë¢)=\n",
      "‚àëÔ∏Å\n",
      "ùëò\n",
      "ùúãùëò,ùúÉ(ùë•,ùë¢)‚ü®ùëìùëò,ùúÉ(ùë¢),ùëîùëò,ùúÉ(ùë•)‚ü© (4)\n",
      "Although the elementary logits ‚ü®ùëìùëò,ùúÉ(ùë¢),ùëîùëò,ùúÉ(ùë•)‚ü©themselves\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\n",
      "3.4 Final MoL Algorithm\n",
      "Pseudocode for the final mixture-of-logits model using dot products\n",
      "as mixture components can be found in Algorithm 1, which is also\n",
      "illustrated in Figure 1(b). In practice, we use simple 2-layer MLPs\n",
      "for ùëûùë¢ùëíùëüùë¶ùëäùëíùëñùëî‚Ñéùë°ùêπùëõ , ùëñùë°ùëíùëöùëäùëíùëñùëî‚Ñéùë°ùêπùëõ , and ùëêùëüùëúùë†ùë†ùëäùëíùëñùëî‚Ñéùë°ùêπùëõ and set\n",
      "their output size to be ùëòùë¢ ¬∑ùëòùë•. We use a simple ùëêùëúùëöùëèùëñùëõùëéùë°ùëñùëúùëõùêπùëõ ,\n",
      "ùëì(ùë¢ùë§,ùë•ùë§,ùëêùë§ )= ùë¢ùë§ ¬∑ùë•ùë§ +ùëêùë§ followed by SiLU nonlinearity [7].\n",
      "Algorithm 1 Mixture of Logits (MoL) Algorithm.\n",
      "1: procedure MoL(ùëëùëéùë°ùëé,ùëûùë¢ùëíùëüùë¶,ùëò,ùëò ‚Ä≤,ùëòùë¢,ùëòùë•,ùëë,ùúè )\n",
      "2: ùë¢ùë†ùëíùëüùê∏ùëöùëèùë† ‚Üêl2Norm(ùë¢ùë†ùëíùëüùê∏ùëöùëèùëÉùëüùëúùëó (ùëûùë¢ùëíùëüùë¶))\n",
      "3: ùëñùë°ùëíùëöùê∏ùëöùëèùë† ‚Üê[] ‚ä≤ Of shape (ùëòùë•,ùëë) √óùëò‚Ä≤\n",
      "4: for ùë• ‚ààùëëùëéùë°ùëé do ‚ä≤ Cachable\n",
      "5: ùëñùë°ùëíùëöùê∏ùëöùëèùë†.ùëéùëùùëùùëíùëõùëë (l2Norm(ùëñùë°ùëíùëöùê∏ùëöùëèùëÉùëüùëúùëó (ùë•)))\n",
      "6: end for\n",
      "7: ùëêùëô ‚Üêùëöùëö(ùë¢ùë†ùëíùëüùê∏ùëöùëèùë†,ùëñùë°ùëíùëöùê∏ùëöùëèùë†.ùëüùëíùë†‚Ñéùëéùëùùëí (‚àí1,ùëë).ùë°())/ùúè\n",
      "8: ùëêùëô ‚Üêùëêùëô.ùë£ùëñùëíùë§ (ùëòùë¢,ùëò‚Ä≤,ùëòùë•).ùëùùëíùëüùëöùë¢ùë°ùëí(1,0,2).ùëüùëíùë†‚Ñéùëéùëùùëí(ùëò‚Ä≤,‚àí1)\n",
      "9: ùëîùëéùë°ùëñùëõùëîùëäùëíùëñùëî‚Ñéùë°ùë† ‚ÜêdecomposedGating(ùëëùëéùë°ùëé,ùëûùë¢ùëíùëüùë¶,ùëêùëô )\n",
      "10: ùë†ùëñùëöùëñùëôùëéùëüùëñùë°ùëñùëíùë† ‚Üê(ùëîùëéùë°ùëñùëõùëîùëäùëíùëñùëî‚Ñéùë°ùë† ¬∑ùëêùëô).ùë†ùë¢ùëö(‚àí1)\n",
      "11: return ùëéùëüùëîùë†ùëúùëüùë°(ùë†ùëñùëöùëñùëôùëéùëüùëñùë°ùëñùëíùë† )[: ùëò]\n",
      "12: end procedure\n",
      "13: procedure decomposedGating(ùëëùëéùë°ùëé,ùë¢,ùëéùëôùëôùêøùëúùëîùëñùë°ùë† )\n",
      "14: ùë¢ùëäùëíùëñùëî‚Ñéùë°ùë† ‚Üêùë¢ùë†ùëíùëüùëäùëíùëñùëî‚Ñéùë°ùêπùëõ (ùë¢)\n",
      "15: ùë•ùëäùëíùëñùëî‚Ñéùë°ùë† ‚Üêùëñùë°ùëíùëöùëäùëíùëñùëî‚Ñéùë°ùêπùëõ (ùëëùëéùë°ùëé) ‚ä≤ Cachable\n",
      "16: ùëêùëüùëúùë†ùë†ùëäùëíùëñùëî‚Ñéùë°ùë† ‚Üêùëêùëüùëúùë†ùë†ùëäùëíùëñùëî‚Ñéùë°ùêπùëõ (ùëéùëôùëôùêøùëúùëîùëñùë°ùë†)\n",
      "17: return softmax (ùëêùëúùëöùëèùëñùëõùëíùêπùëõ(\n",
      "ùë¢ùëäùëíùëñùëî‚Ñéùë°ùë†,ùë•ùëäùëíùëñùëî‚Ñéùë°ùë†,ùëêùëüùëúùë†ùë†ùëäùëíùëñùëî‚Ñéùë°ùë† )) ‚ä≤ Of shape (ùëò‚Ä≤,ùëòùë¢ ¬∑ùëòùë•)\n",
      "18: end procedure\n",
      "The time complexity for caching item-side computations in MoL\n",
      "is |X|¬∑(ùëÇ(ùëñùë°ùëíùëöùê∏ùëöùëèùëÉùëüùëúùëó )+ùëÇ(ùëñùë°ùëíùëöùëäùëíùëñùëî‚Ñéùë°ùêπùëõ )), which is negligible\n",
      "on accelerators. Assuming all 2-layer MLPs have a hidden dimen-\n",
      "sion of ùëë‚Ñé, user embedding input dimension is ùëëùë¢, and all cachable\n",
      "computation are cached, the MoL stage has a computational cost of\n",
      "ùëÇ(ùë¢ùë†ùëíùëüùê∏ùëöùëèùëÉùëüùëúùëó )+ùëÇ(ùëòùë¢ ¬∑ùëë+ùëò‚Ä≤¬∑ùëòùë¢ ¬∑ùëòùë• ¬∑ùëë)+ùëÇ(ùë¢ùë†ùëíùëüùëäùëíùëñùëî‚Ñéùë°ùêπùëõ )+\n",
      "ùëò‚Ä≤¬∑ùëÇ(ùëêùëüùëúùë†ùë†ùëäùëíùëñùëî‚Ñéùë°ùêπùëõ )+ùëò‚Ä≤¬∑ùëÇ(ùëêùëúùëöùëèùëñùëõùëíùêπùëõ)+ùëÇ(ùëò‚Ä≤¬∑ùëòùë¢ ¬∑ùëòùë•). This\n",
      "cost is dominated by the user-item cross parts: ùëÇ(ùëò‚Ä≤¬∑ùëòùë¢ ¬∑ùëòùë• ¬∑ùëë+\n",
      "ùëò‚Ä≤¬∑ùëÇ(ùëêùëüùëúùë†ùë†ùëäùëíùëñùëî‚Ñéùë°ùêπùëõ )+ùëò‚Ä≤¬∑ùëÇ(ùëêùëúùëöùëèùëñùëõùëíùêπùëõ))= ùëÇ(ùëò‚Ä≤ùëòùë¢ùëòùë•(ùëë+ùëë‚Ñé)).\n",
      "4 HIERARCHICAL RETRIEVAL AND\n",
      "INFRASTRUCTURE OPTIMIZATIONS\n",
      "We made the following optimizations to enable large-scale MoL\n",
      "\n",
      "Score: 0.0115\n",
      "Chunk: Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\n",
      "Table 6: Amazon Reviews (Beauty, Games, and Books) results on top of sequential methods.\n",
      "method hr@10 hr@50 hr@200 hr@500 mrr\n",
      "Beauty\n",
      "baseline (BCE) 0.0256 0.0743 0.1604 0.2370 0.0126\n",
      "Dot product + SS 0.0655 (+155.4%) 0.1379 (+85.5%) 0.2271 (+41.6%) 0.2914 (+23.0%) 0.0333 (+164.4%)\n",
      "MLP + SS 0.0393 (+53.4%) 0.1005 (+35.2%) 0.1864 (+16.2%) 0.2465 (+4.0%) 0.0189 (+49.8%)\n",
      "NeuMF + SS 0.0535 (+108.9%) 0.1247 (+67.8%) 0.2152 (+34.2%) 0.2818 (+18.9%) 0.0264 (+109.6%)\n",
      "DeepFM (4√ó4) + SS 0.0494 (+92.9%) 0.1271 (+71.0%) 0.2238 (+39.5%) 0.2953 (+24.6%) 0.0241 (+91.3%)\n",
      "MoL (4√ó4) + SS 0.0541 (+111.0%) 0.1288 (+73.3%) 0.2246 (+40.0%) 0.2961 (+25.0%) 0.0262 (+107.5%)\n",
      "Games\n",
      "baseline (BCE) 0.0794 0.2209 0.4217 0.5642 0.0383\n",
      "Dot product + SS 0.1246 (+57.0%) 0.2771 (+25.4%) 0.4505 (+6.8%) 0.5736 (+1.7%) 0.0600 (+56.7%)\n",
      "MLP + SS 0.1066 (+34.3%) 0.2537 (+14.9%) 0.4293 (+1.8%) 0.5582 (-1.1%) 0.0498 (+30.1%)\n",
      "NeuMF + SS 0.1175 (+48.0%) 0.2722 (+23.2%) 0.4479 (+6.2%) 0.5755 (+2.0%) 0.0559 (+46.0%)\n",
      "DeepFM (4√ó4) + SS 0.1216 (+53.2%) 0.2817 (+27.5%) 0.4690 (+11.2%) 0.5965 (+5.7%) 0.0576 (+50.5%)\n",
      "MoL (4√ó4) + SS 0.1221 (+53.9%) 0.2834 (+28.3%) 0.4713 (+11.8%) 0.5964 (+5.7%) 0.0586 (+53.2%)\n",
      "Books\n",
      "baseline (BCE) 0.0247 0.0660 0.1370 0.2079 0.0123\n",
      "Dot product + SS 0.0317 (+28.3%) 0.0814 (+23.3%) 0.1588 (+15.9%) 0.2295 (+10.4%) 0.0156 (+26.8%)\n",
      "MLP + SS 0.0297 (+20.2%) 0.0759 (+15.0%) 0.1500 (+9.5%) 0.2190 (+5.3%) 0.0144 (+17.1%)\n",
      "NeuMF + SS 0.0358 (+45.0%) 0.0871 (+32.0%) 0.1644 (+20.0%) 0.2338 (+12.5%) 0.0178 (+44.7%)\n",
      "DeepFM (8√ó8) + SS 0.0361 (+46.2%) 0.0905 (+37.1%) 0.1706 (+24.5%) 0.2414 (+16.1%) 0.0179 (+45.5%)\n",
      "MoL (8√ó8) + SS 0.0388 (+57.1%) 0.0934 (+41.5%) 0.1751 (+27.8%) 0.2479 (+19.2%) 0.0194 (+57.7%)\n",
      "Table 7: Ablation studies on top of MoL and sequential methods.\n",
      "ML-1M (HR@10) ML-20M (HR@10) Beauty (HR@200) Games (HR@200) Books (HR@200)\n",
      "MoL 0.3079 0.3114 0.2088 0.4618 0.1751\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to print results\n",
    "def print_results(query: str, results: List[Tuple[float, str]]):\n",
    "    print(f\"\\n=== Query: {query} ===\")\n",
    "    for score, chunk in results:\n",
    "        print(f\"\\nScore: {score:.4f}\")\n",
    "        print(f\"Chunk: {chunk}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Run retrieval for each prompt\n",
    "for prompt in prompts:\n",
    "    results = retrieve_with_mixture_of_logits(prompt)\n",
    "    print_results(prompt, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e7a32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_cosine(query: str, top_k: int = 5) -> List[Tuple[float, str]]:\n",
    "    \"\"\"\n",
    "    Simple cosine similarity retrieval for comparison\n",
    "    \"\"\"\n",
    "    query_embedding = torch.tensor(generator.get_embedding(query))\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = torch.nn.functional.cosine_similarity(\n",
    "        chunk_embeddings, query_embedding.unsqueeze(0))\n",
    "    \n",
    "    # Get top-k results\n",
    "    top_k_values, top_k_indices = torch.topk(similarities, k=top_k)\n",
    "    \n",
    "    return [(score.item(), chunks[idx.item()]) \n",
    "            for score, idx in zip(top_k_values, top_k_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec068a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparing retrieval methods ===\n",
      "\n",
      "Mixture of Logits results:\n",
      "\n",
      "=== Query: What are the theoretical limitations of embedding-based retrieval? ===\n",
      "\n",
      "Score: 0.0119\n",
      "Chunk: On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Iftekhar Naim1 and Jinhyuk Lee1\n",
      "1Google DeepMind,2Johns Hopkins University\n",
      "Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a\n",
      "nascentriseinusingthemforreasoning,instruction-following,coding, andmore. Thesenewbenchmarks\n",
      "push embeddings to work forany queryand any notion of relevancethat could be given. While prior\n",
      "works have pointed out theoretical limitations of vector embeddings, there is a common assumption\n",
      "that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome\n",
      "with better training data and larger models. In this work, we demonstrate that we may encounter these\n",
      "theoretical limitations in realistic settings with extremely simple queries. We connect known results\n",
      "in learning theory, showing that the number of top-ùëò subsets of documents capable of being returned\n",
      "as the result of some query is limited by the dimension of the embedding. We empirically show that\n",
      "this holds true even if we restrict toùëò = 2, and directly optimize on the test set with free parameterized\n",
      "embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these\n",
      "theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple\n",
      "nature of the task. Our work shows the limits of embedding models under the existing single vector\n",
      "paradigm and calls for future research to develop methods that can resolve this fundamental limitation.\n",
      "1. Introduction\n",
      "Over the last two decades, information retrieval (IR) has moved from models dominated by sparse\n",
      "techniques (such as BM25 [Robertson et al., 1995]) to those that use neural language models (LM)\n",
      "as their backbones [Lee et al., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].\n",
      "\n",
      "Score: 0.0117\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments provide theoretical insight for the most common type of embedding model\n",
      "(single vector) they do not hold necessarily for other architectures, such as multi-vector models.\n",
      "Although we showed initial empirical results with non-single vector models, we leave it to future work\n",
      "to extend our theoretical connections to these settings.\n",
      "We also did not show theoretical results for the setting where the user allows some mistakes, e.g.\n",
      "capturing only the majority of the combinations. We leave putting a bound on this scenario to future\n",
      "work and would invite the reader to examine works like Ben-David et al. [2002].\n",
      "We have showed the theoretical connection that proves that some combinations cannot be repre-\n",
      "sented by embedding models, however, we cannot prove apriori whichtypes of combinations they\n",
      "will fail on. Thus, it is possible that there are some instruction-following or reasoning tasks they can\n",
      "solve perfectly, however,we do knowthat there exists some tasks that they will never be able to solve.\n",
      "Acknowledgments\n",
      "We thank Tanmaya Dabral, Zhongli Ding, Anthony Chen, Ming-Wei Chang, Kenton Lee, and Kristina\n",
      "Toutanova for their helpful feedback.\n",
      "References\n",
      "N. Alon, S. Moran, and A. Yehudayoff. Sign rank, vc dimension and spectral gaps. InElectronic\n",
      "Colloquium on Computational Complexity (ECCC), volume 21, page 10, 2014.\n",
      "P. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy. Llm2vec: Large\n",
      "language models are secretly powerful text encoders.arXiv preprint arXiv:2404.05961, 2024.\n",
      "S. Ben-David, N. Eiron, and H. U. Simon. Limitations of learning via embeddings in euclidean half\n",
      "spaces. Journal of Machine Learning Research, 3(Nov):441‚Äì461, 2002.\n",
      "C. Bohler, P. Cheilaris, R. Klein, C.-H. Liu, E. Papadopoulou, and M. Zavershynskyi. On the\n",
      "complexity of higher order abstract voronoi diagrams. Computational Geometry, 48(8):539‚Äì\n",
      "\n",
      "Score: 0.0117\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Our previous results show both theoretically and empirically that embedding models cannot represent\n",
      "all combinations of documents in their top-ùëòsets, making them unable to represent and solve some\n",
      "retrieval tasks. As current embedding models have grown larger (e.g. up to 4096), this has helped\n",
      "reduce negative effects for smaller dataset sizes. However, with enough combinations of top-ùëò sets\n",
      "the dimensionality would have to increase to an infeasible size for non-toy datasets.\n",
      "Thus, our results show an interesting tradeoff: embeddings can represent a large amount of\n",
      "combinations but notall combinations. Although they are useful for first stage results to a degree,\n",
      "more expressive retriever architectures will be needed. We briefly discuss some of these below.\n",
      "Cross-Encoders Although not suitable for first stage retrieval at scale, they are already typically\n",
      "used to improve first stage results. However, is LIMIT challenging for rerankers also?\n",
      "We evaluate a long context reranker, Gemini-2.5-Pro [Comanici et al., 2025] on the small setting\n",
      "as a comparison. We give Gemini all 46 documents and all 1000 queries at once, asking it to output\n",
      "the relevant documents for each query with one generation. We find that it can successfully solve\n",
      "(100%) all 1000 queries in one forward pass. This is in contrast to even the best embedding models\n",
      "with a recall@2 of less than 60% (Figure 4). Thus we can see that LIMIT is simple for state-of-the-art\n",
      "reranker models as they do not have the same limitations based on embedding dimension. However,\n",
      "they still have the limitation of being more computationally expensive than embedding models and\n",
      "thus cannot be used for first stage retrieval when there are large numbers of documents.\n",
      "Multi-vector models Multi-vector models are more expressive through the use of multiple vectors\n",
      "\n",
      "Score: 0.0117\n",
      "Chunk: setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "empirically show how the embedding dimension enables the solving of retrieval tasks. We find there\n",
      "exists a crucial point for each embedding dimension (ùëë) where the number of documents is too large\n",
      "for the embedding dimension to encode all combinations. We then gather these crucial points for a\n",
      "variety ofùëë and show that this relationship can be modeled empirically with a polynomial function.\n",
      "We also go one step further and construct a realistic but simple dataset based on these theoret-\n",
      "ical limitations (called LIMIT). Despite the simplicity of the task (e.g.,who likes Apples? and\n",
      "Jon likes Apples, ...), we find it is very difficult for even state-of-the-art embedding mod-\n",
      "els [Lee et al., 2025, Zhang et al., 2025] on MTEB [Enevoldsen et al., 2025] due to the theoretical\n",
      "underpinnings, and impossible1 for models with small embedding dimensions.\n",
      "Overall, our work contributes: (1) a theoretical basis for the fundamental limitations of embedding\n",
      "models, (2) a best-case empirical analysis showing that this proof holds for any dataset instantiation\n",
      "(by free embedding optimization), and (3) a simple real-world natural language instantiation called\n",
      "LIMIT that even state-of-the-art embedding models cannot solve.\n",
      "These results imply interesting findings for the community: on one hand we see neural embedding\n",
      "models becoming immensely successful. However, academic benchmarks test only a small amount of\n",
      "the queries that could be issued (and these queries are often overfitted to), hiding these limitations.\n",
      "Our work shows that as the tasks given to embedding models require returning ever-increasing\n",
      "combinations of top-ùëòrelevant documents (e.g., through instructions connecting previously unrelated\n",
      "1At least with current optimization techniques for retrieval.\n",
      "2\n",
      "\n",
      "Score: 0.0117\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "documents with logical operators), we will reach a limit of combinations they cannot represent.\n",
      "Thus, the community should be aware of these limitations, both when designing evaluations (as\n",
      "LIMITshows)andbychoosingalternativeretrievalapproaches‚Äìsuchascross-encoders ormulti-vector\n",
      "models ‚Äì when attempting to create models that can handle the full range of instruction-based queries,\n",
      "i.e. any query and relevance definition.\n",
      "2. Related Work\n",
      "2.1. Neural Embedding Models\n",
      "There has been immense progress on embedding models in recent years [Lee et al., 2019, Craswell\n",
      "et al., 2020, BehnamGhader et al., 2024], moving from simple web search (text-only) to advanced\n",
      "instruction-following and multi-modal representations. These models generally followed advances in\n",
      "language models, such as pre-trained LMs [Hoffmann et al., 2022], multi-modal LMs [Li et al., 2024,\n",
      "Team, 2024], and advances in instruction-following [Zhou et al., 2023, Ouyang et al., 2022]. Some\n",
      "of the prominent examples in retrieval include CoPali [Faysse et al., 2024] and DSE [Ma et al., 2024]\n",
      "which focus on multimodal embeddings, Instructor [Su et al., 2022] and FollowIR [Weller et al.,\n",
      "2024a] for instruction following, and GritLM [Muennighoff et al., 2024] and Gemini Embeddings\n",
      "[Lee et al., 2025] for pre-trained LMs turned embedders.\n",
      "Ourwork, thoughfocusedsolelyontextualrepresentationsfor simplicity, appliestoallmodalities\n",
      "of single vector embeddings for any domain of dataset. As the space of things to represent grows\n",
      "(through instructions or multi-modality) they will increasingly run into these theoretical limitations.\n",
      "2.2. Empirical tasks pushing the limits of dense retrieval\n",
      "Retrieval models have been pushed beyond their initial use cases to handle a broad variety of areas.\n",
      "Notable works include efforts to represent a wide group of domains [Thakur et al., 2021, Lee et al.,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Cosine Similarity results:\n",
      "\n",
      "=== Query: What are the theoretical limitations of embedding-based retrieval? ===\n",
      "\n",
      "Score: 0.9233\n",
      "Chunk: On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Iftekhar Naim1 and Jinhyuk Lee1\n",
      "1Google DeepMind,2Johns Hopkins University\n",
      "Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a\n",
      "nascentriseinusingthemforreasoning,instruction-following,coding, andmore. Thesenewbenchmarks\n",
      "push embeddings to work forany queryand any notion of relevancethat could be given. While prior\n",
      "works have pointed out theoretical limitations of vector embeddings, there is a common assumption\n",
      "that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome\n",
      "with better training data and larger models. In this work, we demonstrate that we may encounter these\n",
      "theoretical limitations in realistic settings with extremely simple queries. We connect known results\n",
      "in learning theory, showing that the number of top-ùëò subsets of documents capable of being returned\n",
      "as the result of some query is limited by the dimension of the embedding. We empirically show that\n",
      "this holds true even if we restrict toùëò = 2, and directly optimize on the test set with free parameterized\n",
      "embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these\n",
      "theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple\n",
      "nature of the task. Our work shows the limits of embedding models under the existing single vector\n",
      "paradigm and calls for future research to develop methods that can resolve this fundamental limitation.\n",
      "1. Introduction\n",
      "Over the last two decades, information retrieval (IR) has moved from models dominated by sparse\n",
      "techniques (such as BM25 [Robertson et al., 1995]) to those that use neural language models (LM)\n",
      "as their backbones [Lee et al., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].\n",
      "\n",
      "Score: 0.8959\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments provide theoretical insight for the most common type of embedding model\n",
      "(single vector) they do not hold necessarily for other architectures, such as multi-vector models.\n",
      "Although we showed initial empirical results with non-single vector models, we leave it to future work\n",
      "to extend our theoretical connections to these settings.\n",
      "We also did not show theoretical results for the setting where the user allows some mistakes, e.g.\n",
      "capturing only the majority of the combinations. We leave putting a bound on this scenario to future\n",
      "work and would invite the reader to examine works like Ben-David et al. [2002].\n",
      "We have showed the theoretical connection that proves that some combinations cannot be repre-\n",
      "sented by embedding models, however, we cannot prove apriori whichtypes of combinations they\n",
      "will fail on. Thus, it is possible that there are some instruction-following or reasoning tasks they can\n",
      "solve perfectly, however,we do knowthat there exists some tasks that they will never be able to solve.\n",
      "Acknowledgments\n",
      "We thank Tanmaya Dabral, Zhongli Ding, Anthony Chen, Ming-Wei Chang, Kenton Lee, and Kristina\n",
      "Toutanova for their helpful feedback.\n",
      "References\n",
      "N. Alon, S. Moran, and A. Yehudayoff. Sign rank, vc dimension and spectral gaps. InElectronic\n",
      "Colloquium on Computational Complexity (ECCC), volume 21, page 10, 2014.\n",
      "P. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy. Llm2vec: Large\n",
      "language models are secretly powerful text encoders.arXiv preprint arXiv:2404.05961, 2024.\n",
      "S. Ben-David, N. Eiron, and H. U. Simon. Limitations of learning via embeddings in euclidean half\n",
      "spaces. Journal of Machine Learning Research, 3(Nov):441‚Äì461, 2002.\n",
      "C. Bohler, P. Cheilaris, R. Klein, C.-H. Liu, E. Papadopoulou, and M. Zavershynskyi. On the\n",
      "complexity of higher order abstract voronoi diagrams. Computational Geometry, 48(8):539‚Äì\n",
      "\n",
      "Score: 0.8921\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Our previous results show both theoretically and empirically that embedding models cannot represent\n",
      "all combinations of documents in their top-ùëòsets, making them unable to represent and solve some\n",
      "retrieval tasks. As current embedding models have grown larger (e.g. up to 4096), this has helped\n",
      "reduce negative effects for smaller dataset sizes. However, with enough combinations of top-ùëò sets\n",
      "the dimensionality would have to increase to an infeasible size for non-toy datasets.\n",
      "Thus, our results show an interesting tradeoff: embeddings can represent a large amount of\n",
      "combinations but notall combinations. Although they are useful for first stage results to a degree,\n",
      "more expressive retriever architectures will be needed. We briefly discuss some of these below.\n",
      "Cross-Encoders Although not suitable for first stage retrieval at scale, they are already typically\n",
      "used to improve first stage results. However, is LIMIT challenging for rerankers also?\n",
      "We evaluate a long context reranker, Gemini-2.5-Pro [Comanici et al., 2025] on the small setting\n",
      "as a comparison. We give Gemini all 46 documents and all 1000 queries at once, asking it to output\n",
      "the relevant documents for each query with one generation. We find that it can successfully solve\n",
      "(100%) all 1000 queries in one forward pass. This is in contrast to even the best embedding models\n",
      "with a recall@2 of less than 60% (Figure 4). Thus we can see that LIMIT is simple for state-of-the-art\n",
      "reranker models as they do not have the same limitations based on embedding dimension. However,\n",
      "they still have the limitation of being more computationally expensive than embedding models and\n",
      "thus cannot be used for first stage retrieval when there are large numbers of documents.\n",
      "Multi-vector models Multi-vector models are more expressive through the use of multiple vectors\n",
      "\n",
      "Score: 0.8894\n",
      "Chunk: setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "empirically show how the embedding dimension enables the solving of retrieval tasks. We find there\n",
      "exists a crucial point for each embedding dimension (ùëë) where the number of documents is too large\n",
      "for the embedding dimension to encode all combinations. We then gather these crucial points for a\n",
      "variety ofùëë and show that this relationship can be modeled empirically with a polynomial function.\n",
      "We also go one step further and construct a realistic but simple dataset based on these theoret-\n",
      "ical limitations (called LIMIT). Despite the simplicity of the task (e.g.,who likes Apples? and\n",
      "Jon likes Apples, ...), we find it is very difficult for even state-of-the-art embedding mod-\n",
      "els [Lee et al., 2025, Zhang et al., 2025] on MTEB [Enevoldsen et al., 2025] due to the theoretical\n",
      "underpinnings, and impossible1 for models with small embedding dimensions.\n",
      "Overall, our work contributes: (1) a theoretical basis for the fundamental limitations of embedding\n",
      "models, (2) a best-case empirical analysis showing that this proof holds for any dataset instantiation\n",
      "(by free embedding optimization), and (3) a simple real-world natural language instantiation called\n",
      "LIMIT that even state-of-the-art embedding models cannot solve.\n",
      "These results imply interesting findings for the community: on one hand we see neural embedding\n",
      "models becoming immensely successful. However, academic benchmarks test only a small amount of\n",
      "the queries that could be issued (and these queries are often overfitted to), hiding these limitations.\n",
      "Our work shows that as the tasks given to embedding models require returning ever-increasing\n",
      "combinations of top-ùëòrelevant documents (e.g., through instructions connecting previously unrelated\n",
      "1At least with current optimization techniques for retrieval.\n",
      "2\n",
      "\n",
      "Score: 0.8893\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "documents with logical operators), we will reach a limit of combinations they cannot represent.\n",
      "Thus, the community should be aware of these limitations, both when designing evaluations (as\n",
      "LIMITshows)andbychoosingalternativeretrievalapproaches‚Äìsuchascross-encoders ormulti-vector\n",
      "models ‚Äì when attempting to create models that can handle the full range of instruction-based queries,\n",
      "i.e. any query and relevance definition.\n",
      "2. Related Work\n",
      "2.1. Neural Embedding Models\n",
      "There has been immense progress on embedding models in recent years [Lee et al., 2019, Craswell\n",
      "et al., 2020, BehnamGhader et al., 2024], moving from simple web search (text-only) to advanced\n",
      "instruction-following and multi-modal representations. These models generally followed advances in\n",
      "language models, such as pre-trained LMs [Hoffmann et al., 2022], multi-modal LMs [Li et al., 2024,\n",
      "Team, 2024], and advances in instruction-following [Zhou et al., 2023, Ouyang et al., 2022]. Some\n",
      "of the prominent examples in retrieval include CoPali [Faysse et al., 2024] and DSE [Ma et al., 2024]\n",
      "which focus on multimodal embeddings, Instructor [Su et al., 2022] and FollowIR [Weller et al.,\n",
      "2024a] for instruction following, and GritLM [Muennighoff et al., 2024] and Gemini Embeddings\n",
      "[Lee et al., 2025] for pre-trained LMs turned embedders.\n",
      "Ourwork, thoughfocusedsolelyontextualrepresentationsfor simplicity, appliestoallmodalities\n",
      "of single vector embeddings for any domain of dataset. As the space of things to represent grows\n",
      "(through instructions or multi-modality) they will increasingly run into these theoretical limitations.\n",
      "2.2. Empirical tasks pushing the limits of dense retrieval\n",
      "Retrieval models have been pushed beyond their initial use cases to handle a broad variety of areas.\n",
      "Notable works include efforts to represent a wide group of domains [Thakur et al., 2021, Lee et al.,\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compare both methods\n",
    "prompt = prompts[0]  # Take first prompt as example\n",
    "print(\"\\n=== Comparing retrieval methods ===\")\n",
    "print(\"\\nMixture of Logits results:\")\n",
    "mol_results = retrieve_with_mixture_of_logits(prompt)\n",
    "print_results(prompt, mol_results)\n",
    "\n",
    "print(\"\\nCosine Similarity results:\")\n",
    "cos_results = retrieve_with_cosine(prompt)\n",
    "print_results(prompt, cos_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4561695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query: What are the theoretical limitations of embedding-based retrieval? ===\n",
      "\n",
      "Mixture of Logits vs Cosine Similarity:\n",
      "Index   MoL Score    Cos Score    Same?  \n",
      "----------------------------------------\n",
      "0         0.0119       0.9233      True  \n",
      "1         0.0117       0.8959      True  \n",
      "2         0.0117       0.8921      True  \n",
      "3         0.0117       0.8894      True  \n",
      "4         0.0117       0.8893      True  \n",
      "\n",
      "Sum of top-5 MoL scores: 0.0586\n"
     ]
    }
   ],
   "source": [
    "def compare_methods_detailed(prompt: str, top_k: int = 5):\n",
    "    # Get results from both methods\n",
    "    mol_results = retrieve_with_mixture_of_logits(prompt, top_k)\n",
    "    cos_results = retrieve_with_cosine(prompt, top_k)\n",
    "    \n",
    "    print(f\"\\n=== Query: {prompt} ===\")\n",
    "    print(\"\\nMixture of Logits vs Cosine Similarity:\")\n",
    "    print(f\"{'Index':<6} {'MoL Score':^12} {'Cos Score':^12} {'Same?':^8}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Compare each position\n",
    "    for i in range(top_k):\n",
    "        mol_score, mol_chunk = mol_results[i]\n",
    "        cos_score, cos_chunk = cos_results[i]\n",
    "        is_same = mol_chunk == cos_chunk\n",
    "        print(f\"{i:<6} {mol_score:^12.4f} {cos_score:^12.4f} {str(is_same):^8}\")\n",
    "    \n",
    "    # Show sum of probabilities for MoL\n",
    "    mol_sum = sum(score for score, _ in mol_results)\n",
    "    print(f\"\\nSum of top-{top_k} MoL scores: {mol_sum:.4f}\")\n",
    "\n",
    "# Test with the first prompt\n",
    "compare_methods_detailed(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ec5e734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing different temperatures for: What are the theoretical limitations of embedding-based retrieval? ===\n",
      "Temperature 0.5: Top score = 0.0148\n",
      "Temperature 1.0: Top score = 0.0128\n",
      "Temperature 2.0: Top score = 0.0119\n",
      "Temperature 4.0: Top score = 0.0114\n"
     ]
    }
   ],
   "source": [
    "def test_temperature_effect(prompt: str, temperatures=[0.5, 1.0, 2.0, 4.0]):\n",
    "    print(f\"\\n=== Testing different temperatures for: {prompt} ===\")\n",
    "    for temp in temperatures:\n",
    "        results = retrieve_with_mixture_of_logits(prompt, temperature=temp)\n",
    "        top_score = results[0][0]\n",
    "        print(f\"Temperature {temp:3.1f}: Top score = {top_score:.4f}\")\n",
    "\n",
    "test_temperature_effect(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0380ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_reranking_retrieval(query: str, initial_k: int = 10, final_k: int = 5):\n",
    "    \"\"\"\n",
    "    Enhanced retrieval with neural reranking\n",
    "    \"\"\"\n",
    "    # Stage 1: Get initial candidates using cosine similarity\n",
    "    initial_results = retrieve_with_cosine(query, top_k=initial_k)\n",
    "    \n",
    "    # Stage 2: Prepare candidate set for reranking\n",
    "    candidate_chunks = [chunk for _, chunk in initial_results]\n",
    "    candidate_indices = [chunks.index(chunk) for chunk in candidate_chunks]\n",
    "    candidate_embeddings = chunk_embeddings[candidate_indices]\n",
    "    \n",
    "    # Stage 3: Cross-attention scoring\n",
    "    query_embedding = torch.tensor(generator.get_embedding(query))\n",
    "    \n",
    "    # Compute attention scores between all pairs\n",
    "    attention_matrix = torch.matmul(candidate_embeddings, candidate_embeddings.T)\n",
    "    \n",
    "    # Combine with query relevance\n",
    "    query_scores = torch.matmul(candidate_embeddings, query_embedding)\n",
    "    \n",
    "    # Final scoring incorporating both document-document and query-document relations\n",
    "    final_scores = torch.nn.functional.softmax(\n",
    "        (attention_matrix.mean(dim=1) + query_scores) / 2.0, \n",
    "        dim=0\n",
    "    )\n",
    "    \n",
    "    # Get top-k after reranking\n",
    "    top_k_values, top_k_indices = torch.topk(final_scores, k=final_k)\n",
    "    \n",
    "    return [(score.item(), candidate_chunks[idx.item()]) \n",
    "            for score, idx in zip(top_k_values, top_k_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f69346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Simple Vector Search ===\n",
      "\n",
      "=== Query: What are the theoretical limitations of embedding-based retrieval? ===\n",
      "\n",
      "Score: 0.9233\n",
      "Chunk: On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Iftekhar Naim1 and Jinhyuk Lee1\n",
      "1Google DeepMind,2Johns Hopkins University\n",
      "Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a\n",
      "nascentriseinusingthemforreasoning,instruction-following,coding, andmore. Thesenewbenchmarks\n",
      "push embeddings to work forany queryand any notion of relevancethat could be given. While prior\n",
      "works have pointed out theoretical limitations of vector embeddings, there is a common assumption\n",
      "that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome\n",
      "with better training data and larger models. In this work, we demonstrate that we may encounter these\n",
      "theoretical limitations in realistic settings with extremely simple queries. We connect known results\n",
      "in learning theory, showing that the number of top-ùëò subsets of documents capable of being returned\n",
      "as the result of some query is limited by the dimension of the embedding. We empirically show that\n",
      "this holds true even if we restrict toùëò = 2, and directly optimize on the test set with free parameterized\n",
      "embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these\n",
      "theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple\n",
      "nature of the task. Our work shows the limits of embedding models under the existing single vector\n",
      "paradigm and calls for future research to develop methods that can resolve this fundamental limitation.\n",
      "1. Introduction\n",
      "Over the last two decades, information retrieval (IR) has moved from models dominated by sparse\n",
      "techniques (such as BM25 [Robertson et al., 1995]) to those that use neural language models (LM)\n",
      "as their backbones [Lee et al., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].\n",
      "\n",
      "Score: 0.8959\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments provide theoretical insight for the most common type of embedding model\n",
      "(single vector) they do not hold necessarily for other architectures, such as multi-vector models.\n",
      "Although we showed initial empirical results with non-single vector models, we leave it to future work\n",
      "to extend our theoretical connections to these settings.\n",
      "We also did not show theoretical results for the setting where the user allows some mistakes, e.g.\n",
      "capturing only the majority of the combinations. We leave putting a bound on this scenario to future\n",
      "work and would invite the reader to examine works like Ben-David et al. [2002].\n",
      "We have showed the theoretical connection that proves that some combinations cannot be repre-\n",
      "sented by embedding models, however, we cannot prove apriori whichtypes of combinations they\n",
      "will fail on. Thus, it is possible that there are some instruction-following or reasoning tasks they can\n",
      "solve perfectly, however,we do knowthat there exists some tasks that they will never be able to solve.\n",
      "Acknowledgments\n",
      "We thank Tanmaya Dabral, Zhongli Ding, Anthony Chen, Ming-Wei Chang, Kenton Lee, and Kristina\n",
      "Toutanova for their helpful feedback.\n",
      "References\n",
      "N. Alon, S. Moran, and A. Yehudayoff. Sign rank, vc dimension and spectral gaps. InElectronic\n",
      "Colloquium on Computational Complexity (ECCC), volume 21, page 10, 2014.\n",
      "P. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy. Llm2vec: Large\n",
      "language models are secretly powerful text encoders.arXiv preprint arXiv:2404.05961, 2024.\n",
      "S. Ben-David, N. Eiron, and H. U. Simon. Limitations of learning via embeddings in euclidean half\n",
      "spaces. Journal of Machine Learning Research, 3(Nov):441‚Äì461, 2002.\n",
      "C. Bohler, P. Cheilaris, R. Klein, C.-H. Liu, E. Papadopoulou, and M. Zavershynskyi. On the\n",
      "complexity of higher order abstract voronoi diagrams. Computational Geometry, 48(8):539‚Äì\n",
      "\n",
      "Score: 0.8921\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Our previous results show both theoretically and empirically that embedding models cannot represent\n",
      "all combinations of documents in their top-ùëòsets, making them unable to represent and solve some\n",
      "retrieval tasks. As current embedding models have grown larger (e.g. up to 4096), this has helped\n",
      "reduce negative effects for smaller dataset sizes. However, with enough combinations of top-ùëò sets\n",
      "the dimensionality would have to increase to an infeasible size for non-toy datasets.\n",
      "Thus, our results show an interesting tradeoff: embeddings can represent a large amount of\n",
      "combinations but notall combinations. Although they are useful for first stage results to a degree,\n",
      "more expressive retriever architectures will be needed. We briefly discuss some of these below.\n",
      "Cross-Encoders Although not suitable for first stage retrieval at scale, they are already typically\n",
      "used to improve first stage results. However, is LIMIT challenging for rerankers also?\n",
      "We evaluate a long context reranker, Gemini-2.5-Pro [Comanici et al., 2025] on the small setting\n",
      "as a comparison. We give Gemini all 46 documents and all 1000 queries at once, asking it to output\n",
      "the relevant documents for each query with one generation. We find that it can successfully solve\n",
      "(100%) all 1000 queries in one forward pass. This is in contrast to even the best embedding models\n",
      "with a recall@2 of less than 60% (Figure 4). Thus we can see that LIMIT is simple for state-of-the-art\n",
      "reranker models as they do not have the same limitations based on embedding dimension. However,\n",
      "they still have the limitation of being more computationally expensive than embedding models and\n",
      "thus cannot be used for first stage retrieval when there are large numbers of documents.\n",
      "Multi-vector models Multi-vector models are more expressive through the use of multiple vectors\n",
      "\n",
      "Score: 0.8894\n",
      "Chunk: setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "empirically show how the embedding dimension enables the solving of retrieval tasks. We find there\n",
      "exists a crucial point for each embedding dimension (ùëë) where the number of documents is too large\n",
      "for the embedding dimension to encode all combinations. We then gather these crucial points for a\n",
      "variety ofùëë and show that this relationship can be modeled empirically with a polynomial function.\n",
      "We also go one step further and construct a realistic but simple dataset based on these theoret-\n",
      "ical limitations (called LIMIT). Despite the simplicity of the task (e.g.,who likes Apples? and\n",
      "Jon likes Apples, ...), we find it is very difficult for even state-of-the-art embedding mod-\n",
      "els [Lee et al., 2025, Zhang et al., 2025] on MTEB [Enevoldsen et al., 2025] due to the theoretical\n",
      "underpinnings, and impossible1 for models with small embedding dimensions.\n",
      "Overall, our work contributes: (1) a theoretical basis for the fundamental limitations of embedding\n",
      "models, (2) a best-case empirical analysis showing that this proof holds for any dataset instantiation\n",
      "(by free embedding optimization), and (3) a simple real-world natural language instantiation called\n",
      "LIMIT that even state-of-the-art embedding models cannot solve.\n",
      "These results imply interesting findings for the community: on one hand we see neural embedding\n",
      "models becoming immensely successful. However, academic benchmarks test only a small amount of\n",
      "the queries that could be issued (and these queries are often overfitted to), hiding these limitations.\n",
      "Our work shows that as the tasks given to embedding models require returning ever-increasing\n",
      "combinations of top-ùëòrelevant documents (e.g., through instructions connecting previously unrelated\n",
      "1At least with current optimization techniques for retrieval.\n",
      "2\n",
      "\n",
      "Score: 0.8893\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "documents with logical operators), we will reach a limit of combinations they cannot represent.\n",
      "Thus, the community should be aware of these limitations, both when designing evaluations (as\n",
      "LIMITshows)andbychoosingalternativeretrievalapproaches‚Äìsuchascross-encoders ormulti-vector\n",
      "models ‚Äì when attempting to create models that can handle the full range of instruction-based queries,\n",
      "i.e. any query and relevance definition.\n",
      "2. Related Work\n",
      "2.1. Neural Embedding Models\n",
      "There has been immense progress on embedding models in recent years [Lee et al., 2019, Craswell\n",
      "et al., 2020, BehnamGhader et al., 2024], moving from simple web search (text-only) to advanced\n",
      "instruction-following and multi-modal representations. These models generally followed advances in\n",
      "language models, such as pre-trained LMs [Hoffmann et al., 2022], multi-modal LMs [Li et al., 2024,\n",
      "Team, 2024], and advances in instruction-following [Zhou et al., 2023, Ouyang et al., 2022]. Some\n",
      "of the prominent examples in retrieval include CoPali [Faysse et al., 2024] and DSE [Ma et al., 2024]\n",
      "which focus on multimodal embeddings, Instructor [Su et al., 2022] and FollowIR [Weller et al.,\n",
      "2024a] for instruction following, and GritLM [Muennighoff et al., 2024] and Gemini Embeddings\n",
      "[Lee et al., 2025] for pre-trained LMs turned embedders.\n",
      "Ourwork, thoughfocusedsolelyontextualrepresentationsfor simplicity, appliestoallmodalities\n",
      "of single vector embeddings for any domain of dataset. As the space of things to represent grows\n",
      "(through instructions or multi-modality) they will increasingly run into these theoretical limitations.\n",
      "2.2. Empirical tasks pushing the limits of dense retrieval\n",
      "Retrieval models have been pushed beyond their initial use cases to handle a broad variety of areas.\n",
      "Notable works include efforts to represent a wide group of domains [Thakur et al., 2021, Lee et al.,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Neural Reranking ===\n",
      "\n",
      "=== Query: What are the theoretical limitations of embedding-based retrieval? ===\n",
      "\n",
      "Score: 0.1026\n",
      "Chunk: On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Iftekhar Naim1 and Jinhyuk Lee1\n",
      "1Google DeepMind,2Johns Hopkins University\n",
      "Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a\n",
      "nascentriseinusingthemforreasoning,instruction-following,coding, andmore. Thesenewbenchmarks\n",
      "push embeddings to work forany queryand any notion of relevancethat could be given. While prior\n",
      "works have pointed out theoretical limitations of vector embeddings, there is a common assumption\n",
      "that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome\n",
      "with better training data and larger models. In this work, we demonstrate that we may encounter these\n",
      "theoretical limitations in realistic settings with extremely simple queries. We connect known results\n",
      "in learning theory, showing that the number of top-ùëò subsets of documents capable of being returned\n",
      "as the result of some query is limited by the dimension of the embedding. We empirically show that\n",
      "this holds true even if we restrict toùëò = 2, and directly optimize on the test set with free parameterized\n",
      "embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these\n",
      "theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple\n",
      "nature of the task. Our work shows the limits of embedding models under the existing single vector\n",
      "paradigm and calls for future research to develop methods that can resolve this fundamental limitation.\n",
      "1. Introduction\n",
      "Over the last two decades, information retrieval (IR) has moved from models dominated by sparse\n",
      "techniques (such as BM25 [Robertson et al., 1995]) to those that use neural language models (LM)\n",
      "as their backbones [Lee et al., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].\n",
      "\n",
      "Score: 0.1007\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments provide theoretical insight for the most common type of embedding model\n",
      "(single vector) they do not hold necessarily for other architectures, such as multi-vector models.\n",
      "Although we showed initial empirical results with non-single vector models, we leave it to future work\n",
      "to extend our theoretical connections to these settings.\n",
      "We also did not show theoretical results for the setting where the user allows some mistakes, e.g.\n",
      "capturing only the majority of the combinations. We leave putting a bound on this scenario to future\n",
      "work and would invite the reader to examine works like Ben-David et al. [2002].\n",
      "We have showed the theoretical connection that proves that some combinations cannot be repre-\n",
      "sented by embedding models, however, we cannot prove apriori whichtypes of combinations they\n",
      "will fail on. Thus, it is possible that there are some instruction-following or reasoning tasks they can\n",
      "solve perfectly, however,we do knowthat there exists some tasks that they will never be able to solve.\n",
      "Acknowledgments\n",
      "We thank Tanmaya Dabral, Zhongli Ding, Anthony Chen, Ming-Wei Chang, Kenton Lee, and Kristina\n",
      "Toutanova for their helpful feedback.\n",
      "References\n",
      "N. Alon, S. Moran, and A. Yehudayoff. Sign rank, vc dimension and spectral gaps. InElectronic\n",
      "Colloquium on Computational Complexity (ECCC), volume 21, page 10, 2014.\n",
      "P. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy. Llm2vec: Large\n",
      "language models are secretly powerful text encoders.arXiv preprint arXiv:2404.05961, 2024.\n",
      "S. Ben-David, N. Eiron, and H. U. Simon. Limitations of learning via embeddings in euclidean half\n",
      "spaces. Journal of Machine Learning Research, 3(Nov):441‚Äì461, 2002.\n",
      "C. Bohler, P. Cheilaris, R. Klein, C.-H. Liu, E. Papadopoulou, and M. Zavershynskyi. On the\n",
      "complexity of higher order abstract voronoi diagrams. Computational Geometry, 48(8):539‚Äì\n",
      "\n",
      "Score: 0.1000\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "documents with logical operators), we will reach a limit of combinations they cannot represent.\n",
      "Thus, the community should be aware of these limitations, both when designing evaluations (as\n",
      "LIMITshows)andbychoosingalternativeretrievalapproaches‚Äìsuchascross-encoders ormulti-vector\n",
      "models ‚Äì when attempting to create models that can handle the full range of instruction-based queries,\n",
      "i.e. any query and relevance definition.\n",
      "2. Related Work\n",
      "2.1. Neural Embedding Models\n",
      "There has been immense progress on embedding models in recent years [Lee et al., 2019, Craswell\n",
      "et al., 2020, BehnamGhader et al., 2024], moving from simple web search (text-only) to advanced\n",
      "instruction-following and multi-modal representations. These models generally followed advances in\n",
      "language models, such as pre-trained LMs [Hoffmann et al., 2022], multi-modal LMs [Li et al., 2024,\n",
      "Team, 2024], and advances in instruction-following [Zhou et al., 2023, Ouyang et al., 2022]. Some\n",
      "of the prominent examples in retrieval include CoPali [Faysse et al., 2024] and DSE [Ma et al., 2024]\n",
      "which focus on multimodal embeddings, Instructor [Su et al., 2022] and FollowIR [Weller et al.,\n",
      "2024a] for instruction following, and GritLM [Muennighoff et al., 2024] and Gemini Embeddings\n",
      "[Lee et al., 2025] for pre-trained LMs turned embedders.\n",
      "Ourwork, thoughfocusedsolelyontextualrepresentationsfor simplicity, appliestoallmodalities\n",
      "of single vector embeddings for any domain of dataset. As the space of things to represent grows\n",
      "(through instructions or multi-modality) they will increasingly run into these theoretical limitations.\n",
      "2.2. Empirical tasks pushing the limits of dense retrieval\n",
      "Retrieval models have been pushed beyond their initial use cases to handle a broad variety of areas.\n",
      "Notable works include efforts to represent a wide group of domains [Thakur et al., 2021, Lee et al.,\n",
      "\n",
      "Score: 0.1000\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Our previous results show both theoretically and empirically that embedding models cannot represent\n",
      "all combinations of documents in their top-ùëòsets, making them unable to represent and solve some\n",
      "retrieval tasks. As current embedding models have grown larger (e.g. up to 4096), this has helped\n",
      "reduce negative effects for smaller dataset sizes. However, with enough combinations of top-ùëò sets\n",
      "the dimensionality would have to increase to an infeasible size for non-toy datasets.\n",
      "Thus, our results show an interesting tradeoff: embeddings can represent a large amount of\n",
      "combinations but notall combinations. Although they are useful for first stage results to a degree,\n",
      "more expressive retriever architectures will be needed. We briefly discuss some of these below.\n",
      "Cross-Encoders Although not suitable for first stage retrieval at scale, they are already typically\n",
      "used to improve first stage results. However, is LIMIT challenging for rerankers also?\n",
      "We evaluate a long context reranker, Gemini-2.5-Pro [Comanici et al., 2025] on the small setting\n",
      "as a comparison. We give Gemini all 46 documents and all 1000 queries at once, asking it to output\n",
      "the relevant documents for each query with one generation. We find that it can successfully solve\n",
      "(100%) all 1000 queries in one forward pass. This is in contrast to even the best embedding models\n",
      "with a recall@2 of less than 60% (Figure 4). Thus we can see that LIMIT is simple for state-of-the-art\n",
      "reranker models as they do not have the same limitations based on embedding dimension. However,\n",
      "they still have the limitation of being more computationally expensive than embedding models and\n",
      "thus cannot be used for first stage retrieval when there are large numbers of documents.\n",
      "Multi-vector models Multi-vector models are more expressive through the use of multiple vectors\n",
      "\n",
      "Score: 0.1000\n",
      "Chunk: setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "empirically show how the embedding dimension enables the solving of retrieval tasks. We find there\n",
      "exists a crucial point for each embedding dimension (ùëë) where the number of documents is too large\n",
      "for the embedding dimension to encode all combinations. We then gather these crucial points for a\n",
      "variety ofùëë and show that this relationship can be modeled empirically with a polynomial function.\n",
      "We also go one step further and construct a realistic but simple dataset based on these theoret-\n",
      "ical limitations (called LIMIT). Despite the simplicity of the task (e.g.,who likes Apples? and\n",
      "Jon likes Apples, ...), we find it is very difficult for even state-of-the-art embedding mod-\n",
      "els [Lee et al., 2025, Zhang et al., 2025] on MTEB [Enevoldsen et al., 2025] due to the theoretical\n",
      "underpinnings, and impossible1 for models with small embedding dimensions.\n",
      "Overall, our work contributes: (1) a theoretical basis for the fundamental limitations of embedding\n",
      "models, (2) a best-case empirical analysis showing that this proof holds for any dataset instantiation\n",
      "(by free embedding optimization), and (3) a simple real-world natural language instantiation called\n",
      "LIMIT that even state-of-the-art embedding models cannot solve.\n",
      "These results imply interesting findings for the community: on one hand we see neural embedding\n",
      "models becoming immensely successful. However, academic benchmarks test only a small amount of\n",
      "the queries that could be issued (and these queries are often overfitted to), hiding these limitations.\n",
      "Our work shows that as the tasks given to embedding models require returning ever-increasing\n",
      "combinations of top-ùëòrelevant documents (e.g., through instructions connecting previously unrelated\n",
      "1At least with current optimization techniques for retrieval.\n",
      "2\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def compare_retrieval_methods(query: str):\n",
    "    print(\"\\n=== Simple Vector Search ===\")\n",
    "    simple_results = retrieve_with_cosine(query, top_k=5)\n",
    "    print_results(query, simple_results)\n",
    "    \n",
    "    print(\"\\n=== Neural Reranking ===\")\n",
    "    neural_results = neural_reranking_retrieval(query)\n",
    "    print_results(query, neural_results)\n",
    "\n",
    "# Test with first prompt\n",
    "compare_retrieval_methods(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d85421c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Analyzing differences between retrieval methods for 10 prompts\n",
      "================================================================================\n",
      "\n",
      "Prompt 1: What are the theoretical limitations of embedding-based retrieval?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Same chunks but different ordering:\n",
      "\n",
      "Cosine order:\n",
      "1. (Score: 0.9233) On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Ift...\n",
      "2. (Score: 0.8959) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments pro...\n",
      "3. (Score: 0.8921) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Ou...\n",
      "4. (Score: 0.8894) setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "em...\n",
      "5. (Score: 0.8893) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "documents with logical operators), we wi...\n",
      "\n",
      "Neural order:\n",
      "1. (Score: 0.1026) On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Ift...\n",
      "2. (Score: 0.1007) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments pro...\n",
      "3. (Score: 0.1000) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "documents with logical operators), we wi...\n",
      "4. (Score: 0.1000) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Ou...\n",
      "5. (Score: 0.1000) setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "em...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 2: How does the geometry of the embedding space affect retrieval performance?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'On the Theoretical Limitations of Embedding-Based Retrieval\\nSplit Dim Recall@2 Recall@10 Recall@100\\nTest 32 85.5 98.4 100.0\\nTest 64 90.4 98.7 100.0\\nTest 128 93.1 99.5 99.9\\nTest 256 94.2 99.7 100.0\\nTest 384 95.6 99.6 100.0\\nTest 512 94.0 99.5 99.9\\nTest 768 96.1 99.8 100.0\\nTest 1024 96.5 99.8 100.0\\nTrain 32 0.0 0.0 0.0\\nTrain 64 0.1 0.3 2.2\\nTrain 128 0.2 0.7 3.1\\nTrain 256 0.0 0.0 0.4\\nTrain 384 1.1 2.7 8.3\\nTrain 512 0.7 2.3 9.8\\nTrain 768 0.7 2.4 9.9\\nTrain 1024 1.0 2.8 11.2\\nTable 2|Fine-tuning results in table form. See Figure 5 for the comparable plot.\\n20', 'On the Theoretical Limitations of Embedding-Based Retrieval\\nModel Dim Random Dense Cycle Disjoint\\nBM25 default 96.1 93.0 96.0 96.6\\nE5-Mistral 7B 32 1.7 0.6 1.7 2.2\\nE5-Mistral 7B 64 4.3 0.5 3.3 4.8\\nE5-Mistral 7B 128 10.3 0.9 9.1 10.5\\nE5-Mistral 7B 256 16.9 1.2 14.0 15.5\\nE5-Mistral 7B 512 26.4 2.5 24.0 26.6\\nE5-Mistral 7B 768 31.5 3.1 27.7 30.0\\nE5-Mistral 7B 1024 34.0 3.8 29.5 32.8\\nE5-Mistral 7B 2048 36.8 4.3 33.6 36.7\\nE5-Mistral 7B 3072 38.9 4.7 35.8 37.6\\nE5-Mistral 7B 4096 40.4 4.8 36.6 38.8\\nGTE-ModernColBERT default 71.1 61.8 65.3 70.1\\nGritLM 7B 32 1.5 0.6 1.9 1.5\\nGritLM 7B 64 3.6 0.6 2.9 3.9\\nGritLM 7B 128 8.0 1.6 6.3 8.4\\nGritLM 7B 256 15.8 2.0 14.4 16.0\\nGritLM 7B 512 33.7 4.5 29.5 33.8\\nGritLM 7B 768 39.0 5.6 34.4 40.1\\nGritLM 7B 1024 43.3 6.6 37.4 44.1\\nGritLM 7B 2048 55.3 9.0 49.0 55.8\\nGritLM 7B 3072 61.5 10.9 54.3 61.6\\nGritLM 7B 4096 61.8 10.4 56.6 63.2\\nPromptriever Llama3 8B 32 0.7 0.6 1.2 1.1\\nPromptriever Llama3 8B 64 2.6 1.1 2.8 2.3\\nPromptriever Llama3 8B 128 5.7 1.3 5.7 7.1\\nPromptriever Llama3 8B 256 16.2 1.7 12.6 16.3\\nPromptriever Llama3 8B 512 31.9 4.7 26.0 29.0\\nPromptriever Llama3 8B 768 37.5 8.5 33.2 37.5\\nPromptriever Llama3 8B 1024 42.3 11.8 37.5 40.5\\nPromptriever Llama3 8B 2048 52.7 14.1 49.1 53.7\\nPromptriever Llama3 8B 3072 56.6 15.8 52.9 57.4\\nPromptriever Llama3 8B 4096 62.0 19.4 58.6 63.6\\nQwen3 Embed 32 3.2 0.7 2.7 2.6\\nQwen3 Embed 64 5.4 1.1 5.0 5.7\\nQwen3 Embed 128 9.9 1.9 7.9 9.4\\nQwen3 Embed 256 14.2 2.4 11.6 12.5\\nQwen3 Embed 512 18.0 3.3 14.7 15.9\\nQwen3 Embed 768 19.5 3.5 15.5 18.0\\nQwen3 Embed 1024 20.4 3.6 16.1 18.7\\nQwen3 Embed 2048 22.3 4.1 17.2 21.4\\nQwen3 Embed 3072 21.9 4.3 17.9 21.1\\nQwen3 Embed 4096 22.7 4.5 17.8 20.9\\nGemini Embed 2 0.0 0.1 0.1 0.0\\nGemini Embed 4 0.0 0.0 0.0 0.1\\nGemini Embed 8 0.2 0.0 0.0 0.2\\nGemini Embed 16 0.2 0.0 0.2 0.1\\nGemini Embed 32 0.4 0.0 0.2 0.1\\nGemini Embed 64 0.6 0.2 0.3 0.5\\nGemini Embed 128 1.4 0.3 0.8 1.4\\nGemini Embed 256 7.1 1.2 5.8 7.4\\nGemini Embed 512 18.9 3.6 17.6 19.7\\nGemini Embed 768 33.5 7.6 31.0 34.5'}\n",
      "Only in Neural: {'On the Theoretical Limitations of Embedding-Based Retrieval\\n32 512 1024 2048 3072 4096\\nEmbed Dim\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8Score\\nRecall@2\\n32 512 1024 2048 3072 4096\\nEmbed Dim\\nRecall@10\\n32 512 1024 2048 3072 4096\\nEmbed Dim\\nRecall@100\\nE5-Mistral 7B\\nSnowflake Arctic L\\nGritLM 7B\\nPromptriever Llama3 8B\\nQwen3 Embed\\nGemini Embed\\nBM25\\nGTE-ModernColBERT\\nFigure 3|Scores on the LIMIT task. Despite the simplicity of the task we see that SOTA models\\nstruggle. We also see that the dimensionality of the model is a limiting factor and that as the\\ndimension increases, so does performance. Even multi-vector models struggle. Lexical models like\\nBM25 do very well due to their higher dimensionality. Stars indicate models trained with MRL.\\nModels We evaluate the state-of-the-art embedding models including GritLM [Muennighoff et al.,\\n2024], Qwen 3 Embeddings [Zhang et al., 2025], Promptriever [Weller et al., 2024b], Gemini\\nEmbeddings [Lee et al., 2025], Snowflake‚Äôs Arctic Embed Large v2.0 [Yu et al., 2024], and E5-Mistral\\nInstruct [Wang et al., 2022, 2023]. These models range in embedding dimension (1024 to 4096)\\nas well as in training style (instruction-based, hard negative optimized, etc.). We also evaluate\\nthree non-single vector models to show the distinction: BM25 [Robertson et al., 1995, L√π, 2024],\\ngte-ModernColBERT [Chaffin, 2025, Chaffin and Sourty, 2024], and a token-wise TF-IDF.9\\nWe show results at the full embedding dimension and also with truncated embedding dimension\\n(typically used with matryoshka learning, aka MRL [Kusupati et al., 2022]). For models not trained\\nwith MRL this will result in sub-par scores, thus, models trained with MRL are indicating with stars in\\nthe plots. However, as there are no LLMs with an embedding dimension smaller than 384, we include\\nMRL for all models to small dimensions (32) to show the impact of embedding dimensionality.\\nResults Figure 3 shows the results on the full LIMIT while Figure 4 shows the results on the small', 'On the Theoretical Limitations of Embedding-Based Retrieval\\n32 512 1024 2048 3072 4096\\nEmbed Dim\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Score\\nRecall@2\\n32 512 1024 2048 3072 4096\\nEmbed Dim\\nRecall@10\\n32 512 1024 2048 3072 4096\\nEmbed Dim\\nRecall@20\\nE5-Mistral 7B\\nSnowflake Arctic L\\nGritLM 7B\\nPromptriever Llama3 8B\\nQwen3 Embed\\nGemini Embed\\nBM25\\nGTE-ModernColBERT\\nFigure 4|Scores on the LIMIT small task (N=46) over embedding dimensions. Despite having just\\n46 documents, model struggle even with recall@10 and cannot solve the task even with recall@20.\\nmodels (although still far from solving the task) while BM25 comes close to perfect scores. Both of\\nthese alterative architectures (sparse and multi-vector) offer various trade-offs, see ¬ß5.6 for analysis.\\n5.3. Is this Domain Shift?\\n32128 256 384 512 768 1024\\nEmbed Dim\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Recall@2\\nTrained on:\\nTest\\nTrain\\nFigure 5|Training on LIMIT train does\\nnot significantly help, indicating the\\nissue is not domain shift. But models\\ncan solve it if they overfit to the test set.\\nAlthough our queries look similar to standard web search\\nqueries, we wondered whether there could be some do-\\nmain shift causing the low performance. If so, we would\\nexpect that training on a training set of similar examples\\nwould significantly improve performance. On the other\\nhand, if the task was intrinsically hard, training on the\\ntraining set would provide little help whereas training\\non the test set would allow the model to overfit to those\\ntokens (similar to the free parameterized experiments).\\nTo test this we take an off the shelf embedding model\\nand train it on either the training set (created synthetically\\nusingnon-testsetattributes)ortheofficialtestsetofLIMIT.\\nWe use lightonai/modernbert-embed-large and\\nfine-tune it on these splits, using the full dataset for in\\nbatchnegatives(excludingpositives)usingSentenceTrans-\\nformers [Reimers and Gurevych, 2019]. We show a range of dimensions by projecting the hidden'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 3: What is the 'norm concentration' phenomenon in high-dimensional spaces?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing Liu\\nTable 9: Hyperparameters used for public datasets\\nML-1M ML-20M Beauty Games Books\\nSASRec\\nembedding dim ùëë 50 256 50 50 64\\nencoder blocks ùëè 2 4 2 2 4\\nattention heads ‚Ñé 1 8 1 1 4\\ndropout rate 0.2 0.2 0.5 0.5 0.5\\nDot Product temperature 20 20 20 20 20\\nMLP\\nhidden layer size 512 512 128 128 256\\ndropout rate 0.1 0.1 0.2 0.2 0.2\\nInference FLOPs per (ùë•, ùë¢) 50.5K 256.5K 12.6K 12.6K 32.3K\\nNeuMF\\nGMF dim 32 32 32 32 32\\nMLP hidden dim 256 256 128 128 256\\nMLP output dim 64 64 64 64 64\\ndropout rate 0.1 0.1 0.2 0.2 0.1\\nfinal MLP hidden dim 256 256 128 128 256\\nInference FLOPs per (ùë•, ùë¢) 49.3K 152.3K 24.6K 24.6K 56.3K\\nFM\\nhidden dim for output layer 256 256 128 128 256\\nprediction layer dropout rate 0.2 0.2 0.3 0.3 0.2\\nInference FLOPs per (ùë•, ùë¢) 197.6K 405K 10.7K 10.7K 203.6K\\nMoL\\n(ùëòùë¢ x ùëòùë• x embedding dim) (8 x 8 x 32) (8 x 8 x 32) (4 x 4 x 32) (4 x 4 x 32) (8 x 8 x 32)\\nhidden dim for gating MLPs 128 128 128 128 128\\nhidden dim for embedding proj. MLPs 512 512 N/A N/A 512\\nitem-side embedding proj. MLP dropout rate 0.1 0.1 0.3 0.3 0.1\\ngating softmax dropout rate 0.2 0.2 - - 0.2\\ngating input dropout rate - - 0.2 0.2 -\\ncomponent-level hypersphere embeddings On On Off Off On\\nùúè 20 20 20 20 20\\nInference FLOPs per (ùë•, ùë¢) 211.7K 445.3K 12.9K 12.9K 227.6K\\nNon-cachable inference FLOPs per (ùë•, ùë¢) 32.9K 33.1K 4.4K 4.4K 21.5K'}\n",
      "Only in Neural: {'On the Theoretical Limitations of Embedding-Based Retrieval\\nK. L. Clarkson. Applications of random sampling in computational geometry, ii. InProceedings of the\\nfourth annual symposium on Computational geometry, pages 1‚Äì11, 1988.\\nG. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram,\\nD. Zhang, E. Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality,\\nlong context, and next generation agentic capabilities.arXiv preprint arXiv:2507.06261, 2025.\\nN. Craswell, B. Mitra, E. Yilmaz, D. Campos, and E. M. Voorhees. Overview of the trec 2019 deep\\nlearning track.arXiv preprint arXiv:2003.07820, 2020.\\nK. Enevoldsen, I. Chung, I. Kerboua, M. Kardos, A. Mathur, D. Stap, J. Gala, W. Siblini, D. Krzemi≈Ñski,\\nG. I. Winata, et al. Mmteb: Massive multilingual text embedding benchmark.arXiv preprint\\narXiv:2502.13595, 2025.\\nM. Faysse, H. Sibille, T. Wu, B. Omrani, G. Viaud, C. Hudelot, and P. Colombo. Colpali: Efficient\\ndocument retrieval with vision language models.arXiv preprint arXiv:2407.01449, 2024.\\nH. Hatami and P. Hatami. Structure in communication complexity and constant-cost complexity\\nclasses. arXiv preprint arXiv:2401.14623, 2024.\\nH. Hatami, P. Hatami, W. Pires, R. Tao, and R. Zhao. Lower bound methods for sign-rank and\\ntheir limitations. InApproximation, Randomization, and Combinatorial Optimization. Algorithms\\nand Techniques (APPROX/RANDOM 2022), pages 22‚Äì1. Schloss Dagstuhl‚ÄìLeibniz-Zentrum f√ºr\\nInformatik, 2022.\\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.\\nHendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models.arXiv preprint\\narXiv:2203.15556, 2022.\\nG. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, and E. Grave. Unsupervised\\ndense information retrieval with contrastive learning.arXiv preprint arXiv:2112.09118, 2021.'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 4: Can embedding models truly capture semantic similarity for all tasks?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'2.2. Empirical tasks pushing the limits of dense retrieval\\nRetrieval models have been pushed beyond their initial use cases to handle a broad variety of areas.\\nNotable works include efforts to represent a wide group of domains [Thakur et al., 2021, Lee et al.,\\n2024], a diverse set of instructions [Weller et al., 2024a, Zhou et al., 2024, Oh et al., 2024], and to\\nhandle reasoning over the queries [Xiao et al., 2024, Su et al., 2024]. This has pushed the focus of\\nembedding models from basic keyword matching to embeddings that can represent the full semantic\\nmeaning of language. As such, it is more common than ever to connect what were previously unrelated\\ndocuments into the top-ùëò relevant set,2 increasing the number of combinations that models must be\\nable to represent. This has motivated our interest in understanding the limits of what embeddings\\ncan represent, as current work expects it to handleevery task.\\nPreviousworkhasexploredempiricallythelimitsofmodels: ReimersandGurevych[2020]showed\\nthat smaller dimension embedding models have more false positives, especially with larger-scale\\ncorpora. Ormazabal et al. [2019] showed the empirical limitations of models in the cross-lingual\\nsetting and Yin and Shen [2018] showed how embedding dimensions relate to the bias-variance\\ntradeoff. In contrast, our work provides a theoretical connection between the embedding dimension\\nand the sign-rank of the query relevance (qrel) matrix, while also showing empirical limitations.\\n2.3. Theoretical Limits of Vectors in Geometric Space\\nUnderstanding and finding nearest neighbors in semantic space has a long history in mathematics\\nresearch, withearlyworksuchastheVoronoidiagrambeingstudiedasfarbackas1644andformalized\\nin 1908 [Voronoi, 1908]. The order-k version of the Voronoi diagram (i.e. the Voronoi diagram\\n2You can imagine an easy way to connect any two documents merely by using logical operators, i.e. X and Y.\\n3', 'L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text embeddings by\\nweakly-supervised contrastive pre-training.arXiv preprint arXiv:2212.03533, 2022.\\nL. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Improving text embeddings with\\nlarge language models.arXiv preprint arXiv:2401.00368, 2023.\\nB. Warner, A. Chaffin, B. Clavi√©, O. Weller, O. Hallstr√∂m, S. Taghadouini, A. Gallagher, R. Biswas,\\nF. Ladhak, T. Aarsen, et al. Smarter, better, faster, longer: A modern bidirectional encoder for fast,\\nmemory efficient, and long context finetuning and inference.arXiv preprint arXiv:2412.13663,\\n2024.\\nJ. Wei, Z. Sun, S. Papay, S. McKinney, J. Han, I. Fulford, H. W. Chung, A. T. Passos, W. Fedus, and\\nA. Glaese. Browsecomp: A simple yet challenging benchmark for browsing agents.arXiv preprint\\narXiv:2504.12516, 2025.\\nO. Weller, B. Chang, S. MacAvaney, K. Lo, A. Cohan, B. Van Durme, D. Lawrie, and L. Soldaini.\\nFollowir: Evaluating and teaching information retrieval models to follow instructions.arXiv preprint\\narXiv:2403.15246, 2024a.\\nO. Weller, B. Van Durme, D. Lawrie, A. Paranjape, Y. Zhang, and J. Hessel. Promptriever: Instruction-\\ntrained retrievers can be prompted like language models.arXiv preprint arXiv:2409.11136, 2024b.\\nO. Weller, B. Chang, E. Yang, M. Yarmohammadi, S. Barham, S. MacAvaney, A. Cohan, L. Soldaini,\\nB. Van Durme, and D. Lawrie. mfollowir: a multilingual benchmark for instruction following in\\nretrieval. arXiv preprint arXiv:2501.19264, 2025a.\\n16'}\n",
      "Only in Neural: {'setting where the vectors themselves are directly optimized with the test data. This allows us to\\nempirically show how the embedding dimension enables the solving of retrieval tasks. We find there\\nexists a crucial point for each embedding dimension (ùëë) where the number of documents is too large\\nfor the embedding dimension to encode all combinations. We then gather these crucial points for a\\nvariety ofùëë and show that this relationship can be modeled empirically with a polynomial function.\\nWe also go one step further and construct a realistic but simple dataset based on these theoret-\\nical limitations (called LIMIT). Despite the simplicity of the task (e.g.,who likes Apples? and\\nJon likes Apples, ...), we find it is very difficult for even state-of-the-art embedding mod-\\nels [Lee et al., 2025, Zhang et al., 2025] on MTEB [Enevoldsen et al., 2025] due to the theoretical\\nunderpinnings, and impossible1 for models with small embedding dimensions.\\nOverall, our work contributes: (1) a theoretical basis for the fundamental limitations of embedding\\nmodels, (2) a best-case empirical analysis showing that this proof holds for any dataset instantiation\\n(by free embedding optimization), and (3) a simple real-world natural language instantiation called\\nLIMIT that even state-of-the-art embedding models cannot solve.\\nThese results imply interesting findings for the community: on one hand we see neural embedding\\nmodels becoming immensely successful. However, academic benchmarks test only a small amount of\\nthe queries that could be issued (and these queries are often overfitted to), hiding these limitations.\\nOur work shows that as the tasks given to embedding models require returning ever-increasing\\ncombinations of top-ùëòrelevant documents (e.g., through instructions connecting previously unrelated\\n1At least with current optimization techniques for retrieval.\\n2', 'On the Theoretical Limitations of Embedding-Based Retrieval\\ndocuments with logical operators), we will reach a limit of combinations they cannot represent.\\nThus, the community should be aware of these limitations, both when designing evaluations (as\\nLIMITshows)andbychoosingalternativeretrievalapproaches‚Äìsuchascross-encoders ormulti-vector\\nmodels ‚Äì when attempting to create models that can handle the full range of instruction-based queries,\\ni.e. any query and relevance definition.\\n2. Related Work\\n2.1. Neural Embedding Models\\nThere has been immense progress on embedding models in recent years [Lee et al., 2019, Craswell\\net al., 2020, BehnamGhader et al., 2024], moving from simple web search (text-only) to advanced\\ninstruction-following and multi-modal representations. These models generally followed advances in\\nlanguage models, such as pre-trained LMs [Hoffmann et al., 2022], multi-modal LMs [Li et al., 2024,\\nTeam, 2024], and advances in instruction-following [Zhou et al., 2023, Ouyang et al., 2022]. Some\\nof the prominent examples in retrieval include CoPali [Faysse et al., 2024] and DSE [Ma et al., 2024]\\nwhich focus on multimodal embeddings, Instructor [Su et al., 2022] and FollowIR [Weller et al.,\\n2024a] for instruction following, and GritLM [Muennighoff et al., 2024] and Gemini Embeddings\\n[Lee et al., 2025] for pre-trained LMs turned embedders.\\nOurwork, thoughfocusedsolelyontextualrepresentationsfor simplicity, appliestoallmodalities\\nof single vector embeddings for any domain of dataset. As the space of things to represent grows\\n(through instructions or multi-modality) they will increasingly run into these theoretical limitations.\\n2.2. Empirical tasks pushing the limits of dense retrieval\\nRetrieval models have been pushed beyond their initial use cases to handle a broad variety of areas.\\nNotable works include efforts to represent a wide group of domains [Thakur et al., 2021, Lee et al.,'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 5: What are the failure modes for embedding-based search?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Same chunks but different ordering:\n",
      "\n",
      "Cosine order:\n",
      "1. (Score: 0.8439) On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Ift...\n",
      "2. (Score: 0.8394) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments pro...\n",
      "3. (Score: 0.8369) setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "em...\n",
      "4. (Score: 0.8322) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "32 512 1024 2048 3072 4096\n",
      "Embed Dim\n",
      "0.0...\n",
      "5. (Score: 0.8310) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Ou...\n",
      "\n",
      "Neural order:\n",
      "1. (Score: 0.1016) On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Ift...\n",
      "2. (Score: 0.1009) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments pro...\n",
      "3. (Score: 0.1007) setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "em...\n",
      "4. (Score: 0.1002) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Ou...\n",
      "5. (Score: 0.1000) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "32 512 1024 2048 3072 4096\n",
      "Embed Dim\n",
      "0.0...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 6: How can neural retrieval be optimized for hardware accelerators?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\\nTable 6: Amazon Reviews (Beauty, Games, and Books) results on top of sequential methods.\\nmethod hr@10 hr@50 hr@200 hr@500 mrr\\nBeauty\\nbaseline (BCE) 0.0256 0.0743 0.1604 0.2370 0.0126\\nDot product + SS 0.0655 (+155.4%) 0.1379 (+85.5%) 0.2271 (+41.6%) 0.2914 (+23.0%) 0.0333 (+164.4%)\\nMLP + SS 0.0393 (+53.4%) 0.1005 (+35.2%) 0.1864 (+16.2%) 0.2465 (+4.0%) 0.0189 (+49.8%)\\nNeuMF + SS 0.0535 (+108.9%) 0.1247 (+67.8%) 0.2152 (+34.2%) 0.2818 (+18.9%) 0.0264 (+109.6%)\\nDeepFM (4√ó4) + SS 0.0494 (+92.9%) 0.1271 (+71.0%) 0.2238 (+39.5%) 0.2953 (+24.6%) 0.0241 (+91.3%)\\nMoL (4√ó4) + SS 0.0541 (+111.0%) 0.1288 (+73.3%) 0.2246 (+40.0%) 0.2961 (+25.0%) 0.0262 (+107.5%)\\nGames\\nbaseline (BCE) 0.0794 0.2209 0.4217 0.5642 0.0383\\nDot product + SS 0.1246 (+57.0%) 0.2771 (+25.4%) 0.4505 (+6.8%) 0.5736 (+1.7%) 0.0600 (+56.7%)\\nMLP + SS 0.1066 (+34.3%) 0.2537 (+14.9%) 0.4293 (+1.8%) 0.5582 (-1.1%) 0.0498 (+30.1%)\\nNeuMF + SS 0.1175 (+48.0%) 0.2722 (+23.2%) 0.4479 (+6.2%) 0.5755 (+2.0%) 0.0559 (+46.0%)\\nDeepFM (4√ó4) + SS 0.1216 (+53.2%) 0.2817 (+27.5%) 0.4690 (+11.2%) 0.5965 (+5.7%) 0.0576 (+50.5%)\\nMoL (4√ó4) + SS 0.1221 (+53.9%) 0.2834 (+28.3%) 0.4713 (+11.8%) 0.5964 (+5.7%) 0.0586 (+53.2%)\\nBooks\\nbaseline (BCE) 0.0247 0.0660 0.1370 0.2079 0.0123\\nDot product + SS 0.0317 (+28.3%) 0.0814 (+23.3%) 0.1588 (+15.9%) 0.2295 (+10.4%) 0.0156 (+26.8%)\\nMLP + SS 0.0297 (+20.2%) 0.0759 (+15.0%) 0.1500 (+9.5%) 0.2190 (+5.3%) 0.0144 (+17.1%)\\nNeuMF + SS 0.0358 (+45.0%) 0.0871 (+32.0%) 0.1644 (+20.0%) 0.2338 (+12.5%) 0.0178 (+44.7%)\\nDeepFM (8√ó8) + SS 0.0361 (+46.2%) 0.0905 (+37.1%) 0.1706 (+24.5%) 0.2414 (+16.1%) 0.0179 (+45.5%)\\nMoL (8√ó8) + SS 0.0388 (+57.1%) 0.0934 (+41.5%) 0.1751 (+27.8%) 0.2479 (+19.2%) 0.0194 (+57.7%)\\nTable 7: Ablation studies on top of MoL and sequential methods.\\nML-1M (HR@10) ML-20M (HR@10) Beauty (HR@200) Games (HR@200) Books (HR@200)\\nMoL 0.3079 0.3114 0.2088 0.4618 0.1751', 'branching is involved, can be hard to scale on accelerators.\\nAccelerating Inference for Retrieval. In the traditional dot product\\nretrieval setting, or MIPS (Maximum Inner Product Search), product\\nquantization and hashing techniques [10, 33] have been well studied\\nand are widely used in the non-accelerator retrieval settings.\\nPartitioning the item space is a complementary approach to\\nspeed up inference by reducing search space [8, 22, 24, 30, 47, 50, 51].\\nSearch space can be partitioned with clustering [ 24, 47], spatial-\\npartitioning trees [22, 30], or with approaches where the partition\\nstrategy is learned [8, 50, 51]. This line of work can be viewed as\\nalternatives to h-indexer for hierarchical retrieval settings.\\nA recent line of work investigates efficient MIPS-based KNN\\nretrieval on accelerators [2, 15]. There have not been significant\\nwork on non-MIPS setups on accelerators to date.'}\n",
      "Only in Neural: {'Revisiting Neural Retrieval on Accelerators\\nJiaqi Zhai\\njiaqiz@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nZhaojie Gong\\nzhaojieg@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nYueming Wang\\nyuemingw@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nXiao Sun\\nsunx@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nZheng Yan\\nzyan@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nFu Li\\nleaf123@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nXing Liu\\nxingl@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nABSTRACT\\nRetrieval finds a small number of relevant candidates from a large\\ncorpus for information retrieval and recommendation applications.\\nA key component of retrieval is to model (user, item) similarity,\\nwhich is commonly represented as the dot product of two learned\\nembeddings. This formulation permits efficient inference, com-\\nmonly known as Maximum Inner Product Search (MIPS). Despite\\nits popularity, dot products cannot capture complex user-item in-\\nteractions, which are multifaceted and likely high rank. We hence\\nexamine non-dot-product retrieval settings on accelerators, and\\npropose mixture of logits (MoL), which models (user, item) similarity\\nas an adaptive composition of elementary similarity functions. This\\nnew formulation is expressive, capable of modeling high rank (user,\\nitem) interactions, and further generalizes to the long tail. When\\ncombined with a hierarchical retrieval strategy, h-indexer, we are\\nable to scale up MoL to 100M corpus on a single GPU with latency\\ncomparable to MIPS baselines. On public datasets, our approach\\nleads to uplifts of up to 77.3% in hit rate (HR). Experiments on a\\nlarge recommendation surface at Meta showed strong metric gains\\nand reduced popularity bias, validating the proposed approach‚Äôs\\nperformance and improved generalization.\\nCCS CONCEPTS\\n‚Ä¢ Information systems ‚ÜíLearning to rank ; Recommender\\nsystems; ‚Ä¢ Computing methodologies ‚ÜíMachine learning\\nalgorithms.\\nKEYWORDS', 'this design significantly improves throughput without degrading\\nrecall for suitably chosen values of ùëò‚Ä≤.\\nMain similarity function . The hierarchical retrieval design\\nenables complex neural networks to be used when modeling (user,\\nitem) similarities. In Section 3, we discuss one such instance of\\nsimilarity functions, mixture of logits , that significantly outperforms\\ndot products. When designing this architecture, we aggressively\\nmake intermedidate tensors available for caching (green boxes in\\nFig. 1). For instance, we can cache item-side gating weights and\\ncombine them cheaply with non-cachable weights at inference time.\\nFigure 2: Infra efficiency in production: GPU utilization and\\npeak memory scaling with serving FLOPs.\\nh-indexer. We found that a simple, but highly optimized dot\\nproduct combined with specialized top-ùëò algorithm works well for\\nup to 100M corpus. This stage is co-trained with the main similarity\\nfunction. We discuss this design in details in Section 4.1.\\n3 MIXTURE OF LOGITS: AN\\nACCELERATOR-AWARE MODEL DESIGN\\nIn this section, we propose a new high rank similarity function,\\nmixture of logits (MoL). MoL is designed for neural retrieval settings\\non accelerators. The basic idea in MoL is to parameterize ùúôùúÉ(ùë¢,ùë•)\\nas an adaptive mixture of more elementary logits,\\nùúôùëÄùëúùêø(ùë•,ùë¢)=\\n‚àëÔ∏Å\\nùëò\\nùúãùëò,ùúÉ(ùë•,ùë¢)ùõøùëò,ùúÉ(ùë•,ùë¢) (3)\\nwhere ùúãùëò,ùúÉ(ùë•,ùë¢)are adaptive gating weights and ùõøùëò,ùúÉ(ùë•,ùë¢)are\\nelementary logits.\\nMoL achieves high rank in two ways. First, since ùõøùëò,ùúÉ(ùë•,ùë¢)in\\nEquation 3 can be parameterized by any form of neural networks,\\nùúôùúÉ(ùë•,ùë¢)has arbitrarily high rank. Second, as ùúãùëò,ùúÉ(ùë•,ùë¢)takes ùë• and\\nùë¢as input, we can create a high rank matrixln ùëù(ùë•|ùë¢)by combining\\nlow rank similarity functions, e.g., dot products, in a cost-efficient\\nfashion. For instance, using ùëògroups of dot products (of potentially\\ndifferent embedding dimensions), we can derive\\nùúôùëÄùëúùêøùëëùëúùë° ùëùùëüùëúùëëùë¢ùëêùë°ùë† (ùë•,ùë¢)=\\n‚àëÔ∏Å\\nùëò\\nùúãùëò,ùúÉ(ùë•,ùë¢)‚ü®ùëìùëò,ùúÉ(ùë¢),ùëîùëò,ùúÉ(ùë•)‚ü© (4)\\nAlthough the elementary logits ‚ü®ùëìùëò,ùúÉ(ùë¢),ùëîùëò,ùúÉ(ùë•)‚ü©themselves'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 7: What are the bottlenecks in modern neural retrieval pipelines?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'theoretical results hold empirically as well, through best case optimization of the vectors themselves.\\nWe then make a practical connection to existing state-of-the-art models by creating a simple natural\\nlanguage instantiation of the theory, called LIMIT, that these models cannot solve. Our results imply\\nthat the community should consider how instruction-based retrieval will impact retrievers, as there\\nwill be combinations of top-ùëò documents cannot represent.\\n12'}\n",
      "Only in Neural: {'Revisiting Neural Retrieval on Accelerators\\nJiaqi Zhai\\njiaqiz@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nZhaojie Gong\\nzhaojieg@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nYueming Wang\\nyuemingw@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nXiao Sun\\nsunx@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nZheng Yan\\nzyan@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nFu Li\\nleaf123@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nXing Liu\\nxingl@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nABSTRACT\\nRetrieval finds a small number of relevant candidates from a large\\ncorpus for information retrieval and recommendation applications.\\nA key component of retrieval is to model (user, item) similarity,\\nwhich is commonly represented as the dot product of two learned\\nembeddings. This formulation permits efficient inference, com-\\nmonly known as Maximum Inner Product Search (MIPS). Despite\\nits popularity, dot products cannot capture complex user-item in-\\nteractions, which are multifaceted and likely high rank. We hence\\nexamine non-dot-product retrieval settings on accelerators, and\\npropose mixture of logits (MoL), which models (user, item) similarity\\nas an adaptive composition of elementary similarity functions. This\\nnew formulation is expressive, capable of modeling high rank (user,\\nitem) interactions, and further generalizes to the long tail. When\\ncombined with a hierarchical retrieval strategy, h-indexer, we are\\nable to scale up MoL to 100M corpus on a single GPU with latency\\ncomparable to MIPS baselines. On public datasets, our approach\\nleads to uplifts of up to 77.3% in hit rate (HR). Experiments on a\\nlarge recommendation surface at Meta showed strong metric gains\\nand reduced popularity bias, validating the proposed approach‚Äôs\\nperformance and improved generalization.\\nCCS CONCEPTS\\n‚Ä¢ Information systems ‚ÜíLearning to rank ; Recommender\\nsystems; ‚Ä¢ Computing methodologies ‚ÜíMachine learning\\nalgorithms.\\nKEYWORDS'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 8: What is the trade-off between retrieval effectiveness and efficiency on accelerators?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'theoretical results hold empirically as well, through best case optimization of the vectors themselves.\\nWe then make a practical connection to existing state-of-the-art models by creating a simple natural\\nlanguage instantiation of the theory, called LIMIT, that these models cannot solve. Our results imply\\nthat the community should consider how instruction-based retrieval will impact retrievers, as there\\nwill be combinations of top-ùëò documents cannot represent.\\n12'}\n",
      "Only in Neural: {'Revisiting Neural Retrieval on Accelerators\\nJiaqi Zhai\\njiaqiz@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nZhaojie Gong\\nzhaojieg@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nYueming Wang\\nyuemingw@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nXiao Sun\\nsunx@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nZheng Yan\\nzyan@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nFu Li\\nleaf123@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nXing Liu\\nxingl@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nABSTRACT\\nRetrieval finds a small number of relevant candidates from a large\\ncorpus for information retrieval and recommendation applications.\\nA key component of retrieval is to model (user, item) similarity,\\nwhich is commonly represented as the dot product of two learned\\nembeddings. This formulation permits efficient inference, com-\\nmonly known as Maximum Inner Product Search (MIPS). Despite\\nits popularity, dot products cannot capture complex user-item in-\\nteractions, which are multifaceted and likely high rank. We hence\\nexamine non-dot-product retrieval settings on accelerators, and\\npropose mixture of logits (MoL), which models (user, item) similarity\\nas an adaptive composition of elementary similarity functions. This\\nnew formulation is expressive, capable of modeling high rank (user,\\nitem) interactions, and further generalizes to the long tail. When\\ncombined with a hierarchical retrieval strategy, h-indexer, we are\\nable to scale up MoL to 100M corpus on a single GPU with latency\\ncomparable to MIPS baselines. On public datasets, our approach\\nleads to uplifts of up to 77.3% in hit rate (HR). Experiments on a\\nlarge recommendation surface at Meta showed strong metric gains\\nand reduced popularity bias, validating the proposed approach‚Äôs\\nperformance and improved generalization.\\nCCS CONCEPTS\\n‚Ä¢ Information systems ‚ÜíLearning to rank ; Recommender\\nsystems; ‚Ä¢ Computing methodologies ‚ÜíMachine learning\\nalgorithms.\\nKEYWORDS'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 9: Explain the concept of query-side latency in neural retrieval.\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'theoretical results hold empirically as well, through best case optimization of the vectors themselves.\\nWe then make a practical connection to existing state-of-the-art models by creating a simple natural\\nlanguage instantiation of the theory, called LIMIT, that these models cannot solve. Our results imply\\nthat the community should consider how instruction-based retrieval will impact retrievers, as there\\nwill be combinations of top-ùëò documents cannot represent.\\n12', 'Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\\nùúôùúÉ(ùë¢,ùë•)‚Äôs for ùë•‚Äôs that are positively associated with the user ahead\\nof other ùúôùúÉ(ùë¢,ùë•‚Ä≤)‚Äôs for ùë•‚Ä≤‚ààX.\\nIt‚Äôs useful to formulate the similarity between user ùë¢and item ùë•\\nas a probability distributionùëù(ùë•|ùë¢)[3, 48]. We do so by considering\\nùúôùúÉ(ùë•,ùë¢)as unnormalized logits and pass them through softmax:\\nùëù(ùë•|ùë¢)= ùëíùúôùúÉ (ùë•,ùë¢)\\n√ç\\nùë•‚Ä≤‚ààX ùëíùúôùúÉ (ùë•‚Ä≤,ùë¢) (1)\\nSpecific to information retrieval and recommendation setting,\\nthe size of X can be very large, potentially in the range of mil-\\nlions to billions for practical problem settings. Dot products (two\\ntower, dual encoder setups, etc.) are hence commonly used. In this\\nsetup, we learn user and item representations as twoùëë-dimensional\\nembeddings, ùëìùúÉ(ùë¢) ‚ààRùëë and ùëîùúÉ(ùë•) ‚ààRùëë. We recommend item\\nùë• to user ùë¢ with probability proportional to their inner products,\\nùúôùúÉ(ùë•,ùë¢)‚àº‚ü® ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©; recall that we obtain normalized proba-\\nbility distribution with softmax, hence\\nln ùëù(ùë•|ùë¢)= ‚ü®ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©‚àíùëçùë¢ (2)\\nwhere ùëçùë¢ is the partition function.\\n2.2 Architecture\\nFigure 1 shows our main architecture used in the rest of the paper.\\nWe highlight the main components of this architecture below.\\nOverall flow . Our design decomposes retrieval into multiple\\nstages. These stages run in a cascading fashion to produce the final\\ntop ùëò candidates. One example is illustrated in Figure 1(a), where\\nan accelerator friendly algorithm, h-indexer, is used to find a large\\nnumber of candidates (ùëò‚Ä≤= 105) out of a 100M corpus, and then a\\ncomplex similarity function (e.g., mixture-of-logits) is used to find\\nthe final top ùëò (e.g., 100) candidates. We show in Section 5.2.1 that\\nthis design significantly improves throughput without degrading\\nrecall for suitably chosen values of ùëò‚Ä≤.\\nMain similarity function . The hierarchical retrieval design\\nenables complex neural networks to be used when modeling (user,\\nitem) similarities. In Section 3, we discuss one such instance of'}\n",
      "Only in Neural: {'On the Theoretical Limitations of\\nEmbedding-Based Retrieval\\nOrion Weller*,1,2, Michael Boratko1, Iftekhar Naim1 and Jinhyuk Lee1\\n1Google DeepMind,2Johns Hopkins University\\nVector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a\\nnascentriseinusingthemforreasoning,instruction-following,coding, andmore. Thesenewbenchmarks\\npush embeddings to work forany queryand any notion of relevancethat could be given. While prior\\nworks have pointed out theoretical limitations of vector embeddings, there is a common assumption\\nthat these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome\\nwith better training data and larger models. In this work, we demonstrate that we may encounter these\\ntheoretical limitations in realistic settings with extremely simple queries. We connect known results\\nin learning theory, showing that the number of top-ùëò subsets of documents capable of being returned\\nas the result of some query is limited by the dimension of the embedding. We empirically show that\\nthis holds true even if we restrict toùëò = 2, and directly optimize on the test set with free parameterized\\nembeddings. We then create a realistic dataset called LIMIT that stress tests models based on these\\ntheoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple\\nnature of the task. Our work shows the limits of embedding models under the existing single vector\\nparadigm and calls for future research to develop methods that can resolve this fundamental limitation.\\n1. Introduction\\nOver the last two decades, information retrieval (IR) has moved from models dominated by sparse\\ntechniques (such as BM25 [Robertson et al., 1995]) to those that use neural language models (LM)\\nas their backbones [Lee et al., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].', 'On the Theoretical Limitations of Embedding-Based Retrieval\\nJ. Lee, F. Chen, S. Dua, D. Cer, M. Shanbhogue, I. Naim, G. H. √Åbrego, Z. Li, K. Chen, H. S. Vera, et al.\\nGemini embedding: Generalizable embeddings from gemini.arXiv preprint arXiv:2503.07891,\\n2025.\\nK. Lee, M.-W. Chang, and K. Toutanova. Latent retrieval for weakly supervised open domain question\\nanswering. In A. Korhonen, D. Traum, and L. M√†rquez, editors,Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics, pages 6086‚Äì6096, Florence, Italy, July\\n2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https:\\n//aclanthology.org/P19-1612/.\\nC. Li, Z. Gan, Z. Yang, J. Yang, L. Li, L. Wang, J. Gao, et al. Multimodal foundation models: From\\nspecialists to general-purpose assistants.Foundations and Trends¬Æ in Computer Graphics and Vision,\\n16(1-2):1‚Äì214, 2024.\\nX. H. L√π. Bm25s: Orders of magnitude faster lexical search via eager sparse scoring.arXiv preprint\\narXiv:2407.03618, 2024.\\nX. Ma, S.-C. Lin, M. Li, W. Chen, and J. Lin. Unifying multimodal retrieval via document screenshot\\nembedding. arXiv preprint arXiv:2406.11251, 2024.\\nC. Malaviya, P. Shaw, M.-W. Chang, K. Lee, and K. Toutanova. Quest: A retrieval dataset of entity-\\nseeking queries with implicit set operations.arXiv preprint arXiv:2305.11694, 2023.\\nN. Muennighoff, N. Tazi, L. Magne, and N. Reimers. Mteb: Massive text embedding benchmark.arXiv\\npreprint arXiv:2210.07316, 2022.\\nN. Muennighoff, S. Hongjin, L. Wang, N. Yang, F. Wei, T. Yu, A. Singh, and D. Kiela. Generative\\nrepresentational instruction tuning. InICLR 2024 Workshop: How Far Are We From AGI, 2024.\\nH. Oh, H. Lee, S. Ye, H. Shin, H. Jang, C. Jun, and M. Seo. Instructir: A benchmark for instruction\\nfollowing of information retrieval models.arXiv preprint arXiv:2402.14334, 2024.\\nA. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding.arXiv\\npreprint arXiv:1807.03748, 2018.'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 10: What are some state-of-the-art techniques for efficient neural retrieval on GPUs or TPUs?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\\nTable 6: Amazon Reviews (Beauty, Games, and Books) results on top of sequential methods.\\nmethod hr@10 hr@50 hr@200 hr@500 mrr\\nBeauty\\nbaseline (BCE) 0.0256 0.0743 0.1604 0.2370 0.0126\\nDot product + SS 0.0655 (+155.4%) 0.1379 (+85.5%) 0.2271 (+41.6%) 0.2914 (+23.0%) 0.0333 (+164.4%)\\nMLP + SS 0.0393 (+53.4%) 0.1005 (+35.2%) 0.1864 (+16.2%) 0.2465 (+4.0%) 0.0189 (+49.8%)\\nNeuMF + SS 0.0535 (+108.9%) 0.1247 (+67.8%) 0.2152 (+34.2%) 0.2818 (+18.9%) 0.0264 (+109.6%)\\nDeepFM (4√ó4) + SS 0.0494 (+92.9%) 0.1271 (+71.0%) 0.2238 (+39.5%) 0.2953 (+24.6%) 0.0241 (+91.3%)\\nMoL (4√ó4) + SS 0.0541 (+111.0%) 0.1288 (+73.3%) 0.2246 (+40.0%) 0.2961 (+25.0%) 0.0262 (+107.5%)\\nGames\\nbaseline (BCE) 0.0794 0.2209 0.4217 0.5642 0.0383\\nDot product + SS 0.1246 (+57.0%) 0.2771 (+25.4%) 0.4505 (+6.8%) 0.5736 (+1.7%) 0.0600 (+56.7%)\\nMLP + SS 0.1066 (+34.3%) 0.2537 (+14.9%) 0.4293 (+1.8%) 0.5582 (-1.1%) 0.0498 (+30.1%)\\nNeuMF + SS 0.1175 (+48.0%) 0.2722 (+23.2%) 0.4479 (+6.2%) 0.5755 (+2.0%) 0.0559 (+46.0%)\\nDeepFM (4√ó4) + SS 0.1216 (+53.2%) 0.2817 (+27.5%) 0.4690 (+11.2%) 0.5965 (+5.7%) 0.0576 (+50.5%)\\nMoL (4√ó4) + SS 0.1221 (+53.9%) 0.2834 (+28.3%) 0.4713 (+11.8%) 0.5964 (+5.7%) 0.0586 (+53.2%)\\nBooks\\nbaseline (BCE) 0.0247 0.0660 0.1370 0.2079 0.0123\\nDot product + SS 0.0317 (+28.3%) 0.0814 (+23.3%) 0.1588 (+15.9%) 0.2295 (+10.4%) 0.0156 (+26.8%)\\nMLP + SS 0.0297 (+20.2%) 0.0759 (+15.0%) 0.1500 (+9.5%) 0.2190 (+5.3%) 0.0144 (+17.1%)\\nNeuMF + SS 0.0358 (+45.0%) 0.0871 (+32.0%) 0.1644 (+20.0%) 0.2338 (+12.5%) 0.0178 (+44.7%)\\nDeepFM (8√ó8) + SS 0.0361 (+46.2%) 0.0905 (+37.1%) 0.1706 (+24.5%) 0.2414 (+16.1%) 0.0179 (+45.5%)\\nMoL (8√ó8) + SS 0.0388 (+57.1%) 0.0934 (+41.5%) 0.1751 (+27.8%) 0.2479 (+19.2%) 0.0194 (+57.7%)\\nTable 7: Ablation studies on top of MoL and sequential methods.\\nML-1M (HR@10) ML-20M (HR@10) Beauty (HR@200) Games (HR@200) Books (HR@200)\\nMoL 0.3079 0.3114 0.2088 0.4618 0.1751'}\n",
      "Only in Neural: {'is designed with efficient training and inference on accelerators\\nin mind. It not only significantly outperforms dot-product based\\nbaselines in hit rate, but also better generalizes to torso and long\\ntail where there are limited training examples by utilizing gating\\ndesigns and component-level hypersphere embeddings.\\nOur second contribution is to enable efficient training and in-\\nference with expressive similarity functions (e.g., MoL), on modern\\naccelerators (Section 4). We propose a hierarchical retrieval strat-\\negy, and in particular, an accelerator-friendly algorithm for very\\nlarge corpus called h-indexer. Combined with other optimizations\\nlike quantization and kernel fusions, h-indexer enables us to scale\\nup retrieval to hundreds of millions of items with latency compa-\\nrable to MIPS setups. It‚Äôs worth remarking that, as highlighted in\\nFigure 2, the serving cost, e.g., latency and throughput ‚Äî on GPUs\\nin this work ‚Äî does not necessarily increase with compute thanks\\nto the significantly higher GPU utilization of our model. Our hier-\\narchical design, unlike dot-product, can also harness more compute\\nwith constant GPU memory budget, which is critical given limited\\nmemory capacity in production.\\nOur final contribution is to conduct extensive experiments on\\npublic and industrial datasets (Section 5) with MoL and h-indexer.\\nOur proposed method achieves up to 77.3% gain in HR on standard\\nbenchmark datasets like MovieLens and Amazon Reviews, and up\\nto +27.6% gain at HR@50 over strong baselines on one of the largest\\nrecommendation system deployments at Meta. The strong perfor-\\nmance of MoL validates our hypothesis that expressive similarity\\nfunctions are needed to capture dynamics of real-world datasets at\\nretrieval stage. The proposed method also reduces Matthew effect,\\nshowing that MoL indeed generalizes well over the entire corpus.\\nWe finally compare the proposed approach with related works\\nin Section 6 and conclude in Section 7.\\n2 OVERVIEW'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def analyze_retrieval_differences(prompts: List[str], top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Analyzes differences between cosine similarity and neural reranking for all prompts.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Analyzing differences between retrieval methods for {len(prompts)} prompts\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for idx, prompt in enumerate(prompts, 1):\n",
    "        # Get results from both methods\n",
    "        cosine_results = retrieve_with_cosine(prompt, top_k=top_k)\n",
    "        neural_results = neural_reranking_retrieval(prompt)\n",
    "        \n",
    "        # Extract chunks for easy comparison\n",
    "        cosine_chunks = [chunk for _, chunk in cosine_results]\n",
    "        neural_chunks = [chunk for _, chunk in neural_results]\n",
    "        \n",
    "        # Find differences\n",
    "        different_chunks = set(cosine_chunks) ^ set(neural_chunks)\n",
    "        different_order = cosine_chunks != neural_chunks\n",
    "        \n",
    "        print(f\"\\nPrompt {idx}: {prompt}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if different_chunks or different_order:\n",
    "            print(\"‚ö†Ô∏è Differences detected!\")\n",
    "            \n",
    "            if different_chunks:\n",
    "                print(\"\\nDifferent chunks retrieved:\")\n",
    "                print(\"Only in Cosine:\", set(cosine_chunks) - set(neural_chunks))\n",
    "                print(\"Only in Neural:\", set(neural_chunks) - set(cosine_chunks))\n",
    "            \n",
    "            if different_order and not different_chunks:\n",
    "                print(\"\\nSame chunks but different ordering:\")\n",
    "                print(\"\\nCosine order:\")\n",
    "                for i, (score, chunk) in enumerate(cosine_results, 1):\n",
    "                    print(f\"{i}. (Score: {score:.4f}) {chunk[:100]}...\")\n",
    "                \n",
    "                print(\"\\nNeural order:\")\n",
    "                for i, (score, chunk) in enumerate(neural_results, 1):\n",
    "                    print(f\"{i}. (Score: {score:.4f}) {chunk[:100]}...\")\n",
    "        else:\n",
    "            print(\"‚úì Both methods returned identical results in identical order\")\n",
    "        \n",
    "        print(f\"\\n{'-'*80}\")\n",
    "\n",
    "# Run the analysis for all prompts\n",
    "analyze_retrieval_differences(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b458f005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Analyzing differences between retrieval methods for 10 prompts\n",
      "================================================================================\n",
      "\n",
      "Prompt 1: What are the theoretical limitations of embedding-based retrieval?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Same chunks but different ordering:\n",
      "\n",
      "Cosine order:\n",
      "1. (Score: 0.9233) On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Ift...\n",
      "2. (Score: 0.8959) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments pro...\n",
      "3. (Score: 0.8921) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Ou...\n",
      "4. (Score: 0.8894) setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "em...\n",
      "5. (Score: 0.8893) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "documents with logical operators), we wi...\n",
      "\n",
      "Neural order:\n",
      "1. (Score: 0.1026) On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Ift...\n",
      "2. (Score: 0.1007) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments pro...\n",
      "3. (Score: 0.1000) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "documents with logical operators), we wi...\n",
      "4. (Score: 0.1000) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Ou...\n",
      "5. (Score: 0.1000) setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "em...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 2: How does the geometry of the embedding space affect retrieval performance?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'On the Theoretical Limitations of Embedding-Based Retrieval\\nSplit Dim Recall@2 Recall@10 Recall@100\\nTest 32 85.5 98.4 100.0\\nTest 64 90.4 98.7 100.0\\nTest 128 93.1 99.5 99.9\\nTest 256 94.2 99.7 100.0\\nTest 384 95.6 99.6 100.0\\nTest 512 94.0 99.5 99.9\\nTest 768 96.1 99.8 100.0\\nTest 1024 96.5 99.8 100.0\\nTrain 32 0.0 0.0 0.0\\nTrain 64 0.1 0.3 2.2\\nTrain 128 0.2 0.7 3.1\\nTrain 256 0.0 0.0 0.4\\nTrain 384 1.1 2.7 8.3\\nTrain 512 0.7 2.3 9.8\\nTrain 768 0.7 2.4 9.9\\nTrain 1024 1.0 2.8 11.2\\nTable 2|Fine-tuning results in table form. See Figure 5 for the comparable plot.\\n20', 'On the Theoretical Limitations of Embedding-Based Retrieval\\nModel Dim Random Dense Cycle Disjoint\\nBM25 default 96.1 93.0 96.0 96.6\\nE5-Mistral 7B 32 1.7 0.6 1.7 2.2\\nE5-Mistral 7B 64 4.3 0.5 3.3 4.8\\nE5-Mistral 7B 128 10.3 0.9 9.1 10.5\\nE5-Mistral 7B 256 16.9 1.2 14.0 15.5\\nE5-Mistral 7B 512 26.4 2.5 24.0 26.6\\nE5-Mistral 7B 768 31.5 3.1 27.7 30.0\\nE5-Mistral 7B 1024 34.0 3.8 29.5 32.8\\nE5-Mistral 7B 2048 36.8 4.3 33.6 36.7\\nE5-Mistral 7B 3072 38.9 4.7 35.8 37.6\\nE5-Mistral 7B 4096 40.4 4.8 36.6 38.8\\nGTE-ModernColBERT default 71.1 61.8 65.3 70.1\\nGritLM 7B 32 1.5 0.6 1.9 1.5\\nGritLM 7B 64 3.6 0.6 2.9 3.9\\nGritLM 7B 128 8.0 1.6 6.3 8.4\\nGritLM 7B 256 15.8 2.0 14.4 16.0\\nGritLM 7B 512 33.7 4.5 29.5 33.8\\nGritLM 7B 768 39.0 5.6 34.4 40.1\\nGritLM 7B 1024 43.3 6.6 37.4 44.1\\nGritLM 7B 2048 55.3 9.0 49.0 55.8\\nGritLM 7B 3072 61.5 10.9 54.3 61.6\\nGritLM 7B 4096 61.8 10.4 56.6 63.2\\nPromptriever Llama3 8B 32 0.7 0.6 1.2 1.1\\nPromptriever Llama3 8B 64 2.6 1.1 2.8 2.3\\nPromptriever Llama3 8B 128 5.7 1.3 5.7 7.1\\nPromptriever Llama3 8B 256 16.2 1.7 12.6 16.3\\nPromptriever Llama3 8B 512 31.9 4.7 26.0 29.0\\nPromptriever Llama3 8B 768 37.5 8.5 33.2 37.5\\nPromptriever Llama3 8B 1024 42.3 11.8 37.5 40.5\\nPromptriever Llama3 8B 2048 52.7 14.1 49.1 53.7\\nPromptriever Llama3 8B 3072 56.6 15.8 52.9 57.4\\nPromptriever Llama3 8B 4096 62.0 19.4 58.6 63.6\\nQwen3 Embed 32 3.2 0.7 2.7 2.6\\nQwen3 Embed 64 5.4 1.1 5.0 5.7\\nQwen3 Embed 128 9.9 1.9 7.9 9.4\\nQwen3 Embed 256 14.2 2.4 11.6 12.5\\nQwen3 Embed 512 18.0 3.3 14.7 15.9\\nQwen3 Embed 768 19.5 3.5 15.5 18.0\\nQwen3 Embed 1024 20.4 3.6 16.1 18.7\\nQwen3 Embed 2048 22.3 4.1 17.2 21.4\\nQwen3 Embed 3072 21.9 4.3 17.9 21.1\\nQwen3 Embed 4096 22.7 4.5 17.8 20.9\\nGemini Embed 2 0.0 0.1 0.1 0.0\\nGemini Embed 4 0.0 0.0 0.0 0.1\\nGemini Embed 8 0.2 0.0 0.0 0.2\\nGemini Embed 16 0.2 0.0 0.2 0.1\\nGemini Embed 32 0.4 0.0 0.2 0.1\\nGemini Embed 64 0.6 0.2 0.3 0.5\\nGemini Embed 128 1.4 0.3 0.8 1.4\\nGemini Embed 256 7.1 1.2 5.8 7.4\\nGemini Embed 512 18.9 3.6 17.6 19.7\\nGemini Embed 768 33.5 7.6 31.0 34.5'}\n",
      "Only in Neural: {'On the Theoretical Limitations of Embedding-Based Retrieval\\n32 512 1024 2048 3072 4096\\nEmbed Dim\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8Score\\nRecall@2\\n32 512 1024 2048 3072 4096\\nEmbed Dim\\nRecall@10\\n32 512 1024 2048 3072 4096\\nEmbed Dim\\nRecall@100\\nE5-Mistral 7B\\nSnowflake Arctic L\\nGritLM 7B\\nPromptriever Llama3 8B\\nQwen3 Embed\\nGemini Embed\\nBM25\\nGTE-ModernColBERT\\nFigure 3|Scores on the LIMIT task. Despite the simplicity of the task we see that SOTA models\\nstruggle. We also see that the dimensionality of the model is a limiting factor and that as the\\ndimension increases, so does performance. Even multi-vector models struggle. Lexical models like\\nBM25 do very well due to their higher dimensionality. Stars indicate models trained with MRL.\\nModels We evaluate the state-of-the-art embedding models including GritLM [Muennighoff et al.,\\n2024], Qwen 3 Embeddings [Zhang et al., 2025], Promptriever [Weller et al., 2024b], Gemini\\nEmbeddings [Lee et al., 2025], Snowflake‚Äôs Arctic Embed Large v2.0 [Yu et al., 2024], and E5-Mistral\\nInstruct [Wang et al., 2022, 2023]. These models range in embedding dimension (1024 to 4096)\\nas well as in training style (instruction-based, hard negative optimized, etc.). We also evaluate\\nthree non-single vector models to show the distinction: BM25 [Robertson et al., 1995, L√π, 2024],\\ngte-ModernColBERT [Chaffin, 2025, Chaffin and Sourty, 2024], and a token-wise TF-IDF.9\\nWe show results at the full embedding dimension and also with truncated embedding dimension\\n(typically used with matryoshka learning, aka MRL [Kusupati et al., 2022]). For models not trained\\nwith MRL this will result in sub-par scores, thus, models trained with MRL are indicating with stars in\\nthe plots. However, as there are no LLMs with an embedding dimension smaller than 384, we include\\nMRL for all models to small dimensions (32) to show the impact of embedding dimensionality.\\nResults Figure 3 shows the results on the full LIMIT while Figure 4 shows the results on the small', 'On the Theoretical Limitations of Embedding-Based Retrieval\\n32 512 1024 2048 3072 4096\\nEmbed Dim\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Score\\nRecall@2\\n32 512 1024 2048 3072 4096\\nEmbed Dim\\nRecall@10\\n32 512 1024 2048 3072 4096\\nEmbed Dim\\nRecall@20\\nE5-Mistral 7B\\nSnowflake Arctic L\\nGritLM 7B\\nPromptriever Llama3 8B\\nQwen3 Embed\\nGemini Embed\\nBM25\\nGTE-ModernColBERT\\nFigure 4|Scores on the LIMIT small task (N=46) over embedding dimensions. Despite having just\\n46 documents, model struggle even with recall@10 and cannot solve the task even with recall@20.\\nmodels (although still far from solving the task) while BM25 comes close to perfect scores. Both of\\nthese alterative architectures (sparse and multi-vector) offer various trade-offs, see ¬ß5.6 for analysis.\\n5.3. Is this Domain Shift?\\n32128 256 384 512 768 1024\\nEmbed Dim\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Recall@2\\nTrained on:\\nTest\\nTrain\\nFigure 5|Training on LIMIT train does\\nnot significantly help, indicating the\\nissue is not domain shift. But models\\ncan solve it if they overfit to the test set.\\nAlthough our queries look similar to standard web search\\nqueries, we wondered whether there could be some do-\\nmain shift causing the low performance. If so, we would\\nexpect that training on a training set of similar examples\\nwould significantly improve performance. On the other\\nhand, if the task was intrinsically hard, training on the\\ntraining set would provide little help whereas training\\non the test set would allow the model to overfit to those\\ntokens (similar to the free parameterized experiments).\\nTo test this we take an off the shelf embedding model\\nand train it on either the training set (created synthetically\\nusingnon-testsetattributes)ortheofficialtestsetofLIMIT.\\nWe use lightonai/modernbert-embed-large and\\nfine-tune it on these splits, using the full dataset for in\\nbatchnegatives(excludingpositives)usingSentenceTrans-\\nformers [Reimers and Gurevych, 2019]. We show a range of dimensions by projecting the hidden'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 3: What is the 'norm concentration' phenomenon in high-dimensional spaces?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing Liu\\nTable 9: Hyperparameters used for public datasets\\nML-1M ML-20M Beauty Games Books\\nSASRec\\nembedding dim ùëë 50 256 50 50 64\\nencoder blocks ùëè 2 4 2 2 4\\nattention heads ‚Ñé 1 8 1 1 4\\ndropout rate 0.2 0.2 0.5 0.5 0.5\\nDot Product temperature 20 20 20 20 20\\nMLP\\nhidden layer size 512 512 128 128 256\\ndropout rate 0.1 0.1 0.2 0.2 0.2\\nInference FLOPs per (ùë•, ùë¢) 50.5K 256.5K 12.6K 12.6K 32.3K\\nNeuMF\\nGMF dim 32 32 32 32 32\\nMLP hidden dim 256 256 128 128 256\\nMLP output dim 64 64 64 64 64\\ndropout rate 0.1 0.1 0.2 0.2 0.1\\nfinal MLP hidden dim 256 256 128 128 256\\nInference FLOPs per (ùë•, ùë¢) 49.3K 152.3K 24.6K 24.6K 56.3K\\nFM\\nhidden dim for output layer 256 256 128 128 256\\nprediction layer dropout rate 0.2 0.2 0.3 0.3 0.2\\nInference FLOPs per (ùë•, ùë¢) 197.6K 405K 10.7K 10.7K 203.6K\\nMoL\\n(ùëòùë¢ x ùëòùë• x embedding dim) (8 x 8 x 32) (8 x 8 x 32) (4 x 4 x 32) (4 x 4 x 32) (8 x 8 x 32)\\nhidden dim for gating MLPs 128 128 128 128 128\\nhidden dim for embedding proj. MLPs 512 512 N/A N/A 512\\nitem-side embedding proj. MLP dropout rate 0.1 0.1 0.3 0.3 0.1\\ngating softmax dropout rate 0.2 0.2 - - 0.2\\ngating input dropout rate - - 0.2 0.2 -\\ncomponent-level hypersphere embeddings On On Off Off On\\nùúè 20 20 20 20 20\\nInference FLOPs per (ùë•, ùë¢) 211.7K 445.3K 12.9K 12.9K 227.6K\\nNon-cachable inference FLOPs per (ùë•, ùë¢) 32.9K 33.1K 4.4K 4.4K 21.5K'}\n",
      "Only in Neural: {'On the Theoretical Limitations of Embedding-Based Retrieval\\nK. L. Clarkson. Applications of random sampling in computational geometry, ii. InProceedings of the\\nfourth annual symposium on Computational geometry, pages 1‚Äì11, 1988.\\nG. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram,\\nD. Zhang, E. Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality,\\nlong context, and next generation agentic capabilities.arXiv preprint arXiv:2507.06261, 2025.\\nN. Craswell, B. Mitra, E. Yilmaz, D. Campos, and E. M. Voorhees. Overview of the trec 2019 deep\\nlearning track.arXiv preprint arXiv:2003.07820, 2020.\\nK. Enevoldsen, I. Chung, I. Kerboua, M. Kardos, A. Mathur, D. Stap, J. Gala, W. Siblini, D. Krzemi≈Ñski,\\nG. I. Winata, et al. Mmteb: Massive multilingual text embedding benchmark.arXiv preprint\\narXiv:2502.13595, 2025.\\nM. Faysse, H. Sibille, T. Wu, B. Omrani, G. Viaud, C. Hudelot, and P. Colombo. Colpali: Efficient\\ndocument retrieval with vision language models.arXiv preprint arXiv:2407.01449, 2024.\\nH. Hatami and P. Hatami. Structure in communication complexity and constant-cost complexity\\nclasses. arXiv preprint arXiv:2401.14623, 2024.\\nH. Hatami, P. Hatami, W. Pires, R. Tao, and R. Zhao. Lower bound methods for sign-rank and\\ntheir limitations. InApproximation, Randomization, and Combinatorial Optimization. Algorithms\\nand Techniques (APPROX/RANDOM 2022), pages 22‚Äì1. Schloss Dagstuhl‚ÄìLeibniz-Zentrum f√ºr\\nInformatik, 2022.\\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.\\nHendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models.arXiv preprint\\narXiv:2203.15556, 2022.\\nG. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, and E. Grave. Unsupervised\\ndense information retrieval with contrastive learning.arXiv preprint arXiv:2112.09118, 2021.'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 4: Can embedding models truly capture semantic similarity for all tasks?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'2.2. Empirical tasks pushing the limits of dense retrieval\\nRetrieval models have been pushed beyond their initial use cases to handle a broad variety of areas.\\nNotable works include efforts to represent a wide group of domains [Thakur et al., 2021, Lee et al.,\\n2024], a diverse set of instructions [Weller et al., 2024a, Zhou et al., 2024, Oh et al., 2024], and to\\nhandle reasoning over the queries [Xiao et al., 2024, Su et al., 2024]. This has pushed the focus of\\nembedding models from basic keyword matching to embeddings that can represent the full semantic\\nmeaning of language. As such, it is more common than ever to connect what were previously unrelated\\ndocuments into the top-ùëò relevant set,2 increasing the number of combinations that models must be\\nable to represent. This has motivated our interest in understanding the limits of what embeddings\\ncan represent, as current work expects it to handleevery task.\\nPreviousworkhasexploredempiricallythelimitsofmodels: ReimersandGurevych[2020]showed\\nthat smaller dimension embedding models have more false positives, especially with larger-scale\\ncorpora. Ormazabal et al. [2019] showed the empirical limitations of models in the cross-lingual\\nsetting and Yin and Shen [2018] showed how embedding dimensions relate to the bias-variance\\ntradeoff. In contrast, our work provides a theoretical connection between the embedding dimension\\nand the sign-rank of the query relevance (qrel) matrix, while also showing empirical limitations.\\n2.3. Theoretical Limits of Vectors in Geometric Space\\nUnderstanding and finding nearest neighbors in semantic space has a long history in mathematics\\nresearch, withearlyworksuchastheVoronoidiagrambeingstudiedasfarbackas1644andformalized\\nin 1908 [Voronoi, 1908]. The order-k version of the Voronoi diagram (i.e. the Voronoi diagram\\n2You can imagine an easy way to connect any two documents merely by using logical operators, i.e. X and Y.\\n3', 'L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text embeddings by\\nweakly-supervised contrastive pre-training.arXiv preprint arXiv:2212.03533, 2022.\\nL. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Improving text embeddings with\\nlarge language models.arXiv preprint arXiv:2401.00368, 2023.\\nB. Warner, A. Chaffin, B. Clavi√©, O. Weller, O. Hallstr√∂m, S. Taghadouini, A. Gallagher, R. Biswas,\\nF. Ladhak, T. Aarsen, et al. Smarter, better, faster, longer: A modern bidirectional encoder for fast,\\nmemory efficient, and long context finetuning and inference.arXiv preprint arXiv:2412.13663,\\n2024.\\nJ. Wei, Z. Sun, S. Papay, S. McKinney, J. Han, I. Fulford, H. W. Chung, A. T. Passos, W. Fedus, and\\nA. Glaese. Browsecomp: A simple yet challenging benchmark for browsing agents.arXiv preprint\\narXiv:2504.12516, 2025.\\nO. Weller, B. Chang, S. MacAvaney, K. Lo, A. Cohan, B. Van Durme, D. Lawrie, and L. Soldaini.\\nFollowir: Evaluating and teaching information retrieval models to follow instructions.arXiv preprint\\narXiv:2403.15246, 2024a.\\nO. Weller, B. Van Durme, D. Lawrie, A. Paranjape, Y. Zhang, and J. Hessel. Promptriever: Instruction-\\ntrained retrievers can be prompted like language models.arXiv preprint arXiv:2409.11136, 2024b.\\nO. Weller, B. Chang, E. Yang, M. Yarmohammadi, S. Barham, S. MacAvaney, A. Cohan, L. Soldaini,\\nB. Van Durme, and D. Lawrie. mfollowir: a multilingual benchmark for instruction following in\\nretrieval. arXiv preprint arXiv:2501.19264, 2025a.\\n16'}\n",
      "Only in Neural: {'setting where the vectors themselves are directly optimized with the test data. This allows us to\\nempirically show how the embedding dimension enables the solving of retrieval tasks. We find there\\nexists a crucial point for each embedding dimension (ùëë) where the number of documents is too large\\nfor the embedding dimension to encode all combinations. We then gather these crucial points for a\\nvariety ofùëë and show that this relationship can be modeled empirically with a polynomial function.\\nWe also go one step further and construct a realistic but simple dataset based on these theoret-\\nical limitations (called LIMIT). Despite the simplicity of the task (e.g.,who likes Apples? and\\nJon likes Apples, ...), we find it is very difficult for even state-of-the-art embedding mod-\\nels [Lee et al., 2025, Zhang et al., 2025] on MTEB [Enevoldsen et al., 2025] due to the theoretical\\nunderpinnings, and impossible1 for models with small embedding dimensions.\\nOverall, our work contributes: (1) a theoretical basis for the fundamental limitations of embedding\\nmodels, (2) a best-case empirical analysis showing that this proof holds for any dataset instantiation\\n(by free embedding optimization), and (3) a simple real-world natural language instantiation called\\nLIMIT that even state-of-the-art embedding models cannot solve.\\nThese results imply interesting findings for the community: on one hand we see neural embedding\\nmodels becoming immensely successful. However, academic benchmarks test only a small amount of\\nthe queries that could be issued (and these queries are often overfitted to), hiding these limitations.\\nOur work shows that as the tasks given to embedding models require returning ever-increasing\\ncombinations of top-ùëòrelevant documents (e.g., through instructions connecting previously unrelated\\n1At least with current optimization techniques for retrieval.\\n2', 'On the Theoretical Limitations of Embedding-Based Retrieval\\ndocuments with logical operators), we will reach a limit of combinations they cannot represent.\\nThus, the community should be aware of these limitations, both when designing evaluations (as\\nLIMITshows)andbychoosingalternativeretrievalapproaches‚Äìsuchascross-encoders ormulti-vector\\nmodels ‚Äì when attempting to create models that can handle the full range of instruction-based queries,\\ni.e. any query and relevance definition.\\n2. Related Work\\n2.1. Neural Embedding Models\\nThere has been immense progress on embedding models in recent years [Lee et al., 2019, Craswell\\net al., 2020, BehnamGhader et al., 2024], moving from simple web search (text-only) to advanced\\ninstruction-following and multi-modal representations. These models generally followed advances in\\nlanguage models, such as pre-trained LMs [Hoffmann et al., 2022], multi-modal LMs [Li et al., 2024,\\nTeam, 2024], and advances in instruction-following [Zhou et al., 2023, Ouyang et al., 2022]. Some\\nof the prominent examples in retrieval include CoPali [Faysse et al., 2024] and DSE [Ma et al., 2024]\\nwhich focus on multimodal embeddings, Instructor [Su et al., 2022] and FollowIR [Weller et al.,\\n2024a] for instruction following, and GritLM [Muennighoff et al., 2024] and Gemini Embeddings\\n[Lee et al., 2025] for pre-trained LMs turned embedders.\\nOurwork, thoughfocusedsolelyontextualrepresentationsfor simplicity, appliestoallmodalities\\nof single vector embeddings for any domain of dataset. As the space of things to represent grows\\n(through instructions or multi-modality) they will increasingly run into these theoretical limitations.\\n2.2. Empirical tasks pushing the limits of dense retrieval\\nRetrieval models have been pushed beyond their initial use cases to handle a broad variety of areas.\\nNotable works include efforts to represent a wide group of domains [Thakur et al., 2021, Lee et al.,'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 5: What are the failure modes for embedding-based search?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Same chunks but different ordering:\n",
      "\n",
      "Cosine order:\n",
      "1. (Score: 0.8439) On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Ift...\n",
      "2. (Score: 0.8394) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments pro...\n",
      "3. (Score: 0.8369) setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "em...\n",
      "4. (Score: 0.8322) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "32 512 1024 2048 3072 4096\n",
      "Embed Dim\n",
      "0.0...\n",
      "5. (Score: 0.8310) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Ou...\n",
      "\n",
      "Neural order:\n",
      "1. (Score: 0.1016) On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Ift...\n",
      "2. (Score: 0.1009) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments pro...\n",
      "3. (Score: 0.1007) setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "em...\n",
      "4. (Score: 0.1002) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Ou...\n",
      "5. (Score: 0.1000) On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "32 512 1024 2048 3072 4096\n",
      "Embed Dim\n",
      "0.0...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 6: How can neural retrieval be optimized for hardware accelerators?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\\nTable 6: Amazon Reviews (Beauty, Games, and Books) results on top of sequential methods.\\nmethod hr@10 hr@50 hr@200 hr@500 mrr\\nBeauty\\nbaseline (BCE) 0.0256 0.0743 0.1604 0.2370 0.0126\\nDot product + SS 0.0655 (+155.4%) 0.1379 (+85.5%) 0.2271 (+41.6%) 0.2914 (+23.0%) 0.0333 (+164.4%)\\nMLP + SS 0.0393 (+53.4%) 0.1005 (+35.2%) 0.1864 (+16.2%) 0.2465 (+4.0%) 0.0189 (+49.8%)\\nNeuMF + SS 0.0535 (+108.9%) 0.1247 (+67.8%) 0.2152 (+34.2%) 0.2818 (+18.9%) 0.0264 (+109.6%)\\nDeepFM (4√ó4) + SS 0.0494 (+92.9%) 0.1271 (+71.0%) 0.2238 (+39.5%) 0.2953 (+24.6%) 0.0241 (+91.3%)\\nMoL (4√ó4) + SS 0.0541 (+111.0%) 0.1288 (+73.3%) 0.2246 (+40.0%) 0.2961 (+25.0%) 0.0262 (+107.5%)\\nGames\\nbaseline (BCE) 0.0794 0.2209 0.4217 0.5642 0.0383\\nDot product + SS 0.1246 (+57.0%) 0.2771 (+25.4%) 0.4505 (+6.8%) 0.5736 (+1.7%) 0.0600 (+56.7%)\\nMLP + SS 0.1066 (+34.3%) 0.2537 (+14.9%) 0.4293 (+1.8%) 0.5582 (-1.1%) 0.0498 (+30.1%)\\nNeuMF + SS 0.1175 (+48.0%) 0.2722 (+23.2%) 0.4479 (+6.2%) 0.5755 (+2.0%) 0.0559 (+46.0%)\\nDeepFM (4√ó4) + SS 0.1216 (+53.2%) 0.2817 (+27.5%) 0.4690 (+11.2%) 0.5965 (+5.7%) 0.0576 (+50.5%)\\nMoL (4√ó4) + SS 0.1221 (+53.9%) 0.2834 (+28.3%) 0.4713 (+11.8%) 0.5964 (+5.7%) 0.0586 (+53.2%)\\nBooks\\nbaseline (BCE) 0.0247 0.0660 0.1370 0.2079 0.0123\\nDot product + SS 0.0317 (+28.3%) 0.0814 (+23.3%) 0.1588 (+15.9%) 0.2295 (+10.4%) 0.0156 (+26.8%)\\nMLP + SS 0.0297 (+20.2%) 0.0759 (+15.0%) 0.1500 (+9.5%) 0.2190 (+5.3%) 0.0144 (+17.1%)\\nNeuMF + SS 0.0358 (+45.0%) 0.0871 (+32.0%) 0.1644 (+20.0%) 0.2338 (+12.5%) 0.0178 (+44.7%)\\nDeepFM (8√ó8) + SS 0.0361 (+46.2%) 0.0905 (+37.1%) 0.1706 (+24.5%) 0.2414 (+16.1%) 0.0179 (+45.5%)\\nMoL (8√ó8) + SS 0.0388 (+57.1%) 0.0934 (+41.5%) 0.1751 (+27.8%) 0.2479 (+19.2%) 0.0194 (+57.7%)\\nTable 7: Ablation studies on top of MoL and sequential methods.\\nML-1M (HR@10) ML-20M (HR@10) Beauty (HR@200) Games (HR@200) Books (HR@200)\\nMoL 0.3079 0.3114 0.2088 0.4618 0.1751', 'branching is involved, can be hard to scale on accelerators.\\nAccelerating Inference for Retrieval. In the traditional dot product\\nretrieval setting, or MIPS (Maximum Inner Product Search), product\\nquantization and hashing techniques [10, 33] have been well studied\\nand are widely used in the non-accelerator retrieval settings.\\nPartitioning the item space is a complementary approach to\\nspeed up inference by reducing search space [8, 22, 24, 30, 47, 50, 51].\\nSearch space can be partitioned with clustering [ 24, 47], spatial-\\npartitioning trees [22, 30], or with approaches where the partition\\nstrategy is learned [8, 50, 51]. This line of work can be viewed as\\nalternatives to h-indexer for hierarchical retrieval settings.\\nA recent line of work investigates efficient MIPS-based KNN\\nretrieval on accelerators [2, 15]. There have not been significant\\nwork on non-MIPS setups on accelerators to date.'}\n",
      "Only in Neural: {'Revisiting Neural Retrieval on Accelerators\\nJiaqi Zhai\\njiaqiz@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nZhaojie Gong\\nzhaojieg@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nYueming Wang\\nyuemingw@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nXiao Sun\\nsunx@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nZheng Yan\\nzyan@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nFu Li\\nleaf123@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nXing Liu\\nxingl@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nABSTRACT\\nRetrieval finds a small number of relevant candidates from a large\\ncorpus for information retrieval and recommendation applications.\\nA key component of retrieval is to model (user, item) similarity,\\nwhich is commonly represented as the dot product of two learned\\nembeddings. This formulation permits efficient inference, com-\\nmonly known as Maximum Inner Product Search (MIPS). Despite\\nits popularity, dot products cannot capture complex user-item in-\\nteractions, which are multifaceted and likely high rank. We hence\\nexamine non-dot-product retrieval settings on accelerators, and\\npropose mixture of logits (MoL), which models (user, item) similarity\\nas an adaptive composition of elementary similarity functions. This\\nnew formulation is expressive, capable of modeling high rank (user,\\nitem) interactions, and further generalizes to the long tail. When\\ncombined with a hierarchical retrieval strategy, h-indexer, we are\\nable to scale up MoL to 100M corpus on a single GPU with latency\\ncomparable to MIPS baselines. On public datasets, our approach\\nleads to uplifts of up to 77.3% in hit rate (HR). Experiments on a\\nlarge recommendation surface at Meta showed strong metric gains\\nand reduced popularity bias, validating the proposed approach‚Äôs\\nperformance and improved generalization.\\nCCS CONCEPTS\\n‚Ä¢ Information systems ‚ÜíLearning to rank ; Recommender\\nsystems; ‚Ä¢ Computing methodologies ‚ÜíMachine learning\\nalgorithms.\\nKEYWORDS', 'this design significantly improves throughput without degrading\\nrecall for suitably chosen values of ùëò‚Ä≤.\\nMain similarity function . The hierarchical retrieval design\\nenables complex neural networks to be used when modeling (user,\\nitem) similarities. In Section 3, we discuss one such instance of\\nsimilarity functions, mixture of logits , that significantly outperforms\\ndot products. When designing this architecture, we aggressively\\nmake intermedidate tensors available for caching (green boxes in\\nFig. 1). For instance, we can cache item-side gating weights and\\ncombine them cheaply with non-cachable weights at inference time.\\nFigure 2: Infra efficiency in production: GPU utilization and\\npeak memory scaling with serving FLOPs.\\nh-indexer. We found that a simple, but highly optimized dot\\nproduct combined with specialized top-ùëò algorithm works well for\\nup to 100M corpus. This stage is co-trained with the main similarity\\nfunction. We discuss this design in details in Section 4.1.\\n3 MIXTURE OF LOGITS: AN\\nACCELERATOR-AWARE MODEL DESIGN\\nIn this section, we propose a new high rank similarity function,\\nmixture of logits (MoL). MoL is designed for neural retrieval settings\\non accelerators. The basic idea in MoL is to parameterize ùúôùúÉ(ùë¢,ùë•)\\nas an adaptive mixture of more elementary logits,\\nùúôùëÄùëúùêø(ùë•,ùë¢)=\\n‚àëÔ∏Å\\nùëò\\nùúãùëò,ùúÉ(ùë•,ùë¢)ùõøùëò,ùúÉ(ùë•,ùë¢) (3)\\nwhere ùúãùëò,ùúÉ(ùë•,ùë¢)are adaptive gating weights and ùõøùëò,ùúÉ(ùë•,ùë¢)are\\nelementary logits.\\nMoL achieves high rank in two ways. First, since ùõøùëò,ùúÉ(ùë•,ùë¢)in\\nEquation 3 can be parameterized by any form of neural networks,\\nùúôùúÉ(ùë•,ùë¢)has arbitrarily high rank. Second, as ùúãùëò,ùúÉ(ùë•,ùë¢)takes ùë• and\\nùë¢as input, we can create a high rank matrixln ùëù(ùë•|ùë¢)by combining\\nlow rank similarity functions, e.g., dot products, in a cost-efficient\\nfashion. For instance, using ùëògroups of dot products (of potentially\\ndifferent embedding dimensions), we can derive\\nùúôùëÄùëúùêøùëëùëúùë° ùëùùëüùëúùëëùë¢ùëêùë°ùë† (ùë•,ùë¢)=\\n‚àëÔ∏Å\\nùëò\\nùúãùëò,ùúÉ(ùë•,ùë¢)‚ü®ùëìùëò,ùúÉ(ùë¢),ùëîùëò,ùúÉ(ùë•)‚ü© (4)\\nAlthough the elementary logits ‚ü®ùëìùëò,ùúÉ(ùë¢),ùëîùëò,ùúÉ(ùë•)‚ü©themselves'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 7: What are the bottlenecks in modern neural retrieval pipelines?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'theoretical results hold empirically as well, through best case optimization of the vectors themselves.\\nWe then make a practical connection to existing state-of-the-art models by creating a simple natural\\nlanguage instantiation of the theory, called LIMIT, that these models cannot solve. Our results imply\\nthat the community should consider how instruction-based retrieval will impact retrievers, as there\\nwill be combinations of top-ùëò documents cannot represent.\\n12'}\n",
      "Only in Neural: {'Revisiting Neural Retrieval on Accelerators\\nJiaqi Zhai\\njiaqiz@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nZhaojie Gong\\nzhaojieg@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nYueming Wang\\nyuemingw@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nXiao Sun\\nsunx@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nZheng Yan\\nzyan@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nFu Li\\nleaf123@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nXing Liu\\nxingl@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nABSTRACT\\nRetrieval finds a small number of relevant candidates from a large\\ncorpus for information retrieval and recommendation applications.\\nA key component of retrieval is to model (user, item) similarity,\\nwhich is commonly represented as the dot product of two learned\\nembeddings. This formulation permits efficient inference, com-\\nmonly known as Maximum Inner Product Search (MIPS). Despite\\nits popularity, dot products cannot capture complex user-item in-\\nteractions, which are multifaceted and likely high rank. We hence\\nexamine non-dot-product retrieval settings on accelerators, and\\npropose mixture of logits (MoL), which models (user, item) similarity\\nas an adaptive composition of elementary similarity functions. This\\nnew formulation is expressive, capable of modeling high rank (user,\\nitem) interactions, and further generalizes to the long tail. When\\ncombined with a hierarchical retrieval strategy, h-indexer, we are\\nable to scale up MoL to 100M corpus on a single GPU with latency\\ncomparable to MIPS baselines. On public datasets, our approach\\nleads to uplifts of up to 77.3% in hit rate (HR). Experiments on a\\nlarge recommendation surface at Meta showed strong metric gains\\nand reduced popularity bias, validating the proposed approach‚Äôs\\nperformance and improved generalization.\\nCCS CONCEPTS\\n‚Ä¢ Information systems ‚ÜíLearning to rank ; Recommender\\nsystems; ‚Ä¢ Computing methodologies ‚ÜíMachine learning\\nalgorithms.\\nKEYWORDS'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 8: What is the trade-off between retrieval effectiveness and efficiency on accelerators?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'theoretical results hold empirically as well, through best case optimization of the vectors themselves.\\nWe then make a practical connection to existing state-of-the-art models by creating a simple natural\\nlanguage instantiation of the theory, called LIMIT, that these models cannot solve. Our results imply\\nthat the community should consider how instruction-based retrieval will impact retrievers, as there\\nwill be combinations of top-ùëò documents cannot represent.\\n12'}\n",
      "Only in Neural: {'Revisiting Neural Retrieval on Accelerators\\nJiaqi Zhai\\njiaqiz@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nZhaojie Gong\\nzhaojieg@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nYueming Wang\\nyuemingw@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nXiao Sun\\nsunx@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nZheng Yan\\nzyan@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nFu Li\\nleaf123@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nXing Liu\\nxingl@meta.com\\nMeta Platforms, Inc.\\nMenlo Park, CA, USA\\nABSTRACT\\nRetrieval finds a small number of relevant candidates from a large\\ncorpus for information retrieval and recommendation applications.\\nA key component of retrieval is to model (user, item) similarity,\\nwhich is commonly represented as the dot product of two learned\\nembeddings. This formulation permits efficient inference, com-\\nmonly known as Maximum Inner Product Search (MIPS). Despite\\nits popularity, dot products cannot capture complex user-item in-\\nteractions, which are multifaceted and likely high rank. We hence\\nexamine non-dot-product retrieval settings on accelerators, and\\npropose mixture of logits (MoL), which models (user, item) similarity\\nas an adaptive composition of elementary similarity functions. This\\nnew formulation is expressive, capable of modeling high rank (user,\\nitem) interactions, and further generalizes to the long tail. When\\ncombined with a hierarchical retrieval strategy, h-indexer, we are\\nable to scale up MoL to 100M corpus on a single GPU with latency\\ncomparable to MIPS baselines. On public datasets, our approach\\nleads to uplifts of up to 77.3% in hit rate (HR). Experiments on a\\nlarge recommendation surface at Meta showed strong metric gains\\nand reduced popularity bias, validating the proposed approach‚Äôs\\nperformance and improved generalization.\\nCCS CONCEPTS\\n‚Ä¢ Information systems ‚ÜíLearning to rank ; Recommender\\nsystems; ‚Ä¢ Computing methodologies ‚ÜíMachine learning\\nalgorithms.\\nKEYWORDS'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 9: Explain the concept of query-side latency in neural retrieval.\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'theoretical results hold empirically as well, through best case optimization of the vectors themselves.\\nWe then make a practical connection to existing state-of-the-art models by creating a simple natural\\nlanguage instantiation of the theory, called LIMIT, that these models cannot solve. Our results imply\\nthat the community should consider how instruction-based retrieval will impact retrievers, as there\\nwill be combinations of top-ùëò documents cannot represent.\\n12', 'Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\\nùúôùúÉ(ùë¢,ùë•)‚Äôs for ùë•‚Äôs that are positively associated with the user ahead\\nof other ùúôùúÉ(ùë¢,ùë•‚Ä≤)‚Äôs for ùë•‚Ä≤‚ààX.\\nIt‚Äôs useful to formulate the similarity between user ùë¢and item ùë•\\nas a probability distributionùëù(ùë•|ùë¢)[3, 48]. We do so by considering\\nùúôùúÉ(ùë•,ùë¢)as unnormalized logits and pass them through softmax:\\nùëù(ùë•|ùë¢)= ùëíùúôùúÉ (ùë•,ùë¢)\\n√ç\\nùë•‚Ä≤‚ààX ùëíùúôùúÉ (ùë•‚Ä≤,ùë¢) (1)\\nSpecific to information retrieval and recommendation setting,\\nthe size of X can be very large, potentially in the range of mil-\\nlions to billions for practical problem settings. Dot products (two\\ntower, dual encoder setups, etc.) are hence commonly used. In this\\nsetup, we learn user and item representations as twoùëë-dimensional\\nembeddings, ùëìùúÉ(ùë¢) ‚ààRùëë and ùëîùúÉ(ùë•) ‚ààRùëë. We recommend item\\nùë• to user ùë¢ with probability proportional to their inner products,\\nùúôùúÉ(ùë•,ùë¢)‚àº‚ü® ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©; recall that we obtain normalized proba-\\nbility distribution with softmax, hence\\nln ùëù(ùë•|ùë¢)= ‚ü®ùëìùúÉ(ùë¢),ùëîùúÉ(ùë•)‚ü©‚àíùëçùë¢ (2)\\nwhere ùëçùë¢ is the partition function.\\n2.2 Architecture\\nFigure 1 shows our main architecture used in the rest of the paper.\\nWe highlight the main components of this architecture below.\\nOverall flow . Our design decomposes retrieval into multiple\\nstages. These stages run in a cascading fashion to produce the final\\ntop ùëò candidates. One example is illustrated in Figure 1(a), where\\nan accelerator friendly algorithm, h-indexer, is used to find a large\\nnumber of candidates (ùëò‚Ä≤= 105) out of a 100M corpus, and then a\\ncomplex similarity function (e.g., mixture-of-logits) is used to find\\nthe final top ùëò (e.g., 100) candidates. We show in Section 5.2.1 that\\nthis design significantly improves throughput without degrading\\nrecall for suitably chosen values of ùëò‚Ä≤.\\nMain similarity function . The hierarchical retrieval design\\nenables complex neural networks to be used when modeling (user,\\nitem) similarities. In Section 3, we discuss one such instance of'}\n",
      "Only in Neural: {'On the Theoretical Limitations of\\nEmbedding-Based Retrieval\\nOrion Weller*,1,2, Michael Boratko1, Iftekhar Naim1 and Jinhyuk Lee1\\n1Google DeepMind,2Johns Hopkins University\\nVector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a\\nnascentriseinusingthemforreasoning,instruction-following,coding, andmore. Thesenewbenchmarks\\npush embeddings to work forany queryand any notion of relevancethat could be given. While prior\\nworks have pointed out theoretical limitations of vector embeddings, there is a common assumption\\nthat these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome\\nwith better training data and larger models. In this work, we demonstrate that we may encounter these\\ntheoretical limitations in realistic settings with extremely simple queries. We connect known results\\nin learning theory, showing that the number of top-ùëò subsets of documents capable of being returned\\nas the result of some query is limited by the dimension of the embedding. We empirically show that\\nthis holds true even if we restrict toùëò = 2, and directly optimize on the test set with free parameterized\\nembeddings. We then create a realistic dataset called LIMIT that stress tests models based on these\\ntheoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple\\nnature of the task. Our work shows the limits of embedding models under the existing single vector\\nparadigm and calls for future research to develop methods that can resolve this fundamental limitation.\\n1. Introduction\\nOver the last two decades, information retrieval (IR) has moved from models dominated by sparse\\ntechniques (such as BM25 [Robertson et al., 1995]) to those that use neural language models (LM)\\nas their backbones [Lee et al., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].', 'On the Theoretical Limitations of Embedding-Based Retrieval\\nJ. Lee, F. Chen, S. Dua, D. Cer, M. Shanbhogue, I. Naim, G. H. √Åbrego, Z. Li, K. Chen, H. S. Vera, et al.\\nGemini embedding: Generalizable embeddings from gemini.arXiv preprint arXiv:2503.07891,\\n2025.\\nK. Lee, M.-W. Chang, and K. Toutanova. Latent retrieval for weakly supervised open domain question\\nanswering. In A. Korhonen, D. Traum, and L. M√†rquez, editors,Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics, pages 6086‚Äì6096, Florence, Italy, July\\n2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https:\\n//aclanthology.org/P19-1612/.\\nC. Li, Z. Gan, Z. Yang, J. Yang, L. Li, L. Wang, J. Gao, et al. Multimodal foundation models: From\\nspecialists to general-purpose assistants.Foundations and Trends¬Æ in Computer Graphics and Vision,\\n16(1-2):1‚Äì214, 2024.\\nX. H. L√π. Bm25s: Orders of magnitude faster lexical search via eager sparse scoring.arXiv preprint\\narXiv:2407.03618, 2024.\\nX. Ma, S.-C. Lin, M. Li, W. Chen, and J. Lin. Unifying multimodal retrieval via document screenshot\\nembedding. arXiv preprint arXiv:2406.11251, 2024.\\nC. Malaviya, P. Shaw, M.-W. Chang, K. Lee, and K. Toutanova. Quest: A retrieval dataset of entity-\\nseeking queries with implicit set operations.arXiv preprint arXiv:2305.11694, 2023.\\nN. Muennighoff, N. Tazi, L. Magne, and N. Reimers. Mteb: Massive text embedding benchmark.arXiv\\npreprint arXiv:2210.07316, 2022.\\nN. Muennighoff, S. Hongjin, L. Wang, N. Yang, F. Wei, T. Yu, A. Singh, and D. Kiela. Generative\\nrepresentational instruction tuning. InICLR 2024 Workshop: How Far Are We From AGI, 2024.\\nH. Oh, H. Lee, S. Ye, H. Shin, H. Jang, C. Jun, and M. Seo. Instructir: A benchmark for instruction\\nfollowing of information retrieval models.arXiv preprint arXiv:2402.14334, 2024.\\nA. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding.arXiv\\npreprint arXiv:1807.03748, 2018.'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 10: What are some state-of-the-art techniques for efficient neural retrieval on GPUs or TPUs?\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Differences detected!\n",
      "\n",
      "Different chunks retrieved:\n",
      "Only in Cosine: {'Revisiting Neural Retrieval on Accelerators KDD ‚Äô23, August 6‚Äì10, 2023, Long Beach, CA, USA\\nTable 6: Amazon Reviews (Beauty, Games, and Books) results on top of sequential methods.\\nmethod hr@10 hr@50 hr@200 hr@500 mrr\\nBeauty\\nbaseline (BCE) 0.0256 0.0743 0.1604 0.2370 0.0126\\nDot product + SS 0.0655 (+155.4%) 0.1379 (+85.5%) 0.2271 (+41.6%) 0.2914 (+23.0%) 0.0333 (+164.4%)\\nMLP + SS 0.0393 (+53.4%) 0.1005 (+35.2%) 0.1864 (+16.2%) 0.2465 (+4.0%) 0.0189 (+49.8%)\\nNeuMF + SS 0.0535 (+108.9%) 0.1247 (+67.8%) 0.2152 (+34.2%) 0.2818 (+18.9%) 0.0264 (+109.6%)\\nDeepFM (4√ó4) + SS 0.0494 (+92.9%) 0.1271 (+71.0%) 0.2238 (+39.5%) 0.2953 (+24.6%) 0.0241 (+91.3%)\\nMoL (4√ó4) + SS 0.0541 (+111.0%) 0.1288 (+73.3%) 0.2246 (+40.0%) 0.2961 (+25.0%) 0.0262 (+107.5%)\\nGames\\nbaseline (BCE) 0.0794 0.2209 0.4217 0.5642 0.0383\\nDot product + SS 0.1246 (+57.0%) 0.2771 (+25.4%) 0.4505 (+6.8%) 0.5736 (+1.7%) 0.0600 (+56.7%)\\nMLP + SS 0.1066 (+34.3%) 0.2537 (+14.9%) 0.4293 (+1.8%) 0.5582 (-1.1%) 0.0498 (+30.1%)\\nNeuMF + SS 0.1175 (+48.0%) 0.2722 (+23.2%) 0.4479 (+6.2%) 0.5755 (+2.0%) 0.0559 (+46.0%)\\nDeepFM (4√ó4) + SS 0.1216 (+53.2%) 0.2817 (+27.5%) 0.4690 (+11.2%) 0.5965 (+5.7%) 0.0576 (+50.5%)\\nMoL (4√ó4) + SS 0.1221 (+53.9%) 0.2834 (+28.3%) 0.4713 (+11.8%) 0.5964 (+5.7%) 0.0586 (+53.2%)\\nBooks\\nbaseline (BCE) 0.0247 0.0660 0.1370 0.2079 0.0123\\nDot product + SS 0.0317 (+28.3%) 0.0814 (+23.3%) 0.1588 (+15.9%) 0.2295 (+10.4%) 0.0156 (+26.8%)\\nMLP + SS 0.0297 (+20.2%) 0.0759 (+15.0%) 0.1500 (+9.5%) 0.2190 (+5.3%) 0.0144 (+17.1%)\\nNeuMF + SS 0.0358 (+45.0%) 0.0871 (+32.0%) 0.1644 (+20.0%) 0.2338 (+12.5%) 0.0178 (+44.7%)\\nDeepFM (8√ó8) + SS 0.0361 (+46.2%) 0.0905 (+37.1%) 0.1706 (+24.5%) 0.2414 (+16.1%) 0.0179 (+45.5%)\\nMoL (8√ó8) + SS 0.0388 (+57.1%) 0.0934 (+41.5%) 0.1751 (+27.8%) 0.2479 (+19.2%) 0.0194 (+57.7%)\\nTable 7: Ablation studies on top of MoL and sequential methods.\\nML-1M (HR@10) ML-20M (HR@10) Beauty (HR@200) Games (HR@200) Books (HR@200)\\nMoL 0.3079 0.3114 0.2088 0.4618 0.1751'}\n",
      "Only in Neural: {'is designed with efficient training and inference on accelerators\\nin mind. It not only significantly outperforms dot-product based\\nbaselines in hit rate, but also better generalizes to torso and long\\ntail where there are limited training examples by utilizing gating\\ndesigns and component-level hypersphere embeddings.\\nOur second contribution is to enable efficient training and in-\\nference with expressive similarity functions (e.g., MoL), on modern\\naccelerators (Section 4). We propose a hierarchical retrieval strat-\\negy, and in particular, an accelerator-friendly algorithm for very\\nlarge corpus called h-indexer. Combined with other optimizations\\nlike quantization and kernel fusions, h-indexer enables us to scale\\nup retrieval to hundreds of millions of items with latency compa-\\nrable to MIPS setups. It‚Äôs worth remarking that, as highlighted in\\nFigure 2, the serving cost, e.g., latency and throughput ‚Äî on GPUs\\nin this work ‚Äî does not necessarily increase with compute thanks\\nto the significantly higher GPU utilization of our model. Our hier-\\narchical design, unlike dot-product, can also harness more compute\\nwith constant GPU memory budget, which is critical given limited\\nmemory capacity in production.\\nOur final contribution is to conduct extensive experiments on\\npublic and industrial datasets (Section 5) with MoL and h-indexer.\\nOur proposed method achieves up to 77.3% gain in HR on standard\\nbenchmark datasets like MovieLens and Amazon Reviews, and up\\nto +27.6% gain at HR@50 over strong baselines on one of the largest\\nrecommendation system deployments at Meta. The strong perfor-\\nmance of MoL validates our hypothesis that expressive similarity\\nfunctions are needed to capture dynamics of real-world datasets at\\nretrieval stage. The proposed method also reduces Matthew effect,\\nshowing that MoL indeed generalizes well over the entire corpus.\\nWe finally compare the proposed approach with related works\\nin Section 6 and conclude in Section 7.\\n2 OVERVIEW'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Retrieval Statistics:\n",
      "Total prompts analyzed: 10\n",
      "Different chunks retrieved: 8 (80.0%)\n",
      "Different ordering only: 2 (20.0%)\n",
      "Identical results: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "def print_retrieval_statistics(prompts: List[str], top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Prints statistical summary of differences between retrieval methods.\n",
    "    \"\"\"\n",
    "    total_prompts = len(prompts)\n",
    "    different_chunks_count = 0\n",
    "    different_order_count = 0\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        cosine_chunks = [chunk for _, chunk in retrieve_with_cosine(prompt, top_k=top_k)]\n",
    "        neural_chunks = [chunk for _, chunk in neural_reranking_retrieval(prompt)]\n",
    "        \n",
    "        if set(cosine_chunks) != set(neural_chunks):\n",
    "            different_chunks_count += 1\n",
    "        elif cosine_chunks != neural_chunks:\n",
    "            different_order_count += 1\n",
    "    \n",
    "    print(\"\\nRetrieval Statistics:\")\n",
    "    print(f\"Total prompts analyzed: {total_prompts}\")\n",
    "    print(f\"Different chunks retrieved: {different_chunks_count} ({different_chunks_count/total_prompts*100:.1f}%)\")\n",
    "    print(f\"Different ordering only: {different_order_count} ({different_order_count/total_prompts*100:.1f}%)\")\n",
    "    print(f\"Identical results: {total_prompts-different_chunks_count-different_order_count} \"\n",
    "          f\"({(total_prompts-different_chunks_count-different_order_count)/total_prompts*100:.1f}%)\")\n",
    "\n",
    "# Run both analyses\n",
    "analyze_retrieval_differences(prompts)\n",
    "print_retrieval_statistics(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fe6a0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query: What are the theoretical limitations of embedding-based retrieval? ===\n",
      "\n",
      "Initial Vector DB Results (top 5 of 20):\n",
      "\n",
      "1. Score: 0.9233\n",
      "Chunk: On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Iftekhar Naim1 and Jinhyuk Lee1\n",
      "1Google DeepMind,2Johns Hopkins University\n",
      "Vector embeddings have been ...\n",
      "\n",
      "2. Score: 0.8959\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments provide theoretical insight for the most common type of embedding model\n",
      "(single vector) they do not hol...\n",
      "\n",
      "3. Score: 0.8921\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Our previous results show both theoretically and empirically that embedding models cannot represent\n",
      "al...\n",
      "\n",
      "4. Score: 0.8894\n",
      "Chunk: setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "empirically show how the embedding dimension enables the solving of retrieval tasks. We find there\n",
      "exi...\n",
      "\n",
      "5. Score: 0.8893\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "documents with logical operators), we will reach a limit of combinations they cannot represent.\n",
      "Thus, the community should be aware of these...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "After MOL Reranking:\n",
      "\n",
      "1. Score: 0.0512\n",
      "Chunk: On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Iftekhar Naim1 and Jinhyuk Lee1\n",
      "1Google DeepMind,2Johns Hopkins University\n",
      "Vector embeddings have been ...\n",
      "\n",
      "2. Score: 0.0505\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments provide theoretical insight for the most common type of embedding model\n",
      "(single vector) they do not hol...\n",
      "\n",
      "3. Score: 0.0504\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Our previous results show both theoretically and empirically that embedding models cannot represent\n",
      "al...\n",
      "\n",
      "4. Score: 0.0503\n",
      "Chunk: setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "empirically show how the embedding dimension enables the solving of retrieval tasks. We find there\n",
      "exi...\n",
      "\n",
      "5. Score: 0.0503\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "documents with logical operators), we will reach a limit of combinations they cannot represent.\n",
      "Thus, the community should be aware of these...\n"
     ]
    }
   ],
   "source": [
    "def compare_reranking_effect(query: str, initial_k: int = 20, final_k: int = 5):\n",
    "    \"\"\"\n",
    "    Demonstrates the effect of MOL reranking on vector DB results\n",
    "    \"\"\"\n",
    "    # Step 1: Initial vector DB-style retrieval\n",
    "    initial_results = retrieve_with_cosine(query, top_k=initial_k)\n",
    "    \n",
    "    # Step 2: Prepare candidates for MOL\n",
    "    candidate_chunks = [chunk for _, chunk in initial_results]\n",
    "    candidate_indices = [chunks.index(chunk) for chunk in candidate_chunks]\n",
    "    candidate_embeddings = chunk_embeddings[candidate_indices]\n",
    "    \n",
    "    # Step 3: Apply MOL on just these candidates\n",
    "    query_embedding = torch.tensor(generator.get_embedding(query))\n",
    "    logits = torch.matmul(candidate_embeddings, query_embedding)\n",
    "    attention_weights = torch.nn.functional.softmax(logits / 2.0, dim=0)\n",
    "    \n",
    "    # Get final results\n",
    "    top_k_values, top_k_indices = torch.topk(attention_weights, k=final_k)\n",
    "    mol_results = [(score.item(), candidate_chunks[idx.item()]) \n",
    "                  for score, idx in zip(top_k_values, top_k_indices)]\n",
    "    \n",
    "    # Print comparison\n",
    "    print(f\"\\n=== Query: {query} ===\")\n",
    "    print(\"\\nInitial Vector DB Results (top 5 of 20):\")\n",
    "    for i, (score, chunk) in enumerate(initial_results[:5], 1):\n",
    "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "        print(f\"Chunk: {chunk[:200]}...\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nAfter MOL Reranking:\")\n",
    "    for i, (score, chunk) in enumerate(mol_results, 1):\n",
    "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "        print(f\"Chunk: {chunk[:200]}...\")\n",
    "\n",
    "# Test with first prompt\n",
    "compare_reranking_effect(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e5a4def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query: What are the theoretical limitations of embedding-based retrieval? ===\n",
      "\n",
      "Original Vector DB Results:\n",
      "\n",
      "1. Score: 0.9233\n",
      "Chunk: On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Iftekhar Naim1 and Jinhyuk Lee1\n",
      "1Google DeepMind,2Johns Hopkins University\n",
      "Vector embeddings have been ...\n",
      "\n",
      "2. Score: 0.8959\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments provide theoretical insight for the most common type of embedding model\n",
      "(single vector) they do not hol...\n",
      "\n",
      "3. Score: 0.8921\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Our previous results show both theoretically and empirically that embedding models cannot represent\n",
      "al...\n",
      "\n",
      "4. Score: 0.8894\n",
      "Chunk: setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "empirically show how the embedding dimension enables the solving of retrieval tasks. We find there\n",
      "exi...\n",
      "\n",
      "5. Score: 0.8893\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "documents with logical operators), we will reach a limit of combinations they cannot represent.\n",
      "Thus, the community should be aware of these...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Enhanced MOL Reranking:\n",
      "\n",
      "1. Score: 0.0510\n",
      "Chunk: On the Theoretical Limitations of\n",
      "Embedding-Based Retrieval\n",
      "Orion Weller*,1,2, Michael Boratko1, Iftekhar Naim1 and Jinhyuk Lee1\n",
      "1Google DeepMind,2Johns Hopkins University\n",
      "Vector embeddings have been ...\n",
      "\n",
      "2. Score: 0.0503\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "Limitations\n",
      "Although our experiments provide theoretical insight for the most common type of embedding model\n",
      "(single vector) they do not hol...\n",
      "\n",
      "3. Score: 0.0503\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "5.6. Alternatives to Embedding Models\n",
      "Our previous results show both theoretically and empirically that embedding models cannot represent\n",
      "al...\n",
      "\n",
      "4. Score: 0.0502\n",
      "Chunk: setting where the vectors themselves are directly optimized with the test data. This allows us to\n",
      "empirically show how the embedding dimension enables the solving of retrieval tasks. We find there\n",
      "exi...\n",
      "\n",
      "5. Score: 0.0502\n",
      "Chunk: On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "documents with logical operators), we will reach a limit of combinations they cannot represent.\n",
      "Thus, the community should be aware of these...\n"
     ]
    }
   ],
   "source": [
    "def enhanced_mol_reranking(query: str, initial_k: int = 20, final_k: int = 5):\n",
    "    \"\"\"\n",
    "    Enhanced MOL reranking that considers document-document relationships\n",
    "    \"\"\"\n",
    "    # Step 1: Initial retrieval\n",
    "    initial_results = retrieve_with_cosine(query, top_k=initial_k)\n",
    "    candidate_chunks = [chunk for _, chunk in initial_results]\n",
    "    candidate_indices = [chunks.index(chunk) for chunk in candidate_chunks]\n",
    "    candidate_embeddings = chunk_embeddings[candidate_indices]\n",
    "    \n",
    "    # Step 2: Compute document-document similarities\n",
    "    doc_doc_sim = torch.matmul(candidate_embeddings, candidate_embeddings.T)\n",
    "    \n",
    "    # Step 3: Query-document similarities\n",
    "    query_embedding = torch.tensor(generator.get_embedding(query))\n",
    "    query_doc_sim = torch.matmul(candidate_embeddings, query_embedding)\n",
    "    \n",
    "    # Step 4: Combine both signals\n",
    "    # Factor in document uniqueness by penalizing similar documents\n",
    "    diversity_penalty = 1.0 - (doc_doc_sim.mean(dim=1))  # Lower score for similar docs\n",
    "    combined_scores = query_doc_sim + 0.3 * diversity_penalty\n",
    "    \n",
    "    # Step 5: Apply softmax for final scores\n",
    "    final_scores = torch.nn.functional.softmax(combined_scores / 2.0, dim=0)\n",
    "    \n",
    "    # Get results\n",
    "    top_k_values, top_k_indices = torch.topk(final_scores, k=final_k)\n",
    "    mol_results = [(score.item(), candidate_chunks[idx.item()]) \n",
    "                  for score, idx in zip(top_k_values, top_k_indices)]\n",
    "    \n",
    "    return mol_results\n",
    "\n",
    "# Compare original vs enhanced reranking\n",
    "def compare_reranking_methods(query: str):\n",
    "    print(f\"\\n=== Query: {query} ===\")\n",
    "    \n",
    "    # Original cosine results\n",
    "    print(\"\\nOriginal Vector DB Results:\")\n",
    "    cosine_results = retrieve_with_cosine(query, top_k=5)\n",
    "    for i, (score, chunk) in enumerate(cosine_results, 1):\n",
    "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "        print(f\"Chunk: {chunk[:200]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Enhanced MOL results\n",
    "    print(\"\\nEnhanced MOL Reranking:\")\n",
    "    mol_results = enhanced_mol_reranking(query)\n",
    "    for i, (score, chunk) in enumerate(mol_results, 1):\n",
    "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "        print(f\"Chunk: {chunk[:200]}...\")\n",
    "\n",
    "# Test with first prompt\n",
    "compare_reranking_methods(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5bae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4c344f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451708b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b49c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
